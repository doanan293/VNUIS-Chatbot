{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andv/important/chatbot_rag/.venv/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andv/important/chatbot_rag/.venv/src/tts/TTS/utils/io.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(f, map_location=map_location, **kwargs)\n",
      "GPT2InferenceModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-25 15:10:53,994] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-25 15:10:54,582] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.15.2, git-hash=unknown, git-branch=unknown\n",
      "[2024-10-25 15:10:54,583] [WARNING] [config_utils.py:70:_process_deprecated_field] Config parameter replace_method is deprecated. This parameter is no longer needed, please remove from your call to DeepSpeed-inference\n",
      "[2024-10-25 15:10:54,583] [WARNING] [config_utils.py:70:_process_deprecated_field] Config parameter mp_size is deprecated use tensor_parallel.tp_size instead\n",
      "[2024-10-25 15:10:54,584] [INFO] [logging.py:96:log_dist] [Rank -1] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1\n",
      "[2024-10-25 15:10:54,681] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed-Inference config: {'layer_id': 0, 'hidden_size': 1024, 'intermediate_size': 4096, 'heads': 16, 'num_hidden_layers': -1, 'dtype': torch.float32, 'pre_layer_norm': True, 'norm_type': <NormType.LayerNorm: 1>, 'local_rank': -1, 'stochastic_mode': False, 'epsilon': 1e-05, 'mp_size': 1, 'scale_attention': True, 'triangular_masking': True, 'local_attention': False, 'window_size': 1, 'rotary_dim': -1, 'rotate_half': False, 'rotate_every_two': True, 'return_tuple': True, 'mlp_after_attn': True, 'mlp_act_func_type': <ActivationFuncType.GELU: 1>, 'training_mp_size': 1, 'bigscience_bloom': False, 'max_out_tokens': 1024, 'min_out_tokens': 1, 'scale_attn_by_inverse_layer_idx': False, 'enable_qkv_quantization': False, 'use_mup': False, 'return_single_tuple': False, 'set_empty_params': False, 'transposed_mode': False, 'use_triton': False, 'triton_autotune': False, 'num_kv': -1, 'rope_theta': 10000, 'invert_mask': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/andv/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/andv/.cache/torch_extensions/py310_cu124/transformer_inference/build.ninja...\n",
      "/home/andv/important/chatbot_rag/.venv/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Building extension module transformer_inference...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n",
      "Time to load transformer_inference op: 0.04592084884643555 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module transformer_inference...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models Loaded!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import CrossEncoder, SentenceTransformer, util\n",
    "from underthesea import sent_tokenize\n",
    "from vinorm import TTSnorm\n",
    "from TTS.tts.configs.xtts_config import XttsConfig\n",
    "from TTS.tts.models.xtts import Xtts\n",
    "import io\n",
    "import soundfile as sf  # Th∆∞ vi·ªán x·ª≠ l√Ω √¢m thanh\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "from pydub import AudioSegment\n",
    "from io import BytesIO\n",
    "import librosa\n",
    "from Xu_ly_text import Xu_ly_text, Xu_ly_text_de_doc\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "# from huggingface_hub import login\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# # Get the Hugging Face access token from environment variables\n",
    "# hf_token = os.getenv(\"PROJECTCB1_HUGGINGFACE_ACCESS_TOKEN\")\n",
    "\n",
    "# # Log in to Hugging Face using the access token\n",
    "# if hf_token:\n",
    "#     login(token=hf_token)\n",
    "# else:\n",
    "#     print(\n",
    "#         \"Access token not found. Please set the HUGGINGFACE_ACCESS_TOKEN in your .env file.\"\n",
    "#     )\n",
    "\n",
    "eb_model_path = os.getenv(\"PROJECTCB1_EMBEDDING_MODEL\")\n",
    "embeddings_path = os.getenv(\"PROJECTCB1_EMBEDDING_DATA_PATH\")\n",
    "\n",
    "\n",
    "# H√†m n·ªôi b·ªô\n",
    "def load_embedding_model(embedding_model_path):\n",
    "    embedding_model = SentenceTransformer(\n",
    "        model_name_or_path=embedding_model_path, device=device\n",
    "    )\n",
    "    return embedding_model\n",
    "\n",
    "\n",
    "def load_reranking_model(pr_model_path):\n",
    "    pr_model = CrossEncoder(model_name=pr_model_path, device=device, trust_remote_code=True)\n",
    "    return pr_model\n",
    "\n",
    "\n",
    "def load_embeddings(embeddings_path):\n",
    "    text_chunks_and_embedding_df = pd.read_csv(embeddings_path)\n",
    "    text_chunks_and_embedding_df[\"embedding\"] = text_chunks_and_embedding_df[\n",
    "        \"embedding\"\n",
    "    ].apply(lambda x: np.fromstring(x.strip(\"[]\"), sep=\" \"))\n",
    "    pages_and_chunks = text_chunks_and_embedding_df.to_dict(orient=\"records\")\n",
    "    embeddings = torch.tensor(\n",
    "        np.array(text_chunks_and_embedding_df[\"embedding\"].tolist()),\n",
    "        dtype=torch.float32,\n",
    "    ).to(device)\n",
    "    return embeddings, pages_and_chunks\n",
    "\n",
    "\n",
    "def clear_gpu_cache():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def load_model_tts(xtts_checkpoint, xtts_config, xtts_vocab):\n",
    "    clear_gpu_cache()\n",
    "    config = XttsConfig()\n",
    "    config.load_json(xtts_config)\n",
    "    XTTS_MODEL = Xtts.init_from_config(config)\n",
    "\n",
    "    use_deepspeed = torch.cuda.is_available()\n",
    "\n",
    "    XTTS_MODEL.load_checkpoint(\n",
    "        config,\n",
    "        checkpoint_path=xtts_checkpoint,\n",
    "        vocab_path=xtts_vocab,\n",
    "        use_deepspeed=use_deepspeed,\n",
    "    )\n",
    "    if torch.cuda.is_available():\n",
    "        XTTS_MODEL.cuda()\n",
    "    return XTTS_MODEL\n",
    "\n",
    "\n",
    "def normalize_vietnamese_text(text):\n",
    "    text = Xu_ly_text_de_doc(text)\n",
    "    text = (\n",
    "        TTSnorm(text, unknown=False, lower=False, rule=True)\n",
    "        .replace(\"  \", \" \")\n",
    "        .replace(\":\", \".\")\n",
    "        .replace(\"!.\", \"!\")\n",
    "        .replace(\"?.\", \"?\")\n",
    "        .replace(\" .\", \".\")\n",
    "        .replace(\" ,\", \",\")\n",
    "        .replace('\"', \"\")\n",
    "        .replace(\"'\", \"\")\n",
    "        .replace(\"+\", \" \")\n",
    "        .replace(\"..\", \".\")\n",
    "        .replace(\"AI\", \"√Çy Ai\")\n",
    "        .replace(\"A.I\", \"√Çy Ai\")\n",
    "    )\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "def split_sentences(text, max_length=245):\n",
    "    text = (\n",
    "        text.replace(\"\\n\", \". \").replace(\";\", \".\").replace(\"?\", \".\").replace(\"!\", \".\")\n",
    "    )\n",
    "\n",
    "    sentences = re.findall(r\"[^,.]+[,.]\", text)\n",
    "    grouped_sentences = []\n",
    "    current_group = \"\"\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # N·∫øu th√™m c√¢u v√†o m√† kh√¥ng v∆∞·ª£t qu√° gi·ªõi h·∫°n max_length\n",
    "        if len(current_group) + len(sentence) + 1 < max_length:\n",
    "            if current_group:\n",
    "                current_group += \" \" + sentence  # Gh√©p c√¢u m·ªõi v√†o c√¢u tr∆∞·ªõc ƒë√≥\n",
    "            else:\n",
    "                current_group = sentence  # C√¢u ƒë·∫ßu ti√™n c·ªßa nh√≥m\n",
    "        elif len(sentence) > max_length:  # X·ª≠ l√Ω\n",
    "            if current_group:\n",
    "                grouped_sentences.append(current_group)\n",
    "                current_group = \"\"\n",
    "            k = 0\n",
    "            tamthoi = []\n",
    "            for i in sentence.split(\" \"):\n",
    "                tamthoi += [i]\n",
    "                if len(tamthoi) >= 40:\n",
    "                    grouped_sentences += [\" \".join(tamthoi)]\n",
    "                    tamthoi = []\n",
    "            if tamthoi:\n",
    "                grouped_sentences += [\" \".join(tamthoi)]\n",
    "        else:\n",
    "            grouped_sentences.append(current_group)  # Th√™m nh√≥m v√†o list\n",
    "            current_group = sentence  # Kh·ªüi t·∫°o nh√≥m m·ªõi v·ªõi c√¢u hi·ªán t·∫°i\n",
    "\n",
    "    if current_group:\n",
    "        grouped_sentences.append(current_group)  # Th√™m nh√≥m cu·ªëi c√πng v√†o list\n",
    "\n",
    "    return grouped_sentences\n",
    "\n",
    "\n",
    "# Khai b√°o c√°c m√¥ h√¨nh\n",
    "\n",
    "print(\"Loading models... \")\n",
    "# Load model embedding\n",
    "\n",
    "embedding_model = load_embedding_model(eb_model_path)\n",
    "\n",
    "# Load reranking\n",
    "# rr_model_path = \"itdainb/PhoRanker\"\n",
    "# reranking_model = load_reranking_model(rr_model_path)\n",
    "\n",
    "# Dowload TTS capleaf/viXTTS\n",
    "# from huggingface_hub import snapshot_download\n",
    "\n",
    "# snapshot_download(\n",
    "#     repo_id=\"capleaf/viXTTS\", repo_type=\"model\", local_dir=\"Model/TTS_model\"\n",
    "# )\n",
    "\n",
    "tts_model_path = os.getenv(\"PROJECTCB1_TTS_MODEL\")\n",
    "# Load model TTS capleaf/viXTTS\n",
    "vixtts_model = load_model_tts(\n",
    "    xtts_checkpoint=f\"{tts_model_path}/model.pth\",\n",
    "    xtts_config=f\"{tts_model_path}/config.json\",\n",
    "    xtts_vocab=f\"{tts_model_path}/vocab.json\",\n",
    ")\n",
    "\n",
    "embeddings, pages_and_chunks = load_embeddings(embeddings_path)  # Load embeddings\n",
    "reference_audio = os.getenv(\"PROJECTCB1_REFERENCE_AUDIO\")  # M·∫´u gi·ªçng n√≥i\n",
    "\n",
    "# Load model STT nguyenvulebinh/wav2vec2-base-vietnamese-250h\n",
    "\n",
    "stt_model_path = os.getenv(\"PROJECTCB1_STT_MODEL\")\n",
    "processor = Wav2Vec2Processor.from_pretrained(stt_model_path)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(stt_model_path).to(device)\n",
    "print(\"Models Loaded!\")\n",
    "\n",
    "# processor.save_pretrained(stt_model_path)\n",
    "# model.save_pretrained(stt_model_path)\n",
    "# H√†m s·ª≠ d·ª•ng cho API\n",
    "\n",
    "\n",
    "def run_stt(audio_bytes):\n",
    "    # ƒê·ªçc t·ªáp √¢m thanh t·ª´ byte\n",
    "    audio = AudioSegment.from_file(BytesIO(audio_bytes))\n",
    "\n",
    "    # Chuy·ªÉn ƒë·ªïi √¢m thanh th√†nh m·∫£ng numpy\n",
    "    samples = np.array(audio.get_array_of_samples())\n",
    "\n",
    "    # ƒê·∫£m b·∫£o l√† mono (1 k√™nh)\n",
    "    if audio.channels > 1:\n",
    "        samples = samples.reshape((-1, audio.channels))\n",
    "        samples = samples.mean(\n",
    "            axis=1\n",
    "        )  # L·∫•y trung b√¨nh gi√° tr·ªã c·ªßa t·∫•t c·∫£ c√°c k√™nh ƒë·ªÉ chuy·ªÉn sang mono\n",
    "\n",
    "    # Chu·∫©n h√≥a l·∫°i t·∫ßn s·ªë m·∫´u v·ªÅ 16000 Hz\n",
    "    samples_16k = librosa.resample(\n",
    "        samples.astype(np.float32), orig_sr=audio.frame_rate, target_sr=16000\n",
    "    )\n",
    "\n",
    "    # Tokenize d·ªØ li·ªáu ƒë·∫ßu v√†o\n",
    "    input_values = processor(\n",
    "        samples_16k, return_tensors=\"pt\", padding=\"longest\", sampling_rate=16000\n",
    "    ).input_values\n",
    "\n",
    "    # Chuy·ªÉn sang GPU v√† chuy·ªÉn ƒë·ªïi sang float\n",
    "    input_values = input_values.to(device).float()\n",
    "\n",
    "    # L·∫•y k·∫øt qu·∫£ d·ª± ƒëo√°n t·ª´ m√¥ h√¨nh\n",
    "    logits = model(input_values).logits\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    # Gi·∫£i m√£ k·∫øt qu·∫£ d·ª± ƒëo√°n th√†nh vƒÉn b·∫£n\n",
    "    transcription = processor.batch_decode(predicted_ids)[0]\n",
    "    text = Xu_ly_text(transcription)\n",
    "    return text\n",
    "\n",
    "\n",
    "def run_tts(text, lang=\"vi\"):\n",
    "    if vixtts_model is None or not reference_audio:\n",
    "        return \"You need to run the previous step to load the model !!\", None, None\n",
    "\n",
    "    gpt_cond_latent, speaker_embedding = vixtts_model.get_conditioning_latents(\n",
    "        audio_path=reference_audio,\n",
    "        gpt_cond_len=vixtts_model.config.gpt_cond_len,\n",
    "        max_ref_length=vixtts_model.config.max_ref_len,\n",
    "        sound_norm_refs=vixtts_model.config.sound_norm_refs,\n",
    "    )\n",
    "\n",
    "    # Chu·∫©n h√≥a\n",
    "    tts_text = normalize_vietnamese_text(text)\n",
    "    tts_texts = split_sentences(tts_text)\n",
    "    print(tts_texts)\n",
    "    wav_chunks = []\n",
    "    for text in tts_texts:\n",
    "        if text.strip() == \"\":\n",
    "            continue\n",
    "\n",
    "        wav_chunk = vixtts_model.inference(\n",
    "            text=text,\n",
    "            language=lang,\n",
    "            gpt_cond_latent=gpt_cond_latent,\n",
    "            speaker_embedding=speaker_embedding,\n",
    "            temperature=0.01,  # 0.3\n",
    "            length_penalty=1.0,  # 1.0\n",
    "            repetition_penalty=50.0,  # 10.0\n",
    "            top_k=5,  # 30\n",
    "            top_p=0.95,  # 0.85\n",
    "        )\n",
    "\n",
    "        keep_len = -1\n",
    "        wav_chunk[\"wav\"] = torch.tensor(wav_chunk[\"wav\"][:keep_len])\n",
    "        wav_chunks.append(wav_chunk[\"wav\"])\n",
    "\n",
    "    out_wav = (\n",
    "        torch.cat(wav_chunks, dim=0).squeeze(0).cpu().numpy()\n",
    "    )  # Chuy·ªÉn sang numpy array\n",
    "\n",
    "    # Chuy·ªÉn ƒë·ªïi Tensor th√†nh ƒë·ªãnh d·∫°ng WAV\n",
    "    buffer = io.BytesIO()\n",
    "\n",
    "    # Ghi √¢m thanh v√†o buffer, ƒë·∫£m b·∫£o d·ªØ li·ªáu ƒë·∫ßu v√†o l√† numpy array v√† ƒë·ªãnh d·∫°ng ƒë√∫ng\n",
    "    try:\n",
    "        sf.write(buffer, out_wav, 24000, format=\"WAV\")\n",
    "        buffer.seek(0)\n",
    "        wav_data = buffer.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing WAV file: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "    return wav_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_resources(query: str, n_resources_to_return: int = 5, threshold: int =0.01):\n",
    "    query_embedding = embedding_model.encode(query, convert_to_tensor=True)\n",
    "    # Use cosine similarity instead of dot score\n",
    "    cosine_scores = util.pytorch_cos_sim(query_embedding, embeddings)[0]\n",
    "    \n",
    "    # Get all scores and corresponding indices, then filter based on score > 0.5\n",
    "    scores, indices = torch.topk(input=cosine_scores, k=n_resources_to_return)\n",
    "    filtered_scores_indices = [(score.item(), index.item()) for score, index in zip(scores, indices) if score.item() > threshold]\n",
    "    \n",
    "    # Extract the scores and indices after filtering\n",
    "    filtered_indices = [index for _, index in filtered_scores_indices]\n",
    "    \n",
    "    # Take top 'n_resources_to_return' from the filtered list\n",
    "    # top_scores = filtered_scores[:n_resources_to_return]\n",
    "    top_indices = filtered_indices[:n_resources_to_return]\n",
    "    \n",
    "    context_items = [pages_and_chunks[i] for i in top_indices]\n",
    "    results = [item[\"Final_Answer\"] for item in context_items]\n",
    "    # pr_results = reranking_model.rank(query, results, return_documents=True, top_k=3)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage:\n",
    "# results, scores = retrieve_relevant_resources(\"What is the capital of Japan?\", n_resources_to_return=3)\n",
    "# print(results, scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to format the prompt\n",
    "# def prompt_formatter(query: str, results: list) -> str:\n",
    "#     context = \"* \" + \"\\n\\n* \".join(results)\n",
    "#     base_prompt = \"\"\"H√£y cho b·∫£n th√¢n kh√¥ng gian ƒë·ªÉ suy nghƒ© b·∫±ng c√°ch tr√≠ch xu·∫•t c√°c ƒëo·∫°n vƒÉn c√≥ li√™n quan t·ª´ ng·ªØ c·∫£nh d∆∞·ªõi ƒë√¢y tr∆∞·ªõc khi tr·∫£ l·ªùi c√¢u h·ªèi:\n",
    "# \\n{context}\n",
    "# \\nC√°c ƒëo·∫°n vƒÉn c√≥ li√™n quan: <tr√≠ch xu·∫•t c√°c ƒëo·∫°n vƒÉn c√≥ li√™n quan t·ª´ ng·ªØ c·∫£nh t·∫°i ƒë√¢y>\n",
    "# C√¢u h·ªèi: {query}\n",
    "# \"\"\"\n",
    "#     formatted_prompt = base_prompt.format(context=context, query=query)\n",
    "#     prompt = f\"### C√¢u h·ªèi: {formatted_prompt}\\n\\n### Tr·∫£ l·ªùi:\" \n",
    "#     return prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prompt_formatter(query: str, results: list) -> str:\n",
    "#     context = \"* \" + \"\\n\\n* \".join(results)\n",
    "#     base_prompt = \"\"\"H√£y ƒë·ªÉ cho b·∫£n th√¢n ƒë·ªÉ suy nghƒ© b·∫±ng c√°ch s·ª≠ d·ª•ng c√°c m·ª•c ng·ªØ c·∫£nh sau ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng:\\n\\n{context}\\n\\nC√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng: {query}\"\"\"\n",
    "#     formatted_prompt = base_prompt.format(context=context, query=query)\n",
    "#     prompt = f\"\"\"<|im_start|>system\n",
    "# B·∫°n l√† m·ªôt tr·ª£ l√≠ AI h·ªØu √≠ch. H√£y tr·∫£ l·ªùi ng∆∞·ªùi d√πng m·ªôt c√°ch ch√≠nh x√°c.\n",
    "# <|im_end|>\n",
    "# <|im_start|>user\n",
    "# {formatted_prompt}<|im_end|>\n",
    "# <|im_start|>assistant\n",
    "# \"\"\"\n",
    "#     return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to format the prompt\n",
    "def prompt_formatter(query: str, results: list) -> str:\n",
    "    context = \"* \" + \"\\n\\n* \".join(results)\n",
    "    base_prompt = \"\"\"H√£y cho b·∫£n th√¢n kh√¥ng gian ƒë·ªÉ suy nghƒ© b·∫±ng c√°ch tr√≠ch xu·∫•t c√°c ƒëo·∫°n vƒÉn c√≥ li√™n quan t·ª´ ng·ªØ c·∫£nh d∆∞·ªõi ƒë√¢y tr∆∞·ªõc khi tr·∫£ l·ªùi c√¢u h·ªèi:\n",
    "\\n{context}\n",
    "\\nC√°c ƒëo·∫°n vƒÉn c√≥ li√™n quan: <tr√≠ch xu·∫•t c√°c ƒëo·∫°n vƒÉn c√≥ li√™n quan t·ª´ ng·ªØ c·∫£nh t·∫°i ƒë√¢y>\n",
    "C√¢u h·ªèi: {query}\n",
    "\"\"\"\n",
    "    prompt = base_prompt.format(context=context, query=query)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"N·ªôi dung d·ª± √°n 1 l√† g√¨?\"\n",
    "results = retrieve_relevant_resources(query)\n",
    "prompt = prompt_formatter(query, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### C√¢u h·ªèi: H√£y cho b·∫£n th√¢n kh√¥ng gian ƒë·ªÉ suy nghƒ© b·∫±ng c√°ch tr√≠ch xu·∫•t c√°c ƒëo·∫°n vƒÉn c√≥ li√™n quan t·ª´ ng·ªØ c·∫£nh d∆∞·ªõi ƒë√¢y tr∆∞·ªõc khi tr·∫£ l·ªùi c√¢u h·ªèi:\n",
      "\n",
      "* N·ªôi dung c·ª• th·ªÉ c·ªßa D·ª± √°n 1 l√† g√¨? \"N·ªôi dung c·ª• th·ªÉ c·ªßa D·ª± √°n 1 bao g·ªìm:\n",
      "- N·ªôi dung s·ªë 01: H·ªó tr·ª£ ƒë·∫•t ·ªü: CƒÉn c·ª© qu·ªπ ƒë·∫•t, h·∫°n m·ª©c ƒë·∫•t ·ªü v√† kh·∫£ nƒÉng ng√¢n s√°ch, ·ª¶y ban nh√¢n d√¢n c·∫•p t·ªânh xem x√©t, quy·∫øt ƒë·ªãnh giao ƒë·∫•t ƒë·ªÉ l√†m nh√† ·ªü cho c√°c ƒë·ªëi t∆∞·ª£ng n√™u tr√™n ph√π h·ª£p v·ªõi ƒëi·ªÅu ki·ªán, t·∫≠p qu√°n ·ªü ƒë·ªãa ph∆∞∆°ng v√† ph√°p lu·∫≠t v·ªÅ ƒë·∫•t ƒëai, c·ª• th·ªÉ:\n",
      "+ ·ªû nh·ªØng n∆°i c√≥ ƒëi·ªÅu ki·ªán v·ªÅ ƒë·∫•t ƒëai, ch√≠nh quy·ªÅn ƒë·ªãa ph∆∞∆°ng s·ª≠ d·ª•ng s·ªë ti·ªÅn h·ªó tr·ª£ t·ª´ ng√¢n s√°ch ƒë·ªÉ t·∫°o m·∫∑t b·∫±ng, l√†m h·∫° t·∫ßng k·ªπ thu·∫≠t ƒë·ªÉ c·∫•p ƒë·∫•t ·ªü cho c√°c ƒë·ªëi t∆∞·ª£ng ƒë∆∞·ª£c th·ª• h∆∞·ªüng;\n",
      "+ ·ªû c√°c ƒë·ªãa ph∆∞∆°ng kh√¥ng c√≥ ƒëi·ªÅu ki·ªán v·ªÅ ƒë·∫•t ƒëai, ch√≠nh quy·ªÅn ƒë·ªãa ph∆∞∆°ng b·ªë tr√≠ kinh ph√≠ h·ªó tr·ª£ cho ng∆∞·ªùi d√¢n t·ª± ·ªïn ƒë·ªãnh ch·ªó ·ªü theo h√¨nh th·ª©c xen gh√©p.\n",
      "- N·ªôi dung s·ªë 02: H·ªó tr·ª£ nh√† ·ªü: H·ªó tr·ª£ x√¢y d·ª±ng 01 cƒÉn nh√† theo phong t·ª•c t·∫≠p qu√°n c·ªßa ƒë·ªãa ph∆∞∆°ng, ƒë·ªãnh m·ª©c t√≠nh theo x√¢y d·ª±ng 01 cƒÉn nh√† c·∫•p 4 ƒë·∫£m b·∫£o 3 c·ª©ng (n·ªÅn c·ª©ng, khung - t∆∞·ªùng c·ª©ng, m√°i c·ª©ng).\n",
      "- N·ªôi dung s·ªë 03: H·ªó tr·ª£ ƒë·∫•t s·∫£n xu·∫•t, chuy·ªÉn ƒë·ªïi ngh·ªÅ: H·ªô d√¢n t·ªôc thi·ªÉu s·ªë ngh√®o; h·ªô ngh√®o d√¢n t·ªôc Kinh sinh s·ªëng ·ªü x√£ ƒë·∫∑c bi·ªát kh√≥ khƒÉn, th√¥n ƒë·∫∑c bi·ªát kh√≥ khƒÉn v√πng ƒë·ªìng b√†o d√¢n t·ªôc thi·ªÉu s·ªë v√† mi·ªÅn n√∫i l√†m ngh·ªÅ n√¥ng, l√¢m, ng∆∞ nghi·ªáp kh√¥ng c√≥ ho·∫∑c thi·∫øu t·ª´ 50% ƒë·∫•t s·∫£n xu·∫•t tr·ªü l√™n theo ƒë·ªãnh m·ª©c c·ªßa ƒë·ªãa ph∆∞∆°ng th√¨ ƒë∆∞·ª£c h∆∞·ªüng m·ªôt trong hai ch√≠nh s√°ch sau:\n",
      "+ H·ªó tr·ª£ tr·ª±c ti·∫øp ƒë·∫•t s·∫£n xu·∫•t: H·ªô kh√¥ng c√≥ ƒë·∫•t s·∫£n xu·∫•t n·∫øu c√≥ nhu c·∫ßu th√¨ ƒë∆∞·ª£c ch√≠nh quy·ªÅn ƒë·ªãa ph∆∞∆°ng tr·ª±c ti·∫øp giao ƒë·∫•t s·∫£n xu·∫•t;\n",
      "+ H·ªó tr·ª£ chuy·ªÉn ƒë·ªïi ngh·ªÅ: Tr∆∞·ªùng h·ª£p ch√≠nh quy·ªÅn ƒë·ªãa ph∆∞∆°ng kh√¥ng b·ªë tr√≠ ƒë∆∞·ª£c ƒë·∫•t s·∫£n xu·∫•t th√¨ h·ªô kh√¥ng c√≥ ƒë·∫•t ho·∫∑c thi·∫øu ƒë·∫•t s·∫£n xu·∫•t ƒë∆∞·ª£c h·ªó tr·ª£ chuy·ªÉn ƒë·ªïi ngh·ªÅ.\n",
      "- N·ªôi dung s·ªë 04: H·ªó tr·ª£ n∆∞·ªõc sinh ho·∫°t:\n",
      "+ H·ªó tr·ª£ n∆∞·ªõc sinh ho·∫°t ph√¢n t√°n: ∆Øu ti√™n h·ªó tr·ª£ ƒë·ªÉ mua s·∫Øm trang b·ªã ho·∫∑c x√¢y d·ª±ng b·ªÉ ch·ª©a n∆∞·ªõc ph·ª•c v·ª• sinh ho·∫°t c·ªßa h·ªô gia ƒë√¨nh;\n",
      "+ H·ªó tr·ª£ n∆∞·ªõc sinh ho·∫°t t·∫≠p trung: ƒê·∫ßu t∆∞ x√¢y d·ª±ng c√¥ng tr√¨nh n∆∞·ªõc t·∫≠p trung theo d·ª± √°n ƒë∆∞·ª£c c·∫•p c√≥ th·∫©m quy·ªÅn ph√™ duy·ªát. ∆Øu ti√™n cho ng∆∞·ªùi d√¢n v√πng th∆∞·ªùng xuy√™n x·∫£y ra h·∫°n h√°n, x√¢m nh·∫≠p m·∫∑n, v√πng ƒë·∫∑c bi·ªát kh√≥ khƒÉn, v√πng cao ch∆∞a c√≥ ngu·ªìn n∆∞·ªõc ho·∫∑c thi·∫øu n∆∞·ªõc sinh ho·∫°t h·ª£p v·ªá sinh.\n",
      "- H·ªô gia ƒë√¨nh thu·ªôc di·ªán ƒë·ªëi t∆∞·ª£ng theo quy ƒë·ªãnh c·ªßa D·ª± √°n n√†y c√≥ nhu c·∫ßu vay v·ªën ƒë∆∞·ª£c vay t·ª´ Ng√¢n h√†ng Ch√≠nh s√°ch x√£ h·ªôi ƒë·ªÉ c√≥ ƒë·∫•t ·ªü, x√¢y d·ª±ng m·ªõi ho·∫∑c s·ª≠a ch·ªØa nh√† ·ªü, t·∫°o qu·ªπ ƒë·∫•t s·∫£n xu·∫•t, h·ªçc ngh·ªÅ v√† chuy·ªÉn ƒë·ªïi ngh·ªÅ.\" (Theo Ph·∫ßn III, M·ª•c 1 Quy·∫øt ƒë·ªãnh Th·ªß t∆∞·ªõng ch√≠nh ph·ªß 1719/Qƒê-TTg)\n",
      "\n",
      "* N·ªôi dung c·ªßa Ti·ªÉu d·ª± √°n 1, D·ª± √°n 4 l√† g√¨? \"N·ªôi dung c·ªßa Ti·ªÉu d·ª± √°n 1, D·ª± √°n 4 thu·ªôc Ch∆∞∆°ng tr√¨nh m·ª•c ti√™u qu·ªëc gia ph√°t tri·ªÉn kinh t·∫ø - x√£ h·ªôi v√πng ƒë·ªìng b√†o d√¢n t·ªôc thi·ªÉu s·ªë v√† mi·ªÅn n√∫i:\n",
      "- N·ªôi dung s·ªë 01: ƒê·∫ßu t∆∞ c∆° s·ªü h·∫° t·∫ßng thi·∫øt y·∫øu v√πng ƒë·ªìng b√†o d√¢n t·ªôc thi·ªÉu s·ªë v√† mi·ªÅn n√∫i; ∆∞u ti√™n ƒë·ªëi v·ªõi c√°c x√£ ƒë·∫∑c bi·ªát kh√≥ khƒÉn, th√¥n ƒë·∫∑c bi·ªát kh√≥ khƒÉn.\n",
      "+ ƒê·∫ßu t∆∞ x√¢y d·ª±ng, c·∫£i t·∫°o c√°c c√¥ng tr√¨nh giao th√¥ng n√¥ng th√¥n ph·ª•c v·ª• s·∫£n xu·∫•t, kinh doanh v√† d√¢n sinh; c√¥ng tr√¨nh cung c·∫•p ƒëi·ªán ph·ª•c v·ª• sinh ho·∫°t v√† s·∫£n xu·∫•t, kinh doanh tr√™n ƒë·ªãa b√†n th√¥n, b·∫£n; tr·∫°m chuy·ªÉn ti·∫øp ph√°t thanh x√£, nh√† sinh ho·∫°t c·ªông ƒë·ªìng; tr∆∞·ªùng, l·ªõp h·ªçc ƒë·∫°t chu·∫©n; c√°c c√¥ng tr√¨nh th·ªßy l·ª£i nh·ªè; c√°c c√¥ng tr√¨nh h·∫° t·∫ßng quy m√¥ nh·ªè kh√°c do c·ªông ƒë·ªìng ƒë·ªÅ xu·∫•t, ph√π h·ª£p v·ªõi phong t·ª•c, t·∫≠p qu√°n ƒë·ªÉ ph·ª•c v·ª• nhu c·∫ßu c·ªßa c·ªông ƒë·ªìng, ph√π h·ª£p v·ªõi m·ª•c ti√™u c·ªßa Ch∆∞∆°ng tr√¨nh v√† quy ƒë·ªãnh c·ªßa ph√°p lu·∫≠t; ∆∞u ti√™n c√¥ng tr√¨nh c√≥ nhi·ªÅu h·ªô ngh√®o, ph·ª• n·ªØ h∆∞·ªüng l·ª£i;\n",
      "+ ƒê·∫ßu t∆∞ x√¢y d·ª±ng, n√¢ng c·∫•p, c·∫£i t·∫°o, s·ª≠a ch·ªØa, b·∫£o d∆∞·ª°ng, mua s·∫Øm trang thi·∫øt b·ªã cho c√°c tr·∫°m y t·∫ø x√£ b·∫£o ƒë·∫£m ƒë·∫°t chu·∫©n;\n",
      "+ ƒê·∫ßu t∆∞ c·ª©ng h√≥a ƒë∆∞·ªùng ƒë·∫øn trung t√¢m x√£ ch∆∞a ƒë∆∞·ª£c c·ª©ng h√≥a; ∆∞u ti√™n ƒë·∫ßu t∆∞ ƒë·ªëi v·ªõi c√°c x√£ ch∆∞a c√≥ ƒë∆∞·ªùng t·ª´ trung t√¢m huy·ªán ƒë·∫øn trung t√¢m x√£, ƒë∆∞·ªùng li√™n x√£ (t·ª´ trung t√¢m x√£ ƒë·∫øn trung t√¢m x√£);\n",
      "+ ƒê·∫ßu t∆∞ c∆° s·ªü h·∫° t·∫ßng tr·ªçng ƒëi·ªÉm k·∫øt n·ªëi c√°c x√£ ƒë·∫∑c bi·ªát kh√≥ khƒÉn tr√™n c√πng ƒë·ªãa b√†n (h·ªá th·ªëng h·∫° t·∫ßng ph·ª•c v·ª• gi√°o d·ª•c, y t·∫ø; h·ªá th·ªëng c·∫ßu, ƒë∆∞·ªùng giao th√¥ng; h·∫° t·∫ßng l∆∞·ªõi ƒëi·ªán...); x√¢y d·ª±ng c·∫ßu d√¢n sinh ƒë·ªÉ ph·ª•c v·ª• sinh ho·∫°t, tƒÉng c∆∞·ªùng k·∫øt n·ªëi, t·∫°o tr·ª•c ƒë·ªông l·ª±c ph√°t tri·ªÉn ƒë·ªìng b·ªô tr√™n c∆° s·ªü th√∫c ƒë·∫©y li√™n k·∫øt gi·ªØa c√°c x√£ ƒë·∫∑c bi·ªát kh√≥ khƒÉn nh·∫±m ph√°t huy s·ª©c m·∫°nh ti·ªÉu v√πng gi√∫p ph√°t tri·ªÉn b·ªÅn v·ªØng v√† g√≥p ph·∫ßn x√¢y d·ª±ng n√¥ng th√¥n m·ªõi v√πng ƒë·ªìng b√†o d√¢n t·ªôc thi·ªÉu s·ªë v√† mi·ªÅn n√∫i;\n",
      "+ ƒê·∫ßu t∆∞, h·ªó tr·ª£ kinh ph√≠ x√¢y d·ª±ng th√≠ ƒëi·ªÉm 04 nh√† h·ªèa t√°ng ƒëi·ªán cho ƒë·ªìng b√†o d√¢n t·ªôc thi·ªÉu s·ªë t·ªânh Ninh Thu·∫≠n, B√¨nh Thu·∫≠n (m·ªói t·ªânh 02 c√¥ng tr√¨nh);\n",
      "+ Duy tu, b·∫£o d∆∞·ª°ng c√¥ng tr√¨nh c∆° s·ªü h·∫° t·∫ßng tr√™n ƒë·ªãa b√†n ƒë·∫∑c bi·ªát kh√≥ khƒÉn v√† c√¥ng tr√¨nh c∆° s·ªü h·∫° t·∫ßng c√°c x√£, th√¥n ƒë√£ ƒë·∫ßu t∆∞ t·ª´ giai ƒëo·∫°n tr∆∞·ªõc.\n",
      "- N·ªôi dung s·ªë 02: ƒê·∫ßu t∆∞ x√¢y d·ª±ng, c·∫£i t·∫°o n√¢ng c·∫•p m·∫°ng l∆∞·ªõi ch·ª£ v√πng ƒë·ªìng b√†o d√¢n t·ªôc thi·ªÉu s·ªë v√† mi·ªÅn n√∫i.\"\n",
      " (Theo Ph·∫ßn III, M·ª•c 4 Quy·∫øt ƒë·ªãnh Th·ªß t∆∞·ªõng ch√≠nh ph·ªß 1719/Qƒê-TTg)\n",
      "\n",
      "* M·ª•c ti√™u c·ªßa N·ªôi dung 1, Ti·ªÉu d·ª± √°n 1, D·ª± √°n 10 l√† g√¨? \"M·ª•c ti√™u c·ªßa N·ªôi dung s·ªë 01, Ti·ªÉu d·ª± √°n 1, D·ª± √°n 10 l√†: \n",
      "X√¢y d·ª±ng, n√¢ng cao ch·∫•t l∆∞·ª£ng v√† hi·ªáu qu·∫£ c√¥ng t√°c v·∫≠n ƒë·ªông, ph√°t huy vai tr√≤ c·ªßa l·ª±c l∆∞·ª£ng c·ªët c√°n v√† ng∆∞·ªùi c√≥ uy t√≠n trong v√πng ƒë·ªìng b√†o d√¢n t·ªôc thi·ªÉu s·ªë v√† mi·ªÅn n√∫i. Bi·ªÉu d∆∞∆°ng, t√¥n vinh, ghi nh·∫≠n c√¥ng lao, s·ª± ƒë√≥ng g√≥p c·ªßa c√°c ƒëi·ªÉn h√¨nh ti√™n ti·∫øn trong v√πng ƒë·ªìng b√†o d√¢n t·ªôc thi·ªÉu s·ªë v√† mi·ªÅn n√∫i trong s·ª± nghi·ªáp x√¢y d·ª±ng, b·∫£o v·ªá T·ªï qu·ªëc v√† h·ªôi nh·∫≠p qu·ªëc t·∫ø.\"\n",
      " (Theo Ph·∫ßn III, M·ª•c 10 Quy·∫øt ƒë·ªãnh Th·ªß t∆∞·ªõng ch√≠nh ph·ªß 1719/Qƒê-TTg)\n",
      "\n",
      "* N·ªôi dung c·ªßa Ti·ªÉu d·ª± √°n 1, D·ª± √°n 5 l√† g√¨? \"N·ªôi dung c·ªßa Ti·ªÉu d·ª± √°n 1, D·ª± √°n 5 thu·ªôc Ch∆∞∆°ng tr√¨nh m·ª•c ti√™u qu·ªëc gia ph√°t tri·ªÉn kinh t·∫ø - x√£ h·ªôi v√πng ƒë·ªìng b√†o d√¢n t·ªôc thi·ªÉu s·ªë v√† mi·ªÅn n√∫i:\n",
      "+ ƒê·∫ßu t∆∞ c∆° s·ªü v·∫≠t ch·∫•t, trang thi·∫øt b·ªã cho c√°c tr∆∞·ªùng ph·ªï th√¥ng d√¢n t·ªôc n·ªôi tr√∫, b√°n tr√∫, c√≥ h·ªçc sinh b√°n tr√∫:\n",
      ". N√¢ng c·∫•p, c·∫£i t·∫°o c∆° s·ªü v·∫≠t ch·∫•t kh·ªëi ph√≤ng/c√¥ng tr√¨nh ph·ª•c v·ª• ƒÉn, ·ªü, sinh ho·∫°t cho h·ªçc sinh v√† ph√≤ng c√¥ng v·ª• gi√°o vi√™n;\n",
      ". N√¢ng c·∫•p, c·∫£i t·∫°o c∆° s·ªü v·∫≠t ch·∫•t/kh·ªëi ph√≤ng/c√¥ng tr√¨nh ph·ª•c v·ª• h·ªçc t·∫≠p; b·ªï sung, n√¢ng c·∫•p c√°c c√¥ng tr√¨nh ph·ª• tr·ª£ kh√°c;\n",
      ". ƒê·∫ßu t∆∞ c∆° s·ªü v·∫≠t ch·∫•t ph·ª•c v·ª• chuy·ªÉn ƒë·ªïi s·ªë gi√°o d·ª•c ph·ª•c v·ª• vi·ªác gi·∫£ng d·∫°y v√† h·ªçc t·∫≠p tr·ª±c tuy·∫øn cho h·ªçc sinh d√¢n t·ªôc thi·ªÉu s·ªë;\n",
      ". ∆Øu ti√™n ƒë·∫ßu t∆∞ x√¢y d·ª±ng tr∆∞·ªùng d√¢n t·ªôc n·ªôi tr√∫ cho huy·ªán c√≥ ƒë√¥ng ƒë·ªìng b√†o d√¢n t·ªôc thi·ªÉu s·ªë sinh s·ªëng nh∆∞ng ch∆∞a c√≥ ho·∫∑c ph·∫£i ƒëi thu√™ ƒë·ªãa ƒëi·ªÉm ƒë·ªÉ t·ªï ch·ª©c ho·∫°t ƒë·ªông.\n",
      "+ X√≥a m√π ch·ªØ cho ng∆∞·ªùi d√¢n v√πng ƒë·ªìng b√†o d√¢n t·ªôc thi·ªÉu s·ªë:\n",
      ". X√¢y d·ª±ng t√†i li·ªáu ph·ª•c v·ª• h∆∞·ªõng d·∫´n d·∫°y xo√° m√π ch·ªØ, thi·∫øt k·∫ø c√¥ng ngh·ªá, thi·∫øt b·ªã l∆∞u tr·ªØ c∆° s·ªü d·ªØ li·ªáu v·ªÅ xo√° m√π ch·ªØ, d·∫°y h·ªçc xo√° m√π ch·ªØ;\n",
      ". B·ªìi d∆∞·ª°ng, t·∫≠p hu·∫•n, truy·ªÅn th√¥ng, tuy√™n truy·ªÅn;\n",
      ". H·ªó tr·ª£ ng∆∞·ªùi d√¢n tham gia h·ªçc xo√° m√π ch·ªØ;\n",
      ". H·ªó tr·ª£ t√†i li·ªáu h·ªçc t·∫≠p, s√°ch gi√°o khoa, vƒÉn ph√≤ng ph·∫©m.\"\n",
      " (Theo Ph·∫ßn III, M·ª•c 5 Quy·∫øt ƒë·ªãnh Th·ªß t∆∞·ªõng ch√≠nh ph·ªß 1719/Qƒê-TTg)\n",
      "\n",
      "* N·ªôi dung c·ªßa D·ª± √°n 2 l√† g√¨? \"N·ªôi dung c·ªßa D·ª± √°n 2 thu·ªôc Ch∆∞∆°ng tr√¨nh m·ª•c ti√™u qu·ªëc gia ph√°t tri·ªÉn kinh t·∫ø - x√£ h·ªôi v√πng ƒë·ªìng b√†o d√¢n t·ªôc thi·ªÉu s·ªë v√† mi·ªÅn n√∫i:\n",
      "- H·ªó tr·ª£ kh·∫£o s√°t v·ªã tr√≠, ƒë·ªãa ƒëi·ªÉm ph·ª•c v·ª• c√¥ng t√°c quy ho·∫°ch, l·∫≠p, th·∫©m ƒë·ªãnh, ph√™ duy·ªát d·ª± √°n ƒë·∫ßu t∆∞ b·ªë tr√≠ ·ªïn ƒë·ªãnh d√¢n c∆∞.\n",
      "- H·ªó tr·ª£ ƒë·∫ßu t∆∞ x√¢y d·ª±ng k·∫øt c·∫•u h·∫° t·∫ßng:\n",
      "+ B·ªìi th∆∞·ªùng, gi·∫£i ph√≥ng m·∫∑t b·∫±ng, san l·∫•p m·∫∑t b·∫±ng ƒë·∫•t ·ªü t·∫°i ƒëi·ªÉm t√°i ƒë·ªãnh c∆∞;\n",
      "+ Khai hoang ƒë·∫•t s·∫£n xu·∫•t;\n",
      "+ ƒê·∫ßu t∆∞ x√¢y d·ª±ng: ƒê∆∞·ªùng giao th√¥ng (n·ªôi v√πng d·ª± √°n v√† ƒë∆∞·ªùng n·ªëi ƒëi·ªÉm d√¢n c∆∞ m·ªõi ƒë·∫øn tuy·∫øn giao th√¥ng g·∫ßn nh·∫•t); c√¥ng tr√¨nh th·ªßy l·ª£i nh·ªè, ƒëi·ªán, n∆∞·ªõc sinh ho·∫°t v√† m·ªôt s·ªë c√¥ng tr√¨nh thi·∫øt y·∫øu kh√°c.\n",
      "- H·ªó tr·ª£ tr·ª±c ti·∫øp h·ªô gia ƒë√¨nh:\n",
      "+ H·ªó tr·ª£ nh√† ·ªü, ƒë·∫•t s·∫£n xu·∫•t (ƒë·ªëi v·ªõi tr∆∞·ªùng h·ª£p ph·∫£i thay ƒë·ªïi ch·ªó ·ªü);\n",
      "+ H·ªó tr·ª£ kinh ph√≠ di chuy·ªÉn c√°c h·ªô t·ª´ n∆°i ·ªü c≈© ƒë·∫øn n∆°i t√°i ƒë·ªãnh c∆∞;\n",
      "+ Th·ª±c hi·ªán c√°c ch√≠nh s√°ch h·ªó tr·ª£ hi·ªán h√†nh kh√°c ƒë·ªëi v·ªõi c√°c h·ªô ƒë∆∞·ª£c b·ªë tr√≠ ·ªïn ƒë·ªãnh nh∆∞ ng∆∞·ªùi d√¢n t·∫°i ch·ªó.\n",
      "- H·ªó tr·ª£ ƒë·ªãa b√†n b·ªë tr√≠ d√¢n xen gh√©p:\n",
      "+ ƒêi·ªÅu ch·ªânh ƒë·∫•t ·ªü, ƒë·∫•t s·∫£n xu·∫•t giao cho c√°c h·ªô m·ªõi ƒë·∫øn (khai hoang, b·ªìi th∆∞·ªùng theo quy ƒë·ªãnh khi thu h·ªìi ƒë·∫•t c·ªßa c√°c t·ªï ch·ª©c, c√° nh√¢n khi thu h·ªìi ƒë·∫•t);\n",
      "+ X√¢y m·ªõi ho·∫∑c n√¢ng c·∫•p l·ªõp h·ªçc, tr·∫°m y t·∫ø, c√°c c√¥ng tr√¨nh th·ªßy l·ª£i n·ªôi ƒë·ªìng, ƒë∆∞·ªùng d√¢n sinh, ƒëi·ªán, n∆∞·ªõc sinh ho·∫°t v√† m·ªôt s·ªë c√¥ng tr√¨nh h·∫° t·∫ßng thi·∫øt y·∫øu kh√°c.\" (Theo Ph·∫ßn III, M·ª•c 1 Quy·∫øt ƒë·ªãnh Th·ªß t∆∞·ªõng ch√≠nh ph·ªß 1719/Qƒê-TTg)\n",
      "\n",
      "C√°c ƒëo·∫°n vƒÉn c√≥ li√™n quan: <tr√≠ch xu·∫•t c√°c ƒëo·∫°n vƒÉn c√≥ li√™n quan t·ª´ ng·ªØ c·∫£nh t·∫°i ƒë√¢y>\n",
      "C√¢u h·ªèi: N·ªôi dung d·ª± √°n 1 l√† g√¨?\n",
      "\n",
      "\n",
      "### Tr·∫£ l·ªùi:\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andv/.cache/huggingface/modules/transformers_modules/vinai/PhoGPT-4B-Chat/f56fc6d71f147a3a293fdab56676337dc6f641e1/configuration_mpt.py:114: UserWarning: alibi or rope is turned on, setting `learned_pos_emb` to `False.`\n",
      "  warnings.warn(f'alibi or rope is turned on, setting `learned_pos_emb` to `False.`')\n",
      "/home/andv/.cache/huggingface/modules/transformers_modules/vinai/PhoGPT-4B-Chat/f56fc6d71f147a3a293fdab56676337dc6f641e1/configuration_mpt.py:141: UserWarning: If not using a Prefix Language Model, we recommend setting \"attn_impl\" to \"flash\" instead of \"triton\".\n",
      "  warnings.warn(UserWarning('If not using a Prefix Language Model, we recommend setting \"attn_impl\" to \"flash\" instead of \"triton\".'))\n"
     ]
    }
   ],
   "source": [
    "# coding: utf8\n",
    "import torch\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_path = \"vinai/PhoGPT-4B-Chat\"  \n",
    "# model_path = \"vilm/vinallama-2.7b-chat\"\n",
    "# model_path = \"/home/andv/important/Chatbot/finetune_llm/phogpt-mergerd-with-config-1\"\n",
    "\n",
    "# model_path = \"/home/andv/important/Chatbot/finetune\"\n",
    "\n",
    "# config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)  \n",
    "# config.init_device = \"cuda\"\n",
    "# config.attn_config['attn_impl'] = 'flash' # If installed: this will use either Flash Attention V1 or V2 depending on what is installed\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path,\n",
    "                                            #  config=config, \n",
    "                                             torch_dtype=torch.bfloat16, \n",
    "                                             trust_remote_code=True).to(\"cuda\")\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_path, config = config, trust_remote_code=True).to(\"cuda\")\n",
    "# If your GPU does not support bfloat16:\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_path, config=config, torch_dtype=torch.float16, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(query:str):\n",
    "    results = retrieve_relevant_resources(query=query, n_resources_to_return=3, threshold=0.5)\n",
    "    input_prompt = prompt_formatter(query=query, results=results)\n",
    "    input_ids = tokenizer(input_prompt, return_tensors=\"pt\")  \n",
    "\n",
    "    outputs = model.generate(  \n",
    "        inputs=input_ids[\"input_ids\"].to(\"cuda\"),  \n",
    "        attention_mask=input_ids[\"attention_mask\"].to(\"cuda\"),  \n",
    "        do_sample=True,  \n",
    "        temperature=0.8,  \n",
    "        top_k=50,  \n",
    "        top_p=0.9,  \n",
    "        max_new_tokens=1024,  \n",
    "        eos_token_id=tokenizer.eos_token_id,  \n",
    "        pad_token_id=tokenizer.pad_token_id  \n",
    "    )  \n",
    "\n",
    "    response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]  \n",
    "    # response = tokenizer.decode(outputs[0])\n",
    "    # response = response.split(\"### Tr·∫£ l·ªùi:\")[1]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ask(\"N·ªôi dung d·ª± √°n 1 l√† g√¨, li·ªát k√™ c√°c n·ªôi dung ƒë·∫•y?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
