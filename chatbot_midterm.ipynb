{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models... \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import CrossEncoder, SentenceTransformer, util\n",
    "import json\n",
    "# from underthesea import sent_tokenize\n",
    "# from vinorm import TTSnorm\n",
    "# from TTS.tts.configs.xtts_config import XttsConfig\n",
    "# from TTS.tts.models.xtts import Xtts\n",
    "# import io\n",
    "# import soundfile as sf  # Thư viện xử lý âm thanh\n",
    "# from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "# from pydub import AudioSegment\n",
    "# from io import BytesIO\n",
    "# import librosa\n",
    "# from Xu_ly_text import Xu_ly_text, Xu_ly_text_de_doc\n",
    "# import re\n",
    "# from dotenv import load_dotenv\n",
    "# import os\n",
    "# from huggingface_hub import login\n",
    "\n",
    "# load_dotenv()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# # Get the Hugging Face access token from environment variables\n",
    "# hf_token = os.getenv(\"PROJECTCB1_HUGGINGFACE_ACCESS_TOKEN\")\n",
    "\n",
    "# # Log in to Hugging Face using the access token\n",
    "# if hf_token:\n",
    "#     login(token=hf_token)\n",
    "# else:\n",
    "#     print(\n",
    "#         \"Access token not found. Please set the HUGGINGFACE_ACCESS_TOKEN in your .env file.\"\n",
    "#     )\n",
    "\n",
    "eb_model_path = \"./Model/vnuis_embedding_bge\"\n",
    "embeddings_path = \"./Data/Embedding.csv\"\n",
    "\n",
    "\n",
    "# Hàm nội bộ\n",
    "def load_embedding_model(embedding_model_path):\n",
    "    embedding_model = SentenceTransformer(\n",
    "        model_name_or_path=embedding_model_path, device=device,\n",
    "        trust_remote_code= True\n",
    "    )\n",
    "    return embedding_model\n",
    "\n",
    "# def load_reranking_model(pr_model_path):\n",
    "#     pr_model = CrossEncoder(model_name=pr_model_path, device=device, trust_remote_code=True)\n",
    "#     return pr_model\n",
    "\n",
    "\n",
    "def load_embeddings(embeddings_path):\n",
    "    text_chunks_and_embedding_df = pd.read_csv(embeddings_path)\n",
    "    \n",
    "    # Convert the embedding column from a JSON string to a list of floats\n",
    "    text_chunks_and_embedding_df[\"embedding\"] = text_chunks_and_embedding_df[\"embedding\"].apply(json.loads)\n",
    "    \n",
    "    # Convert to PyTorch tensor\n",
    "    embeddings = torch.tensor(\n",
    "        np.array(text_chunks_and_embedding_df[\"embedding\"].tolist()),\n",
    "        dtype=torch.float32,\n",
    "    ).to(device)\n",
    "    \n",
    "    pages_and_chunks = text_chunks_and_embedding_df.to_dict(orient=\"records\")\n",
    "    return embeddings, pages_and_chunks\n",
    "\n",
    "\n",
    "# Khai báo các mô hình\n",
    "\n",
    "print(\"Loading models... \")\n",
    "# Load model embedding\n",
    "\n",
    "embedding_model = load_embedding_model(eb_model_path)\n",
    "\n",
    "# Load reranking\n",
    "# rr_model_path = \"itdainb/PhoRanker\"\n",
    "# reranking_model = load_reranking_model(rr_model_path)\n",
    "\n",
    "# Dowload TTS capleaf/viXTTS\n",
    "# from huggingface_hub import snapshot_download\n",
    "\n",
    "# snapshot_download(\n",
    "#     repo_id=\"capleaf/viXTTS\", repo_type=\"model\", local_dir=\"Model/TTS_model\"\n",
    "# )\n",
    "\n",
    "embeddings, pages_and_chunks = load_embeddings(embeddings_path)  # Load embeddings\n",
    "\n",
    "# Load model STT nguyenvulebinh/wav2vec2-base-vietnamese-250h\n",
    "\n",
    "# processor.save_pretrained(stt_model_path)\n",
    "# model.save_pretrained(stt_model_path)\n",
    "# Hàm sử dụng cho API\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_resources(query: str, n_resources_to_return: int = 3, threshold: int =0.1):\n",
    "    query_embedding = embedding_model.encode(query, convert_to_tensor=True)\n",
    "    # Use cosine similarity instead of dot score\n",
    "    cosine_scores = util.pytorch_cos_sim(query_embedding, embeddings)[0]\n",
    "    \n",
    "    # Get all scores and corresponding indices, then filter based on score > 0.5\n",
    "    scores, indices = torch.topk(input=cosine_scores, k=n_resources_to_return)\n",
    "    filtered_scores_indices = [(score.item(), index.item()) for score, index in zip(scores, indices) if score.item() > threshold]\n",
    "    \n",
    "    # Extract the scores and indices after filtering\n",
    "    filtered_indices = [index for _, index in filtered_scores_indices]\n",
    "    \n",
    "    # Take top 'n_resources_to_return' from the filtered list\n",
    "    # top_scores = filtered_scores[:n_resources_to_return]\n",
    "    top_indices = filtered_indices[:n_resources_to_return]\n",
    "    \n",
    "    context_items = [pages_and_chunks[i] for i in top_indices]\n",
    "    results = [item[\"Relevant docs\"] for item in context_items]\n",
    "    # ques = [item[\"Question\"] for item in context_items]\n",
    "    # pr_results = reranking_model.rank(query, results, return_documents=True, top_k=5)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import TextStreamer\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "model_path = \"SeaLLMs/SeaLLMs-v3-1.5B-Chat\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  model_path,\n",
    "  torch_dtype=torch.bfloat16, \n",
    "  device_map=device,\n",
    "  # load_in_4bit = True,\n",
    "  attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# prepare messages to model\n",
    "# Các đoạn văn có liên quan: <trích xuất các đoạn văn có liên quan từ ngữ cảnh tại đây>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to format the prompt\n",
    "def prompt_formatter_root(query: str, results: list) -> str:\n",
    "    context = '- \"' + '\"\\n\\n- \"'.join(results) + '\"'\n",
    "\n",
    "    base_prompt = \"\"\"Hãy cho bản thân không gian để suy nghĩ bằng cách trích xuất các đoạn văn có liên quan từ ngữ cảnh dưới đây trước khi trả lời câu hỏi của người dùng.\n",
    "Sử dụng các đoạn ngữ cảnh sau để trả lời câu hỏi của người dùng:\n",
    "\n",
    "{context}\n",
    "\n",
    "Đừng trả về cách suy nghĩ của bạn ví dụ như \"dựa vào như cảnh, trong ngữ cảnh cung cấp,...\". Trả về trực tiếp câu trả lời\n",
    "Câu hỏi của người dùng: {query}\n",
    "Trả lời:\"\"\"\n",
    "    prompt = base_prompt.format(context=context, query=query)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hãy cho bản thân không gian để suy nghĩ bằng cách trích xuất các đoạn văn có liên quan từ ngữ cảnh dưới đây trước khi trả lời câu hỏi của người dùng.\n",
      "Sử dụng các đoạn ngữ cảnh sau để trả lời câu hỏi của người dùng:\n",
      "\n",
      "- \"Cách tính điểm đánh giá bộ phận, điểm học phần theo thang điểm 10:\n",
      "\n",
      "Điểm đánh giá bộ phận và điểm thi kết thúc học phần được chấm theo thang điểm 10 (từ 0 đến 10), có lẻ đến một chữ số thập phân.\n",
      "\n",
      "Điểm học phần là tổng của điểm đánh giá bộ phận và điểm thi kết thúc học phần sau khi đã tính trọng số được quy định trong đề cương học phần và được làm tròn đến một chữ số thập phân, sau đó được chuyển thành điểm chữ.\n",
      "\n",
      "a) Loại đạt:\n",
      "\n",
      "9,0 – 10,0 tương ứng với A+\n",
      "8,5 – 8,9 tương ứng với A\n",
      "8,0 – 8,4 tương ứng với B+\n",
      "7,0 – 7,9 tương ứng với B\n",
      "6,5 – 6,9 tương ứng với C+\n",
      "5,5 – 6,4 tương ứng với C\n",
      "5,0 – 5,4 tương ứng với D+\n",
      "4,0 – 4,9 tương ứng với D\n",
      "b) Loại không đạt:\n",
      "\n",
      "Dưới 4,0 tương ứng với F\"\n",
      "\n",
      "- \"Hạng tốt nghiệp được xác định theo điểm trung bình chung tích lũy (GPA) của toàn khóa học, cụ thể như sau:\n",
      "\n",
      "a) Xuất sắc: Điểm trung bình chung tích lũy (GPA) từ 3,60 đến 4,00\n",
      "\n",
      "b) Giỏi: Điểm trung bình chung tích lũy (GPA) từ 3,20 đến 3,59\n",
      "\n",
      "c) Khá: Điểm trung bình chung tích lũy (GPA) từ 2,50 đến 3,19\n",
      "\n",
      "d) Trung bình: Điểm trung bình chung tích lũy (GPA) từ 2,00 đến 2,49.\"\n",
      "\n",
      "- \"Nguyên tắc xét học bổng của học bổng Tài năng:\n",
      "Học bổng thu hút nhân tài được xét, cấp vào đầu mỗi khóa học. \n",
      "Kết thúc mỗi học kì, kết quả học tập, thành tích nghiên cứu khoa học, rèn luyện được sử dụng làm căn cứ để xét duy trì học bổng cho học kì đó. \n",
      "Đối với giai đoạn học chương trình tiếng Anh dự bị (nếu có), sinh viên được xét học bổng căn cứ vào điểm ngoại ngữ thuộc chương trình tiếng Anh dự bị và điểm các học phần tiếng  Việt, quy theo thang điểm 4 và tính trung bình chung của 2 cấu phần điểm. Riêng  đối với sinh viên chương trình Liên kết quốc tế, điểm của sinh viên được xét dựa  theo điểm ngoại ngữ thuộc chương trình tiếng Anh dự bị. \n",
      "Sinh viên được thụ hưởng học bổng có trách nhiệm thực hiện đầy đủ, đúng hạn các nghĩa vụ tài chính và các nghĩa vụ khác với nhà trường\n",
      "\"\n",
      "\n",
      "Đừng trả về cách suy nghĩ của bạn ví dụ như \"dựa vào như cảnh, trong ngữ cảnh cung cấp,...\". Trả về trực tiếp câu trả lời\n",
      "Câu hỏi của người dùng: Bao nhiêu điểm thì đạt B\n",
      "Trả lời:\n"
     ]
    }
   ],
   "source": [
    "query = \"Bao nhiêu điểm thì đạt B\"\n",
    "results = retrieve_relevant_resources(query = query)\n",
    "print(prompt_formatter_root(query=query, results=results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xin chào\n",
      "Xin chào! Tôi là VNU-IS Chatbot, tôi sẽ cố gắng giúp bạn với bất kỳ câu hỏi nào bạn có. Bạn cần hỗ trợ gì ạ?\n",
      "bạn có thể làm những gì ?\n",
      "ISVC đã tổ chức các sự kiện như \"Tết Sum vầy\", \"Red Day\", \"ECOFISH\", \"Tọa đàm Thay đổi lối sống\", \"Reply 2010\", \"I Can't, We Can\" và \"Vầng Trăng Cho Em\". ISVC cũng tổ chức các hoạt động như \"Cơ hội nghề nghiệp\", \"Ngôn ngữ Anh\" và \"Định hướng đối ngoại\".\n",
      "\n",
      "Về câu hỏi của bạn, tôi sẽ trả lời như sau:\n",
      "\n",
      "- Chào bạn, tôi là VNU-IS Chatbot, tôi sẵn sàng hỗ trợ bạn với mọi nhu cầu của bạn. Bạn cần hỗ trợ gì?\n"
     ]
    }
   ],
   "source": [
    "# def ask(query:str) -> str:\n",
    "# messages = [\n",
    "# {\"role\": \"system\", \"content\": \"Bạn là một trợ lí Tiếng Việt hữu ích. Hãy trả lời câu hỏi của người dùng một cách chính xác.\"},\n",
    "# ]\n",
    "messages = [\n",
    "{\"role\": \"system\", \"content\": \"Bạn là Chatbot của Trường Quốc Tế - Đại học Quốc Gia Hà Nội. Hãy trả lời câu hỏi của người dùng một cách chính xác.\"},\n",
    "]\n",
    "# query = \"Đó là những dự án nào vậy?\"\n",
    "lastest_conversation = []\n",
    "while True:\n",
    "    query = input(\"Nhập câu hỏi: \")\n",
    "    print(query)\n",
    "    results = retrieve_relevant_resources(query, n_resources_to_return=2, threshold = 0.3)\n",
    "    prompt = prompt_formatter_root(query, results)\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "    generated_ids = model.generate(model_inputs.input_ids, \n",
    "                                do_sample=True,\n",
    "                                temperature=0.1,  \n",
    "                                    top_k=20,  \n",
    "                                    top_p=0.95,  \n",
    "                                    max_new_tokens=512,\n",
    "                                    repetition_penalty = 1.05,  \n",
    "                                streamer=streamer)\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    # old_messages.append({\"role\": \"user\", \"content\": query})\n",
    "    # old_messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "    lastest_conversation.clear()\n",
    "    lastest_conversation.extend([query, response])\n",
    "    messages.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-10-31 19:48:24.675 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-31 19:48:24.675 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-31 19:48:24.702 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/ipykernel_launcher.py [ARGUMENTS]\n",
      "2024-10-31 19:48:24.703 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-31 19:48:24.703 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-31 19:48:24.704 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-31 19:48:24.704 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-31 19:48:24.704 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-31 19:48:24.704 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-31 19:48:24.705 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-31 19:48:24.705 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-31 19:48:24.705 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-31 19:48:24.705 Session state does not function when running a script without `streamlit run`\n",
      "2024-10-31 19:48:24.705 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-31 19:48:24.705 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-31 19:48:24.706 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-31 19:48:24.706 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-31 19:48:24.706 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-31 19:48:24.706 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-31 19:48:24.706 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "# import streamlit as st\n",
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "\n",
    "# # Initialization\n",
    "# st.set_page_config(page_title=\"Vietnamese Chatbot\", page_icon=\"💬\")\n",
    "# st.title(\"Vietnamese Chatbot\")\n",
    "\n",
    "# # Define initial system message\n",
    "# messages = [\n",
    "#     {\"role\": \"system\", \"content\": \"Bạn là một trợ lí Tiếng Việt hữu ích. Hãy trả lời câu hỏi của người dùng một cách chính xác.\"},\n",
    "# ]\n",
    "# lastest_conversation = []\n",
    "\n",
    "# # Streamlit UI\n",
    "# def main():\n",
    "#     st.write(\"### Hỏi và Đáp\")\n",
    "#     query = st.text_input(\"Nhập câu hỏi:\")\n",
    "\n",
    "#     if st.button(\"Gửi\") and query:\n",
    "#         st.write(f\"**Bạn hỏi:** {query}\")\n",
    "#         response = get_response(query)\n",
    "#         st.write(f\"**Trợ lý:** {response}\")\n",
    "\n",
    "\n",
    "# def get_response(query):\n",
    "#     # Update conversation context\n",
    "#     global lastest_conversation, messages\n",
    "#     if lastest_conversation:\n",
    "#         query = rewrite_query(query=query, lastest_conversation=lastest_conversation)\n",
    "\n",
    "#     results = retrieve_relevant_resources(query, n_resources_to_return=5, threshold=0.5)\n",
    "#     prompt = prompt_formatter_root(query, results)\n",
    "#     messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    \n",
    "#     # Generate the response\n",
    "#     text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "#     model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "#     streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "#     generated_ids = model.generate(\n",
    "#         model_inputs.input_ids,\n",
    "#         do_sample=True,\n",
    "#         temperature=0.1,\n",
    "#         top_k=40,\n",
    "#         top_p=0.95,\n",
    "#         max_new_tokens=1024,\n",
    "#         repetition_penalty=1.05,\n",
    "#         streamer=streamer\n",
    "#     )\n",
    "#     generated_ids = [\n",
    "#         output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "#     ]\n",
    "#     response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "#     # Update messages and latest conversation\n",
    "#     lastest_conversation.clear()\n",
    "#     lastest_conversation.extend([query, response])\n",
    "#     messages.pop()\n",
    "#     return response\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
