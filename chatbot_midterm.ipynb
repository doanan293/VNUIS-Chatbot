{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models... \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import CrossEncoder, SentenceTransformer, util\n",
    "import json\n",
    "# from underthesea import sent_tokenize\n",
    "# from vinorm import TTSnorm\n",
    "# from TTS.tts.configs.xtts_config import XttsConfig\n",
    "# from TTS.tts.models.xtts import Xtts\n",
    "# import io\n",
    "# import soundfile as sf  # Th∆∞ vi·ªán x·ª≠ l√Ω √¢m thanh\n",
    "# from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "# from pydub import AudioSegment\n",
    "# from io import BytesIO\n",
    "# import librosa\n",
    "# from Xu_ly_text import Xu_ly_text, Xu_ly_text_de_doc\n",
    "# import re\n",
    "# from dotenv import load_dotenv\n",
    "# import os\n",
    "# from huggingface_hub import login\n",
    "\n",
    "# load_dotenv()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# # Get the Hugging Face access token from environment variables\n",
    "# hf_token = os.getenv(\"PROJECTCB1_HUGGINGFACE_ACCESS_TOKEN\")\n",
    "\n",
    "# # Log in to Hugging Face using the access token\n",
    "# if hf_token:\n",
    "#     login(token=hf_token)\n",
    "# else:\n",
    "#     print(\n",
    "#         \"Access token not found. Please set the HUGGINGFACE_ACCESS_TOKEN in your .env file.\"\n",
    "#     )\n",
    "\n",
    "eb_model_path = \"./Model/vnuis_embedding_bge\"\n",
    "embeddings_path = \"./Data/Embedding.csv\"\n",
    "\n",
    "\n",
    "# H√†m n·ªôi b·ªô\n",
    "def load_embedding_model(embedding_model_path):\n",
    "    embedding_model = SentenceTransformer(\n",
    "        model_name_or_path=embedding_model_path, device=device,\n",
    "        trust_remote_code= True\n",
    "    )\n",
    "    return embedding_model\n",
    "\n",
    "# def load_reranking_model(pr_model_path):\n",
    "#     pr_model = CrossEncoder(model_name=pr_model_path, device=device, trust_remote_code=True)\n",
    "#     return pr_model\n",
    "\n",
    "\n",
    "def load_embeddings(embeddings_path):\n",
    "    text_chunks_and_embedding_df = pd.read_csv(embeddings_path)\n",
    "    \n",
    "    # Convert the embedding column from a JSON string to a list of floats\n",
    "    text_chunks_and_embedding_df[\"embedding\"] = text_chunks_and_embedding_df[\"embedding\"].apply(json.loads)\n",
    "    \n",
    "    # Convert to PyTorch tensor\n",
    "    embeddings = torch.tensor(\n",
    "        np.array(text_chunks_and_embedding_df[\"embedding\"].tolist()),\n",
    "        dtype=torch.float32,\n",
    "    ).to(device)\n",
    "    \n",
    "    pages_and_chunks = text_chunks_and_embedding_df.to_dict(orient=\"records\")\n",
    "    return embeddings, pages_and_chunks\n",
    "\n",
    "\n",
    "# Khai b√°o c√°c m√¥ h√¨nh\n",
    "\n",
    "print(\"Loading models... \")\n",
    "# Load model embedding\n",
    "\n",
    "embedding_model = load_embedding_model(eb_model_path)\n",
    "\n",
    "# Load reranking\n",
    "# rr_model_path = \"itdainb/PhoRanker\"\n",
    "# reranking_model = load_reranking_model(rr_model_path)\n",
    "\n",
    "# Dowload TTS capleaf/viXTTS\n",
    "# from huggingface_hub import snapshot_download\n",
    "\n",
    "# snapshot_download(\n",
    "#     repo_id=\"capleaf/viXTTS\", repo_type=\"model\", local_dir=\"Model/TTS_model\"\n",
    "# )\n",
    "\n",
    "embeddings, pages_and_chunks = load_embeddings(embeddings_path)  # Load embeddings\n",
    "\n",
    "# Load model STT nguyenvulebinh/wav2vec2-base-vietnamese-250h\n",
    "\n",
    "# processor.save_pretrained(stt_model_path)\n",
    "# model.save_pretrained(stt_model_path)\n",
    "# H√†m s·ª≠ d·ª•ng cho API\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_resources(query: str, n_resources_to_return: int = 3, threshold: int =0.1):\n",
    "    query_embedding = embedding_model.encode(query, convert_to_tensor=True)\n",
    "    # Use cosine similarity instead of dot score\n",
    "    cosine_scores = util.pytorch_cos_sim(query_embedding, embeddings)[0]\n",
    "    \n",
    "    # Get all scores and corresponding indices, then filter based on score > 0.5\n",
    "    scores, indices = torch.topk(input=cosine_scores, k=n_resources_to_return)\n",
    "    filtered_scores_indices = [(score.item(), index.item()) for score, index in zip(scores, indices) if score.item() > threshold]\n",
    "    \n",
    "    # Extract the scores and indices after filtering\n",
    "    filtered_indices = [index for _, index in filtered_scores_indices]\n",
    "    \n",
    "    # Take top 'n_resources_to_return' from the filtered list\n",
    "    # top_scores = filtered_scores[:n_resources_to_return]\n",
    "    top_indices = filtered_indices[:n_resources_to_return]\n",
    "    \n",
    "    context_items = [pages_and_chunks[i] for i in top_indices]\n",
    "    results = [item[\"Relevant docs\"] for item in context_items]\n",
    "    # ques = [item[\"Question\"] for item in context_items]\n",
    "    # pr_results = reranking_model.rank(query, results, return_documents=True, top_k=5)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import TextStreamer\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "model_path = \"SeaLLMs/SeaLLMs-v3-1.5B-Chat\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  model_path,\n",
    "  torch_dtype=torch.bfloat16, \n",
    "  device_map=device,\n",
    "  # load_in_4bit = True,\n",
    "  attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# prepare messages to model\n",
    "# C√°c ƒëo·∫°n vƒÉn c√≥ li√™n quan: <tr√≠ch xu·∫•t c√°c ƒëo·∫°n vƒÉn c√≥ li√™n quan t·ª´ ng·ªØ c·∫£nh t·∫°i ƒë√¢y>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to format the prompt\n",
    "def prompt_formatter_root(query: str, results: list) -> str:\n",
    "    context = '- \"' + '\"\\n\\n- \"'.join(results) + '\"'\n",
    "\n",
    "    base_prompt = \"\"\"H√£y cho b·∫£n th√¢n kh√¥ng gian ƒë·ªÉ suy nghƒ© b·∫±ng c√°ch tr√≠ch xu·∫•t c√°c ƒëo·∫°n vƒÉn c√≥ li√™n quan t·ª´ ng·ªØ c·∫£nh d∆∞·ªõi ƒë√¢y tr∆∞·ªõc khi tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng.\n",
    "S·ª≠ d·ª•ng c√°c ƒëo·∫°n ng·ªØ c·∫£nh sau ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng:\n",
    "\n",
    "{context}\n",
    "\n",
    "ƒê·ª´ng tr·∫£ v·ªÅ c√°ch suy nghƒ© c·ªßa b·∫°n v√≠ d·ª• nh∆∞ \"d·ª±a v√†o nh∆∞ c·∫£nh, trong ng·ªØ c·∫£nh cung c·∫•p,...\". Tr·∫£ v·ªÅ tr·ª±c ti·∫øp c√¢u tr·∫£ l·ªùi\n",
    "C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng: {query}\n",
    "Tr·∫£ l·ªùi:\"\"\"\n",
    "    prompt = base_prompt.format(context=context, query=query)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H√£y cho b·∫£n th√¢n kh√¥ng gian ƒë·ªÉ suy nghƒ© b·∫±ng c√°ch tr√≠ch xu·∫•t c√°c ƒëo·∫°n vƒÉn c√≥ li√™n quan t·ª´ ng·ªØ c·∫£nh d∆∞·ªõi ƒë√¢y tr∆∞·ªõc khi tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng.\n",
      "S·ª≠ d·ª•ng c√°c ƒëo·∫°n ng·ªØ c·∫£nh sau ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng:\n",
      "\n",
      "- \"C√°ch t√≠nh ƒëi·ªÉm ƒë√°nh gi√° b·ªô ph·∫≠n, ƒëi·ªÉm h·ªçc ph·∫ßn theo thang ƒëi·ªÉm 10:\n",
      "\n",
      "ƒêi·ªÉm ƒë√°nh gi√° b·ªô ph·∫≠n v√† ƒëi·ªÉm thi k·∫øt th√∫c h·ªçc ph·∫ßn ƒë∆∞·ª£c ch·∫•m theo thang ƒëi·ªÉm 10 (t·ª´ 0 ƒë·∫øn 10), c√≥ l·∫ª ƒë·∫øn m·ªôt ch·ªØ s·ªë th·∫≠p ph√¢n.\n",
      "\n",
      "ƒêi·ªÉm h·ªçc ph·∫ßn l√† t·ªïng c·ªßa ƒëi·ªÉm ƒë√°nh gi√° b·ªô ph·∫≠n v√† ƒëi·ªÉm thi k·∫øt th√∫c h·ªçc ph·∫ßn sau khi ƒë√£ t√≠nh tr·ªçng s·ªë ƒë∆∞·ª£c quy ƒë·ªãnh trong ƒë·ªÅ c∆∞∆°ng h·ªçc ph·∫ßn v√† ƒë∆∞·ª£c l√†m tr√≤n ƒë·∫øn m·ªôt ch·ªØ s·ªë th·∫≠p ph√¢n, sau ƒë√≥ ƒë∆∞·ª£c chuy·ªÉn th√†nh ƒëi·ªÉm ch·ªØ.\n",
      "\n",
      "a) Lo·∫°i ƒë·∫°t:\n",
      "\n",
      "9,0 ‚Äì 10,0 t∆∞∆°ng ·ª©ng v·ªõi A+\n",
      "8,5 ‚Äì 8,9 t∆∞∆°ng ·ª©ng v·ªõi A\n",
      "8,0 ‚Äì 8,4 t∆∞∆°ng ·ª©ng v·ªõi B+\n",
      "7,0 ‚Äì 7,9 t∆∞∆°ng ·ª©ng v·ªõi B\n",
      "6,5 ‚Äì 6,9 t∆∞∆°ng ·ª©ng v·ªõi C+\n",
      "5,5 ‚Äì 6,4 t∆∞∆°ng ·ª©ng v·ªõi C\n",
      "5,0 ‚Äì 5,4 t∆∞∆°ng ·ª©ng v·ªõi D+\n",
      "4,0 ‚Äì 4,9 t∆∞∆°ng ·ª©ng v·ªõi D\n",
      "b) Lo·∫°i kh√¥ng ƒë·∫°t:\n",
      "\n",
      "D∆∞·ªõi 4,0 t∆∞∆°ng ·ª©ng v·ªõi F\"\n",
      "\n",
      "- \"H·∫°ng t·ªët nghi·ªáp ƒë∆∞·ª£c x√°c ƒë·ªãnh theo ƒëi·ªÉm trung b√¨nh chung t√≠ch l≈©y (GPA) c·ªßa to√†n kh√≥a h·ªçc, c·ª• th·ªÉ nh∆∞ sau:\n",
      "\n",
      "a) Xu·∫•t s·∫Øc: ƒêi·ªÉm trung b√¨nh chung t√≠ch l≈©y (GPA) t·ª´ 3,60 ƒë·∫øn 4,00\n",
      "\n",
      "b) Gi·ªèi: ƒêi·ªÉm trung b√¨nh chung t√≠ch l≈©y (GPA) t·ª´ 3,20 ƒë·∫øn 3,59\n",
      "\n",
      "c) Kh√°: ƒêi·ªÉm trung b√¨nh chung t√≠ch l≈©y (GPA) t·ª´ 2,50 ƒë·∫øn 3,19\n",
      "\n",
      "d) Trung b√¨nh: ƒêi·ªÉm trung b√¨nh chung t√≠ch l≈©y (GPA) t·ª´ 2,00 ƒë·∫øn 2,49.\"\n",
      "\n",
      "- \"Nguy√™n t·∫Øc x√©t h·ªçc b·ªïng c·ªßa h·ªçc b·ªïng T√†i nƒÉng:\n",
      "H·ªçc b·ªïng thu h√∫t nh√¢n t√†i ƒë∆∞·ª£c x√©t, c·∫•p v√†o ƒë·∫ßu m·ªói kh√≥a h·ªçc. \n",
      "K·∫øt th√∫c m·ªói h·ªçc k√¨, k·∫øt qu·∫£ h·ªçc t·∫≠p, th√†nh t√≠ch nghi√™n c·ª©u khoa h·ªçc, r√®n luy·ªán ƒë∆∞·ª£c s·ª≠ d·ª•ng l√†m cƒÉn c·ª© ƒë·ªÉ x√©t duy tr√¨ h·ªçc b·ªïng cho h·ªçc k√¨ ƒë√≥. \n",
      "ƒê·ªëi v·ªõi giai ƒëo·∫°n h·ªçc ch∆∞∆°ng tr√¨nh ti·∫øng Anh d·ª± b·ªã (n·∫øu c√≥), sinh vi√™n ƒë∆∞·ª£c x√©t h·ªçc b·ªïng cƒÉn c·ª© v√†o ƒëi·ªÉm ngo·∫°i ng·ªØ thu·ªôc ch∆∞∆°ng tr√¨nh ti·∫øng Anh d·ª± b·ªã v√† ƒëi·ªÉm c√°c h·ªçc ph·∫ßn ti·∫øng  Vi·ªát, quy theo thang ƒëi·ªÉm 4 v√† t√≠nh trung b√¨nh chung c·ªßa 2 c·∫•u ph·∫ßn ƒëi·ªÉm. Ri√™ng  ƒë·ªëi v·ªõi sinh vi√™n ch∆∞∆°ng tr√¨nh Li√™n k·∫øt qu·ªëc t·∫ø, ƒëi·ªÉm c·ªßa sinh vi√™n ƒë∆∞·ª£c x√©t d·ª±a  theo ƒëi·ªÉm ngo·∫°i ng·ªØ thu·ªôc ch∆∞∆°ng tr√¨nh ti·∫øng Anh d·ª± b·ªã. \n",
      "Sinh vi√™n ƒë∆∞·ª£c th·ª• h∆∞·ªüng h·ªçc b·ªïng c√≥ tr√°ch nhi·ªám th·ª±c hi·ªán ƒë·∫ßy ƒë·ªß, ƒë√∫ng h·∫°n c√°c nghƒ©a v·ª• t√†i ch√≠nh v√† c√°c nghƒ©a v·ª• kh√°c v·ªõi nh√† tr∆∞·ªùng\n",
      "\"\n",
      "\n",
      "ƒê·ª´ng tr·∫£ v·ªÅ c√°ch suy nghƒ© c·ªßa b·∫°n v√≠ d·ª• nh∆∞ \"d·ª±a v√†o nh∆∞ c·∫£nh, trong ng·ªØ c·∫£nh cung c·∫•p,...\". Tr·∫£ v·ªÅ tr·ª±c ti·∫øp c√¢u tr·∫£ l·ªùi\n",
      "C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng: Bao nhi√™u ƒëi·ªÉm th√¨ ƒë·∫°t B\n",
      "Tr·∫£ l·ªùi:\n"
     ]
    }
   ],
   "source": [
    "query = \"Bao nhi√™u ƒëi·ªÉm th√¨ ƒë·∫°t B\"\n",
    "results = retrieve_relevant_resources(query = query)\n",
    "print(prompt_formatter_root(query=query, results=results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xin ch√†o\n",
      "Xin ch√†o! T√¥i l√† VNU-IS Chatbot, t√¥i s·∫Ω c·ªë g·∫Øng gi√∫p b·∫°n v·ªõi b·∫•t k·ª≥ c√¢u h·ªèi n√†o b·∫°n c√≥. B·∫°n c·∫ßn h·ªó tr·ª£ g√¨ ·∫°?\n",
      "b·∫°n c√≥ th·ªÉ l√†m nh·ªØng g√¨ ?\n",
      "ISVC ƒë√£ t·ªï ch·ª©c c√°c s·ª± ki·ªán nh∆∞ \"T·∫øt Sum v·∫ßy\", \"Red Day\", \"ECOFISH\", \"T·ªça ƒë√†m Thay ƒë·ªïi l·ªëi s·ªëng\", \"Reply 2010\", \"I Can't, We Can\" v√† \"V·∫ßng TrƒÉng Cho Em\". ISVC c≈©ng t·ªï ch·ª©c c√°c ho·∫°t ƒë·ªông nh∆∞ \"C∆° h·ªôi ngh·ªÅ nghi·ªáp\", \"Ng√¥n ng·ªØ Anh\" v√† \"ƒê·ªãnh h∆∞·ªõng ƒë·ªëi ngo·∫°i\".\n",
      "\n",
      "V·ªÅ c√¢u h·ªèi c·ªßa b·∫°n, t√¥i s·∫Ω tr·∫£ l·ªùi nh∆∞ sau:\n",
      "\n",
      "- Ch√†o b·∫°n, t√¥i l√† VNU-IS Chatbot, t√¥i s·∫µn s√†ng h·ªó tr·ª£ b·∫°n v·ªõi m·ªçi nhu c·∫ßu c·ªßa b·∫°n. B·∫°n c·∫ßn h·ªó tr·ª£ g√¨?\n"
     ]
    }
   ],
   "source": [
    "# def ask(query:str) -> str:\n",
    "# messages = [\n",
    "# {\"role\": \"system\", \"content\": \"B·∫°n l√† m·ªôt tr·ª£ l√≠ Ti·∫øng Vi·ªát h·ªØu √≠ch. H√£y tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng m·ªôt c√°ch ch√≠nh x√°c.\"},\n",
    "# ]\n",
    "messages = [\n",
    "{\"role\": \"system\", \"content\": \"B·∫°n l√† Chatbot c·ªßa Tr∆∞·ªùng Qu·ªëc T·∫ø - ƒê·∫°i h·ªçc Qu·ªëc Gia H√† N·ªôi. H√£y tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng m·ªôt c√°ch ch√≠nh x√°c.\"},\n",
    "]\n",
    "# query = \"ƒê√≥ l√† nh·ªØng d·ª± √°n n√†o v·∫≠y?\"\n",
    "lastest_conversation = []\n",
    "while True:\n",
    "    query = input(\"Nh·∫≠p c√¢u h·ªèi: \")\n",
    "    print(query)\n",
    "    results = retrieve_relevant_resources(query, n_resources_to_return=2, threshold = 0.3)\n",
    "    prompt = prompt_formatter_root(query, results)\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "    generated_ids = model.generate(model_inputs.input_ids, \n",
    "                                do_sample=True,\n",
    "                                temperature=0.1,  \n",
    "                                    top_k=20,  \n",
    "                                    top_p=0.95,  \n",
    "                                    max_new_tokens=512,\n",
    "                                    repetition_penalty = 1.05,  \n",
    "                                streamer=streamer)\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    # old_messages.append({\"role\": \"user\", \"content\": query})\n",
    "    # old_messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "    lastest_conversation.clear()\n",
    "    lastest_conversation.extend([query, response])\n",
    "    messages.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-10-31 19:48:24.675 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-31 19:48:24.675 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-31 19:48:24.702 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/ipykernel_launcher.py [ARGUMENTS]\n",
      "2024-10-31 19:48:24.703 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-31 19:48:24.703 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-31 19:48:24.704 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-31 19:48:24.704 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-31 19:48:24.704 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-31 19:48:24.704 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-31 19:48:24.705 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-31 19:48:24.705 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-31 19:48:24.705 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-31 19:48:24.705 Session state does not function when running a script without `streamlit run`\n",
      "2024-10-31 19:48:24.705 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-31 19:48:24.705 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-31 19:48:24.706 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-31 19:48:24.706 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-31 19:48:24.706 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-31 19:48:24.706 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-31 19:48:24.706 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "# import streamlit as st\n",
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "\n",
    "# # Initialization\n",
    "# st.set_page_config(page_title=\"Vietnamese Chatbot\", page_icon=\"üí¨\")\n",
    "# st.title(\"Vietnamese Chatbot\")\n",
    "\n",
    "# # Define initial system message\n",
    "# messages = [\n",
    "#     {\"role\": \"system\", \"content\": \"B·∫°n l√† m·ªôt tr·ª£ l√≠ Ti·∫øng Vi·ªát h·ªØu √≠ch. H√£y tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng m·ªôt c√°ch ch√≠nh x√°c.\"},\n",
    "# ]\n",
    "# lastest_conversation = []\n",
    "\n",
    "# # Streamlit UI\n",
    "# def main():\n",
    "#     st.write(\"### H·ªèi v√† ƒê√°p\")\n",
    "#     query = st.text_input(\"Nh·∫≠p c√¢u h·ªèi:\")\n",
    "\n",
    "#     if st.button(\"G·ª≠i\") and query:\n",
    "#         st.write(f\"**B·∫°n h·ªèi:** {query}\")\n",
    "#         response = get_response(query)\n",
    "#         st.write(f\"**Tr·ª£ l√Ω:** {response}\")\n",
    "\n",
    "\n",
    "# def get_response(query):\n",
    "#     # Update conversation context\n",
    "#     global lastest_conversation, messages\n",
    "#     if lastest_conversation:\n",
    "#         query = rewrite_query(query=query, lastest_conversation=lastest_conversation)\n",
    "\n",
    "#     results = retrieve_relevant_resources(query, n_resources_to_return=5, threshold=0.5)\n",
    "#     prompt = prompt_formatter_root(query, results)\n",
    "#     messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    \n",
    "#     # Generate the response\n",
    "#     text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "#     model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "#     streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "#     generated_ids = model.generate(\n",
    "#         model_inputs.input_ids,\n",
    "#         do_sample=True,\n",
    "#         temperature=0.1,\n",
    "#         top_k=40,\n",
    "#         top_p=0.95,\n",
    "#         max_new_tokens=1024,\n",
    "#         repetition_penalty=1.05,\n",
    "#         streamer=streamer\n",
    "#     )\n",
    "#     generated_ids = [\n",
    "#         output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "#     ]\n",
    "#     response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "#     # Update messages and latest conversation\n",
    "#     lastest_conversation.clear()\n",
    "#     lastest_conversation.extend([query, response])\n",
    "#     messages.pop()\n",
    "#     return response\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
