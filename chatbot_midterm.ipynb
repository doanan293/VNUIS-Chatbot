{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models... \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import CrossEncoder, SentenceTransformer, util\n",
    "import json\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# # Get the Hugging Face access token from environment variables\n",
    "# hf_token = os.getenv(\"PROJECTCB1_HUGGINGFACE_ACCESS_TOKEN\")\n",
    "\n",
    "# # Log in to Hugging Face using the access token\n",
    "# if hf_token:\n",
    "#     login(token=hf_token)\n",
    "# else:\n",
    "#     print(\n",
    "#         \"Access token not found. Please set the HUGGINGFACE_ACCESS_TOKEN in your .env file.\"\n",
    "#     )\n",
    "\n",
    "eb_model_path = \"./Model/vnuis_embedding_bge_final\"\n",
    "embeddings_path = \"./Data/Embedding.csv\"\n",
    "\n",
    "\n",
    "# Hàm nội bộ\n",
    "def load_embedding_model(embedding_model_path):\n",
    "    embedding_model = SentenceTransformer(\n",
    "        model_name_or_path=embedding_model_path, \n",
    "        device=device,\n",
    "        # model_kwargs={\"torch_dtype\": \"bfloat16\"},\n",
    "        trust_remote_code= True\n",
    "    )\n",
    "    return embedding_model\n",
    "\n",
    "# def load_reranking_model(pr_model_path):\n",
    "#     pr_model = CrossEncoder(model_name=pr_model_path, device=device, trust_remote_code=True)\n",
    "#     return pr_model\n",
    "\n",
    "\n",
    "def load_embeddings(embeddings_path):\n",
    "    text_chunks_and_embedding_df = pd.read_csv(embeddings_path)\n",
    "    \n",
    "    # Convert the embedding column from a JSON string to a list of floats\n",
    "    text_chunks_and_embedding_df[\"embedding\"] = text_chunks_and_embedding_df[\"embedding\"].apply(json.loads)\n",
    "    \n",
    "    # Convert to PyTorch tensor\n",
    "    embeddings = torch.tensor(\n",
    "        np.array(text_chunks_and_embedding_df[\"embedding\"].tolist()),\n",
    "        dtype=torch.float32,\n",
    "    ).to(device)\n",
    "    \n",
    "    pages_and_chunks = text_chunks_and_embedding_df.to_dict(orient=\"records\")\n",
    "    return embeddings, pages_and_chunks\n",
    "\n",
    "\n",
    "# Khai báo các mô hình\n",
    "\n",
    "print(\"Loading models... \")\n",
    "# Load model embedding\n",
    "\n",
    "embedding_model = load_embedding_model(eb_model_path)\n",
    "\n",
    "# Load reranking\n",
    "# rr_model_path = \"itdainb/PhoRanker\"\n",
    "# reranking_model = load_reranking_model(rr_model_path)\n",
    "\n",
    "# Dowload TTS capleaf/viXTTS\n",
    "# from huggingface_hub import snapshot_download\n",
    "\n",
    "# snapshot_download(\n",
    "#     repo_id=\"capleaf/viXTTS\", repo_type=\"model\", local_dir=\"Model/TTS_model\"\n",
    "# )\n",
    "\n",
    "embeddings, pages_and_chunks = load_embeddings(embeddings_path)  # Load embeddings\n",
    "\n",
    "# Load model STT nguyenvulebinh/wav2vec2-base-vietnamese-250h\n",
    "\n",
    "# processor.save_pretrained(stt_model_path)\n",
    "# model.save_pretrained(stt_model_path)\n",
    "# Hàm sử dụng cho API\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_resources(query: str, n_resources_to_return: int = 3, threshold: int =0.1):\n",
    "    query_embedding = embedding_model.encode(query, convert_to_tensor=True)\n",
    "    # Use cosine similarity instead of dot score\n",
    "    cosine_scores = util.pytorch_cos_sim(query_embedding, embeddings)[0]\n",
    "    \n",
    "    # Get all scores and corresponding indices, then filter based on score > 0.5\n",
    "    scores, indices = torch.topk(input=cosine_scores, k=n_resources_to_return)\n",
    "    filtered_scores_indices = [(score.item(), index.item()) for score, index in zip(scores, indices) if score.item() > threshold]\n",
    "    \n",
    "    # Extract the scores and indices after filtering\n",
    "    filtered_indices = [index for _, index in filtered_scores_indices]\n",
    "    \n",
    "    # Take top 'n_resources_to_return' from the filtered list\n",
    "    # top_scores = filtered_scores[:n_resources_to_return]\n",
    "    top_indices = filtered_indices[:n_resources_to_return]\n",
    "    \n",
    "    context_items = [pages_and_chunks[i] for i in top_indices]\n",
    "    results = [item[\"Relevant docs\"] for item in context_items]\n",
    "    # ques = [item[\"Question\"] for item in context_items]\n",
    "    # pr_results = reranking_model.rank(query, results, return_documents=True, top_k=5)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68871f5869a1496589cb19a85cd3e730",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import TextStreamer\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "model_path = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  model_path,\n",
    "  torch_dtype=torch.bfloat16, \n",
    "  device_map=device,\n",
    "  # load_in_4bit = True,\n",
    "  attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# prepare messages to model\n",
    "# Các đoạn văn có liên quan: <trích xuất các đoạn văn có liên quan từ ngữ cảnh tại đây>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to format the prompt\n",
    "def prompt_formatter_root(query: str, results: list) -> str:\n",
    "    context = '- \"' + '\"\\n\\n- \"'.join(results) + '\"'\n",
    "\n",
    "    base_prompt = \"\"\"Hãy cho bản thân không gian để suy nghĩ bằng cách trích xuất các đoạn văn có liên quan từ ngữ cảnh dưới đây trước khi trả lời câu hỏi của người dùng.\n",
    "Sử dụng các đoạn ngữ cảnh sau để trả lời câu hỏi của người dùng:\n",
    "\n",
    "{context}\n",
    "\n",
    "Câu hỏi của người dùng: {query}\n",
    "Không sử dung các câu dẫn dắt, hãy trả về trực tiếp câu trả lời ngắn gọn, súc tích.\n",
    "Trả lời:\"\"\"\n",
    "    prompt = base_prompt.format(context=context, query=query)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ask(query:str) -> str:\n",
    "# messages = [\n",
    "# {\"role\": \"system\", \"content\": \"Bạn là một trợ lí Tiếng Việt hữu ích. Hãy trả lời câu hỏi của người dùng một cách chính xác.\"},\n",
    "# ]\n",
    "\n",
    "def ask(query:str)->str:\n",
    "    messages = [{\"role\": \"system\", \"content\": \"\"\"Tôi là Chatbot của Trường Quốc Tế - Đại học Quốc Gia Hà Nội. Hãy trả lời câu hỏi của người dùng một cách chính xác.\"\"\",}]\n",
    "    print(f\"Câu hỏi của người dùng:{query}\" )\n",
    "    results = retrieve_relevant_resources(query, n_resources_to_return=5, threshold = 0.1)\n",
    "    prompt = prompt_formatter_root(query, results)\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "    generated_ids = model.generate(model_inputs.input_ids, \n",
    "                                do_sample=True,\n",
    "                                temperature=0.1,  \n",
    "                                    top_k=40,  \n",
    "                                    top_p=0.95,  \n",
    "                                    max_new_tokens=512,\n",
    "                                    repetition_penalty = 1.05,  \n",
    "                                streamer=streamer)\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    # old_messages.append({\"role\": \"user\", \"content\": query})\n",
    "    # old_messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hãy cho bản thân không gian để suy nghĩ bằng cách trích xuất các đoạn văn có liên quan từ ngữ cảnh dưới đây trước khi trả lời câu hỏi của người dùng.\n",
      "Sử dụng các đoạn ngữ cảnh sau để trả lời câu hỏi của người dùng:\n",
      "\n",
      "- \"Cách tính điểm đánh giá bộ phận, điểm học phần theo thang điểm 10:\n",
      "a) Loại đạt:\n",
      "\n",
      "9,0 – 10,0 tương ứng với A+\n",
      "8,5 – 8,9 tương ứng với A\n",
      "8,0 – 8,4 tương ứng với B+\n",
      "7,0 – 7,9 tương ứng với B\n",
      "6,5 – 6,9 tương ứng với C+\n",
      "5,5 – 6,4 tương ứng với C\n",
      "5,0 – 5,4 tương ứng với D+\n",
      "4,0 – 4,9 tương ứng với D\n",
      "b) Loại không đạt:\n",
      "\n",
      "Dưới 4,0 tương ứng với F\"\n",
      "\n",
      "- \"Bao nhiêu điểm thì đạt D? 4,0 – 4,9 tương ứng với D theo thang điểm 10\"\n",
      "\n",
      "Câu hỏi của người dùng: Cách tính điểm đánh giá theo thang điểm 10?\n",
      "Không sử dung các câu dẫn dắt, hãy trả về trực tiếp câu trả lời ngắn gọn, súc tích.\n",
      "Trả lời:\n"
     ]
    }
   ],
   "source": [
    "query = \"Cách tính điểm đánh giá theo thang điểm 10?\"\n",
    "results = retrieve_relevant_resources(query, n_resources_to_return=2, threshold = 0.1)\n",
    "print(prompt_formatter_root(query=query, results=results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Câu hỏi của người dùng:Cách tính điểm đánh giá theo thang điểm 10?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Điểm đạt A+: 9,0 - 10,0  \n",
      "Điểm đạt A: 8,5 - 8,9  \n",
      "Điểm đạt B+: 8,0 - 8,4  \n",
      "Điểm đạt B: 7,0 - 7,9  \n",
      "Điểm đạt C+: 5,5 - 6,4  \n",
      "Điểm đạt C: 5,0 - 5,4  \n",
      "Điểm đạt D+: 4,0 - 4,9  \n",
      "Điểm đạt D: 4,0 - 4,9  \n",
      "Điểm không đạt: Dưới 4,0\n"
     ]
    }
   ],
   "source": [
    "response = ask(query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Điểm đạt A+: 9,0 - 10,0  \n",
      "Điểm đạt A: 8,5 - 8,9  \n",
      "Điểm đạt B+: 8,0 - 8,4  \n",
      "Điểm đạt B: 7,0 - 7,9  \n",
      "Điểm đạt C+: 5,5 - 6,4  \n",
      "Điểm đạt C: 5,0 - 5,4  \n",
      "Điểm đạt D+: 4,0 - 4,9  \n",
      "Điểm đạt D: 4,0 - 4,9  \n",
      "Điểm không đạt: Dưới 4,0\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import streamlit as st\n",
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "\n",
    "# # Initialization\n",
    "# st.set_page_config(page_title=\"Vietnamese Chatbot\", page_icon=\"💬\")\n",
    "# st.title(\"Vietnamese Chatbot\")\n",
    "\n",
    "# # Define initial system message\n",
    "# messages = [\n",
    "#     {\"role\": \"system\", \"content\": \"Bạn là một trợ lí Tiếng Việt hữu ích. Hãy trả lời câu hỏi của người dùng một cách chính xác.\"},\n",
    "# ]\n",
    "# lastest_conversation = []\n",
    "\n",
    "# # Streamlit UI\n",
    "# def main():\n",
    "#     st.write(\"### Hỏi và Đáp\")\n",
    "#     query = st.text_input(\"Nhập câu hỏi:\")\n",
    "\n",
    "#     if st.button(\"Gửi\") and query:\n",
    "#         st.write(f\"**Bạn hỏi:** {query}\")\n",
    "#         response = get_response(query)\n",
    "#         st.write(f\"**Trợ lý:** {response}\")\n",
    "\n",
    "\n",
    "# def get_response(query):\n",
    "#     # Update conversation context\n",
    "#     global lastest_conversation, messages\n",
    "#     if lastest_conversation:\n",
    "#         query = rewrite_query(query=query, lastest_conversation=lastest_conversation)\n",
    "\n",
    "#     results = retrieve_relevant_resources(query, n_resources_to_return=5, threshold=0.5)\n",
    "#     prompt = prompt_formatter_root(query, results)\n",
    "#     messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    \n",
    "#     # Generate the response\n",
    "#     text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "#     model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "#     streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "#     generated_ids = model.generate(\n",
    "#         model_inputs.input_ids,\n",
    "#         do_sample=True,\n",
    "#         temperature=0.1,\n",
    "#         top_k=40,\n",
    "#         top_p=0.95,\n",
    "#         max_new_tokens=1024,\n",
    "#         repetition_penalty=1.05,\n",
    "#         streamer=streamer\n",
    "#     )\n",
    "#     generated_ids = [\n",
    "#         output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "#     ]\n",
    "#     response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "#     # Update messages and latest conversation\n",
    "#     lastest_conversation.clear()\n",
    "#     lastest_conversation.extend([query, response])\n",
    "#     messages.pop()\n",
    "#     return response\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
