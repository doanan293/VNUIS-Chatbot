{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andv/important/chatbot_rag/.venv/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andv/important/chatbot_rag/.venv/src/tts/TTS/utils/io.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(f, map_location=map_location, **kwargs)\n",
      "GPT2InferenceModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-27 16:53:28,037] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-27 16:53:28,701] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.15.2, git-hash=unknown, git-branch=unknown\n",
      "[2024-10-27 16:53:28,702] [WARNING] [config_utils.py:70:_process_deprecated_field] Config parameter replace_method is deprecated. This parameter is no longer needed, please remove from your call to DeepSpeed-inference\n",
      "[2024-10-27 16:53:28,702] [WARNING] [config_utils.py:70:_process_deprecated_field] Config parameter mp_size is deprecated use tensor_parallel.tp_size instead\n",
      "[2024-10-27 16:53:28,702] [INFO] [logging.py:96:log_dist] [Rank -1] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1\n",
      "[2024-10-27 16:53:28,797] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed-Inference config: {'layer_id': 0, 'hidden_size': 1024, 'intermediate_size': 4096, 'heads': 16, 'num_hidden_layers': -1, 'dtype': torch.float32, 'pre_layer_norm': True, 'norm_type': <NormType.LayerNorm: 1>, 'local_rank': -1, 'stochastic_mode': False, 'epsilon': 1e-05, 'mp_size': 1, 'scale_attention': True, 'triangular_masking': True, 'local_attention': False, 'window_size': 1, 'rotary_dim': -1, 'rotate_half': False, 'rotate_every_two': True, 'return_tuple': True, 'mlp_after_attn': True, 'mlp_act_func_type': <ActivationFuncType.GELU: 1>, 'training_mp_size': 1, 'bigscience_bloom': False, 'max_out_tokens': 1024, 'min_out_tokens': 1, 'scale_attn_by_inverse_layer_idx': False, 'enable_qkv_quantization': False, 'use_mup': False, 'return_single_tuple': False, 'set_empty_params': False, 'transposed_mode': False, 'use_triton': False, 'triton_autotune': False, 'num_kv': -1, 'rope_theta': 10000, 'invert_mask': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/andv/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/andv/.cache/torch_extensions/py310_cu124/transformer_inference/build.ninja...\n",
      "/home/andv/important/chatbot_rag/.venv/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Building extension module transformer_inference...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n",
      "Time to load transformer_inference op: 0.04631924629211426 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module transformer_inference...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models Loaded!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import CrossEncoder, SentenceTransformer, util\n",
    "from underthesea import sent_tokenize\n",
    "from vinorm import TTSnorm\n",
    "from TTS.tts.configs.xtts_config import XttsConfig\n",
    "from TTS.tts.models.xtts import Xtts\n",
    "import io\n",
    "import soundfile as sf  # Th∆∞ vi·ªán x·ª≠ l√Ω √¢m thanh\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "from pydub import AudioSegment\n",
    "from io import BytesIO\n",
    "import librosa\n",
    "from Xu_ly_text import Xu_ly_text, Xu_ly_text_de_doc\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "# from huggingface_hub import login\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# # Get the Hugging Face access token from environment variables\n",
    "# hf_token = os.getenv(\"PROJECTCB1_HUGGINGFACE_ACCESS_TOKEN\")\n",
    "\n",
    "# # Log in to Hugging Face using the access token\n",
    "# if hf_token:\n",
    "#     login(token=hf_token)\n",
    "# else:\n",
    "#     print(\n",
    "#         \"Access token not found. Please set the HUGGINGFACE_ACCESS_TOKEN in your .env file.\"\n",
    "#     )\n",
    "\n",
    "eb_model_path = os.getenv(\"PROJECTCB1_EMBEDDING_MODEL\")\n",
    "embeddings_path = os.getenv(\"PROJECTCB1_EMBEDDING_DATA_PATH\")\n",
    "\n",
    "\n",
    "# H√†m n·ªôi b·ªô\n",
    "def load_embedding_model(embedding_model_path):\n",
    "    embedding_model = SentenceTransformer(\n",
    "        model_name_or_path=embedding_model_path, device=device\n",
    "    )\n",
    "    return embedding_model\n",
    "\n",
    "\n",
    "# def load_reranking_model(pr_model_path):\n",
    "#     pr_model = CrossEncoder(model_name=pr_model_path, device=device, trust_remote_code=True)\n",
    "#     return pr_model\n",
    "\n",
    "\n",
    "def load_embeddings(embeddings_path):\n",
    "    text_chunks_and_embedding_df = pd.read_csv(embeddings_path)\n",
    "    text_chunks_and_embedding_df[\"embedding\"] = text_chunks_and_embedding_df[\n",
    "        \"embedding\"\n",
    "    ].apply(lambda x: np.fromstring(x.strip(\"[]\"), sep=\" \"))\n",
    "    pages_and_chunks = text_chunks_and_embedding_df.to_dict(orient=\"records\")\n",
    "    embeddings = torch.tensor(\n",
    "        np.array(text_chunks_and_embedding_df[\"embedding\"].tolist()),\n",
    "        dtype=torch.float32,\n",
    "    ).to(device)\n",
    "    return embeddings, pages_and_chunks\n",
    "\n",
    "\n",
    "def clear_gpu_cache():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def load_model_tts(xtts_checkpoint, xtts_config, xtts_vocab):\n",
    "    clear_gpu_cache()\n",
    "    config = XttsConfig()\n",
    "    config.load_json(xtts_config)\n",
    "    XTTS_MODEL = Xtts.init_from_config(config)\n",
    "\n",
    "    use_deepspeed = torch.cuda.is_available()\n",
    "\n",
    "    XTTS_MODEL.load_checkpoint(\n",
    "        config,\n",
    "        checkpoint_path=xtts_checkpoint,\n",
    "        vocab_path=xtts_vocab,\n",
    "        use_deepspeed=use_deepspeed,\n",
    "    )\n",
    "    if torch.cuda.is_available():\n",
    "        XTTS_MODEL.cuda()\n",
    "    return XTTS_MODEL\n",
    "\n",
    "\n",
    "def normalize_vietnamese_text(text):\n",
    "    text = Xu_ly_text_de_doc(text)\n",
    "    text = (\n",
    "        TTSnorm(text, unknown=False, lower=False, rule=True)\n",
    "        .replace(\"  \", \" \")\n",
    "        .replace(\":\", \".\")\n",
    "        .replace(\"!.\", \"!\")\n",
    "        .replace(\"?.\", \"?\")\n",
    "        .replace(\" .\", \".\")\n",
    "        .replace(\" ,\", \",\")\n",
    "        .replace('\"', \"\")\n",
    "        .replace(\"'\", \"\")\n",
    "        .replace(\"+\", \" \")\n",
    "        .replace(\"..\", \".\")\n",
    "        .replace(\"AI\", \"√Çy Ai\")\n",
    "        .replace(\"A.I\", \"√Çy Ai\")\n",
    "    )\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "def split_sentences(text, max_length=245):\n",
    "    text = (\n",
    "        text.replace(\"\\n\", \". \").replace(\";\", \".\").replace(\"?\", \".\").replace(\"!\", \".\")\n",
    "    )\n",
    "\n",
    "    sentences = re.findall(r\"[^,.]+[,.]\", text)\n",
    "    grouped_sentences = []\n",
    "    current_group = \"\"\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # N·∫øu th√™m c√¢u v√†o m√† kh√¥ng v∆∞·ª£t qu√° gi·ªõi h·∫°n max_length\n",
    "        if len(current_group) + len(sentence) + 1 < max_length:\n",
    "            if current_group:\n",
    "                current_group += \" \" + sentence  # Gh√©p c√¢u m·ªõi v√†o c√¢u tr∆∞·ªõc ƒë√≥\n",
    "            else:\n",
    "                current_group = sentence  # C√¢u ƒë·∫ßu ti√™n c·ªßa nh√≥m\n",
    "        elif len(sentence) > max_length:  # X·ª≠ l√Ω\n",
    "            if current_group:\n",
    "                grouped_sentences.append(current_group)\n",
    "                current_group = \"\"\n",
    "            k = 0\n",
    "            tamthoi = []\n",
    "            for i in sentence.split(\" \"):\n",
    "                tamthoi += [i]\n",
    "                if len(tamthoi) >= 40:\n",
    "                    grouped_sentences += [\" \".join(tamthoi)]\n",
    "                    tamthoi = []\n",
    "            if tamthoi:\n",
    "                grouped_sentences += [\" \".join(tamthoi)]\n",
    "        else:\n",
    "            grouped_sentences.append(current_group)  # Th√™m nh√≥m v√†o list\n",
    "            current_group = sentence  # Kh·ªüi t·∫°o nh√≥m m·ªõi v·ªõi c√¢u hi·ªán t·∫°i\n",
    "\n",
    "    if current_group:\n",
    "        grouped_sentences.append(current_group)  # Th√™m nh√≥m cu·ªëi c√πng v√†o list\n",
    "\n",
    "    return grouped_sentences\n",
    "\n",
    "\n",
    "# Khai b√°o c√°c m√¥ h√¨nh\n",
    "\n",
    "print(\"Loading models... \")\n",
    "# Load model embedding\n",
    "\n",
    "embedding_model = load_embedding_model(eb_model_path)\n",
    "\n",
    "# Load reranking\n",
    "# rr_model_path = \"itdainb/PhoRanker\"\n",
    "# reranking_model = load_reranking_model(rr_model_path)\n",
    "\n",
    "# Dowload TTS capleaf/viXTTS\n",
    "# from huggingface_hub import snapshot_download\n",
    "\n",
    "# snapshot_download(\n",
    "#     repo_id=\"capleaf/viXTTS\", repo_type=\"model\", local_dir=\"Model/TTS_model\"\n",
    "# )\n",
    "\n",
    "tts_model_path = os.getenv(\"PROJECTCB1_TTS_MODEL\")\n",
    "# Load model TTS capleaf/viXTTS\n",
    "vixtts_model = load_model_tts(\n",
    "    xtts_checkpoint=f\"{tts_model_path}/model.pth\",\n",
    "    xtts_config=f\"{tts_model_path}/config.json\",\n",
    "    xtts_vocab=f\"{tts_model_path}/vocab.json\",\n",
    ")\n",
    "\n",
    "embeddings, pages_and_chunks = load_embeddings(embeddings_path)  # Load embeddings\n",
    "reference_audio = os.getenv(\"PROJECTCB1_REFERENCE_AUDIO\")  # M·∫´u gi·ªçng n√≥i\n",
    "\n",
    "# Load model STT nguyenvulebinh/wav2vec2-base-vietnamese-250h\n",
    "\n",
    "stt_model_path = os.getenv(\"PROJECTCB1_STT_MODEL\")\n",
    "processor = Wav2Vec2Processor.from_pretrained(stt_model_path)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(stt_model_path).to(device)\n",
    "print(\"Models Loaded!\")\n",
    "\n",
    "# processor.save_pretrained(stt_model_path)\n",
    "# model.save_pretrained(stt_model_path)\n",
    "# H√†m s·ª≠ d·ª•ng cho API\n",
    "\n",
    "\n",
    "def run_stt(audio_bytes):\n",
    "    # ƒê·ªçc t·ªáp √¢m thanh t·ª´ byte\n",
    "    audio = AudioSegment.from_file(BytesIO(audio_bytes))\n",
    "\n",
    "    # Chuy·ªÉn ƒë·ªïi √¢m thanh th√†nh m·∫£ng numpy\n",
    "    samples = np.array(audio.get_array_of_samples())\n",
    "\n",
    "    # ƒê·∫£m b·∫£o l√† mono (1 k√™nh)\n",
    "    if audio.channels > 1:\n",
    "        samples = samples.reshape((-1, audio.channels))\n",
    "        samples = samples.mean(\n",
    "            axis=1\n",
    "        )  # L·∫•y trung b√¨nh gi√° tr·ªã c·ªßa t·∫•t c·∫£ c√°c k√™nh ƒë·ªÉ chuy·ªÉn sang mono\n",
    "\n",
    "    # Chu·∫©n h√≥a l·∫°i t·∫ßn s·ªë m·∫´u v·ªÅ 16000 Hz\n",
    "    samples_16k = librosa.resample(\n",
    "        samples.astype(np.float32), orig_sr=audio.frame_rate, target_sr=16000\n",
    "    )\n",
    "\n",
    "    # Tokenize d·ªØ li·ªáu ƒë·∫ßu v√†o\n",
    "    input_values = processor(\n",
    "        samples_16k, return_tensors=\"pt\", padding=\"longest\", sampling_rate=16000\n",
    "    ).input_values\n",
    "\n",
    "    # Chuy·ªÉn sang GPU v√† chuy·ªÉn ƒë·ªïi sang float\n",
    "    input_values = input_values.to(device).float()\n",
    "\n",
    "    # L·∫•y k·∫øt qu·∫£ d·ª± ƒëo√°n t·ª´ m√¥ h√¨nh\n",
    "    logits = model(input_values).logits\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    # Gi·∫£i m√£ k·∫øt qu·∫£ d·ª± ƒëo√°n th√†nh vƒÉn b·∫£n\n",
    "    transcription = processor.batch_decode(predicted_ids)[0]\n",
    "    text = Xu_ly_text(transcription)\n",
    "    return text\n",
    "\n",
    "\n",
    "def run_tts(text, lang=\"vi\"):\n",
    "    if vixtts_model is None or not reference_audio:\n",
    "        return \"You need to run the previous step to load the model !!\", None, None\n",
    "\n",
    "    gpt_cond_latent, speaker_embedding = vixtts_model.get_conditioning_latents(\n",
    "        audio_path=reference_audio,\n",
    "        gpt_cond_len=vixtts_model.config.gpt_cond_len,\n",
    "        max_ref_length=vixtts_model.config.max_ref_len,\n",
    "        sound_norm_refs=vixtts_model.config.sound_norm_refs,\n",
    "    )\n",
    "\n",
    "    # Chu·∫©n h√≥a\n",
    "    tts_text = normalize_vietnamese_text(text)\n",
    "    tts_texts = split_sentences(tts_text)\n",
    "    print(tts_texts)\n",
    "    wav_chunks = []\n",
    "    for text in tts_texts:\n",
    "        if text.strip() == \"\":\n",
    "            continue\n",
    "\n",
    "        wav_chunk = vixtts_model.inference(\n",
    "            text=text,\n",
    "            language=lang,\n",
    "            gpt_cond_latent=gpt_cond_latent,\n",
    "            speaker_embedding=speaker_embedding,\n",
    "            temperature=0.01,  # 0.3\n",
    "            length_penalty=1.0,  # 1.0\n",
    "            repetition_penalty=50.0,  # 10.0\n",
    "            top_k=5,  # 30\n",
    "            top_p=0.95,  # 0.85\n",
    "        )\n",
    "\n",
    "        keep_len = -1\n",
    "        wav_chunk[\"wav\"] = torch.tensor(wav_chunk[\"wav\"][:keep_len])\n",
    "        wav_chunks.append(wav_chunk[\"wav\"])\n",
    "\n",
    "    out_wav = (\n",
    "        torch.cat(wav_chunks, dim=0).squeeze(0).cpu().numpy()\n",
    "    )  # Chuy·ªÉn sang numpy array\n",
    "\n",
    "    # Chuy·ªÉn ƒë·ªïi Tensor th√†nh ƒë·ªãnh d·∫°ng WAV\n",
    "    buffer = io.BytesIO()\n",
    "\n",
    "    # Ghi √¢m thanh v√†o buffer, ƒë·∫£m b·∫£o d·ªØ li·ªáu ƒë·∫ßu v√†o l√† numpy array v√† ƒë·ªãnh d·∫°ng ƒë√∫ng\n",
    "    try:\n",
    "        sf.write(buffer, out_wav, 24000, format=\"WAV\")\n",
    "        buffer.seek(0)\n",
    "        wav_data = buffer.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing WAV file: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "    return wav_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_resources(query: str, n_resources_to_return: int = 15, threshold: int =0.2):\n",
    "    query_embedding = embedding_model.encode(query, convert_to_tensor=True)\n",
    "    # Use cosine similarity instead of dot score\n",
    "    cosine_scores = util.pytorch_cos_sim(query_embedding, embeddings)[0]\n",
    "    \n",
    "    # Get all scores and corresponding indices, then filter based on score > 0.5\n",
    "    scores, indices = torch.topk(input=cosine_scores, k=n_resources_to_return)\n",
    "    filtered_scores_indices = [(score.item(), index.item()) for score, index in zip(scores, indices) if score.item() > threshold]\n",
    "    \n",
    "    # Extract the scores and indices after filtering\n",
    "    filtered_indices = [index for _, index in filtered_scores_indices]\n",
    "    \n",
    "    # Take top 'n_resources_to_return' from the filtered list\n",
    "    # top_scores = filtered_scores[:n_resources_to_return]\n",
    "    top_indices = filtered_indices[:n_resources_to_return]\n",
    "    \n",
    "    context_items = [pages_and_chunks[i] for i in top_indices]\n",
    "    results = [item[\"Final_Answer\"] for item in context_items]\n",
    "    # ques = [item[\"Question\"] for item in context_items]\n",
    "    # pr_results = reranking_model.rank(query, results, return_documents=True, top_k=5)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = retrieve_relevant_resources(\"T·ªïng v·ªën c·ªßa ch∆∞∆°ng tr√¨nh m·ª•c ti√™u qu·ªëc gia l√† bao nhi√™u?\", n_resources_to_return=10, threshold=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # coding: utf8\n",
    "# import torch\n",
    "# from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# model_path = \"vinai/PhoGPT-4B-Chat\"  \n",
    "# # model_path = \"vilm/vinallama-2.7b-chat\"\n",
    "# # model_path = \"/home/andv/important/Chatbot/finetune_llm/phogpt-mergerd-with-config-1\"\n",
    "\n",
    "# # model_path = \"/home/andv/important/Chatbot/finetune\"\n",
    "\n",
    "# # config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)  \n",
    "# # config.init_device = \"cuda\"\n",
    "# # config.attn_config['attn_impl'] = 'flash' # If installed: this will use either Flash Attention V1 or V2 depending on what is installed\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_path,\n",
    "#                                             #  config=config, \n",
    "#                                              torch_dtype=torch.bfloat16, \n",
    "#                                              trust_remote_code=True).to(\"cuda\")\n",
    "\n",
    "# # model = AutoModelForCausalLM.from_pretrained(model_path, config = config, trust_remote_code=True).to(\"cuda\")\n",
    "# # If your GPU does not support bfloat16:\n",
    "# # model = AutoModelForCausalLM.from_pretrained(model_path, config=config, torch_dtype=torch.float16, trust_remote_code=True)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import TextStreamer\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "model_path = \"SeaLLMs/SeaLLMs-v3-1.5B-Chat\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  model_path,\n",
    "  torch_dtype=torch.bfloat16, \n",
    "  device_map=device,\n",
    "  # load_in_4bit = True,\n",
    "  attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# prepare messages to model\n",
    "# C√°c ƒëo·∫°n vƒÉn c√≥ li√™n quan: <tr√≠ch xu·∫•t c√°c ƒëo·∫°n vƒÉn c√≥ li√™n quan t·ª´ ng·ªØ c·∫£nh t·∫°i ƒë√¢y>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to format the prompt\n",
    "def prompt_formatter(query: str, results: list) -> str:\n",
    "    context = \"* \" + \"\\n\\n* \".join(results)\n",
    "    base_prompt = \"\"\"H√£y cho b·∫£n th√¢n kh√¥ng gian ƒë·ªÉ suy nghƒ© b·∫±ng c√°ch tr√≠ch xu·∫•t c√°c ƒëo·∫°n vƒÉn c√≥ li√™n quan t·ª´ ng·ªØ c·∫£nh d∆∞·ªõi ƒë√¢y tr∆∞·ªõc khi tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng.\n",
    "S·ª≠ d·ª•ng c√°c ƒëo·∫°n ng·ªØ c·∫£nh sau ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng:\n",
    "\n",
    "{context}\n",
    "\n",
    "C√°c ƒëo·∫°n vƒÉn c√≥ li√™n quan: <tr√≠ch xu·∫•t c√°c ƒëo·∫°n vƒÉn c√≥ li√™n quan t·ª´ ng·ªØ c·∫£nh t·∫°i ƒë√¢y>\n",
    "C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng: {query}\n",
    "\"\"\"\n",
    "    prompt = base_prompt.format(context=context, query=query)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to format the prompt\n",
    "# def prompt_formatter(query: str, results: list) -> str:\n",
    "#     context = \"* \" + \"\\n\\n* \".join(results)\n",
    "#     base_prompt = \"\"\"D·ª±a v√†o c√°c ƒëo·∫°n ng·ªØ c·∫£nh d∆∞·ªõi ƒë√¢y ƒë·ªÉ tr·∫£ l·ªùi cho c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng:\n",
    "\n",
    "# {context}\n",
    "\n",
    "# C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng: {query}\n",
    "# \"\"\"\n",
    "#     prompt = base_prompt.format(context=context, query=query)\n",
    "#     return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"B·∫°n l√† m·ªôt tr·ª£ l√≠ Ti·∫øng Vi·ªát h·ªØu √≠ch. H√£y tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng m·ªôt c√°ch ch√≠nh x√°c.\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"N·ªôi dung d·ª± √°n 1 ch∆∞∆°ng tr√¨nh m·ª•c ti√™u qu·ªëc gia l√† g√¨?\"\n",
    "results = retrieve_relevant_resources(query, n_resources_to_return=5, threshold = 0.5)\n",
    "prompt = prompt_formatter(query, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H√£y cho b·∫£n th√¢n kh√¥ng gian ƒë·ªÉ suy nghƒ© b·∫±ng c√°ch tr√≠ch xu·∫•t c√°c ƒëo·∫°n vƒÉn c√≥ li√™n quan t·ª´ ng·ªØ c·∫£nh d∆∞·ªõi ƒë√¢y tr∆∞·ªõc khi tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng.\n",
      "S·ª≠ d·ª•ng c√°c ƒëo·∫°n ng·ªØ c·∫£nh sau ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng:\n",
      "\n",
      "* N·ªôi dung c·ª• th·ªÉ c·ªßa D·ª± √°n 1 l√† g√¨?\n",
      "N·ªôi dung c·ª• th·ªÉ c·ªßa D·ª± √°n 1 bao g·ªìm:\n",
      "- N·ªôi dung s·ªë 01: H·ªó tr·ª£ ƒë·∫•t ·ªü: CƒÉn c·ª© qu·ªπ ƒë·∫•t, h·∫°n m·ª©c ƒë·∫•t ·ªü v√† kh·∫£ nƒÉng ng√¢n s√°ch, ·ª¶y ban nh√¢n d√¢n c·∫•p t·ªânh xem x√©t, quy·∫øt ƒë·ªãnh giao ƒë·∫•t ƒë·ªÉ l√†m nh√† ·ªü cho c√°c ƒë·ªëi t∆∞·ª£ng n√™u tr√™n ph√π h·ª£p v·ªõi ƒëi·ªÅu ki·ªán, t·∫≠p qu√°n ·ªü ƒë·ªãa ph∆∞∆°ng v√† ph√°p lu·∫≠t v·ªÅ ƒë·∫•t ƒëai, c·ª• th·ªÉ:\n",
      "+ ·ªû nh·ªØng n∆°i c√≥ ƒëi·ªÅu ki·ªán v·ªÅ ƒë·∫•t ƒëai, ch√≠nh quy·ªÅn ƒë·ªãa ph∆∞∆°ng s·ª≠ d·ª•ng s·ªë ti·ªÅn h·ªó tr·ª£ t·ª´ ng√¢n s√°ch ƒë·ªÉ t·∫°o m·∫∑t b·∫±ng, l√†m h·∫° t·∫ßng k·ªπ thu·∫≠t ƒë·ªÉ c·∫•p ƒë·∫•t ·ªü cho c√°c ƒë·ªëi t∆∞·ª£ng ƒë∆∞·ª£c th·ª• h∆∞·ªüng;\n",
      "+ ·ªû c√°c ƒë·ªãa ph∆∞∆°ng kh√¥ng c√≥ ƒëi·ªÅu ki·ªán v·ªÅ ƒë·∫•t ƒëai, ch√≠nh quy·ªÅn ƒë·ªãa ph∆∞∆°ng b·ªë tr√≠ kinh ph√≠ h·ªó tr·ª£ cho ng∆∞·ªùi d√¢n t·ª± ·ªïn ƒë·ªãnh ch·ªó ·ªü theo h√¨nh th·ª©c xen gh√©p.\n",
      "- N·ªôi dung s·ªë 02: H·ªó tr·ª£ nh√† ·ªü: H·ªó tr·ª£ x√¢y d·ª±ng 01 cƒÉn nh√† theo phong t·ª•c t·∫≠p qu√°n c·ªßa ƒë·ªãa ph∆∞∆°ng, ƒë·ªãnh m·ª©c t√≠nh theo x√¢y d·ª±ng 01 cƒÉn nh√† c·∫•p 4 ƒë·∫£m b·∫£o 3 c·ª©ng (n·ªÅn c·ª©ng, khung - t∆∞·ªùng c·ª©ng, m√°i c·ª©ng).\n",
      "- N·ªôi dung s·ªë 03: H·ªó tr·ª£ ƒë·∫•t s·∫£n xu·∫•t, chuy·ªÉn ƒë·ªïi ngh·ªÅ: H·ªô d√¢n t·ªôc thi·ªÉu s·ªë ngh√®o; h·ªô ngh√®o d√¢n t·ªôc Kinh sinh s·ªëng ·ªü x√£ ƒë·∫∑c bi·ªát kh√≥ khƒÉn, th√¥n ƒë·∫∑c bi·ªát kh√≥ khƒÉn v√πng ƒë·ªìng b√†o d√¢n t·ªôc thi·ªÉu s·ªë v√† mi·ªÅn n√∫i l√†m ngh·ªÅ n√¥ng, l√¢m, ng∆∞ nghi·ªáp kh√¥ng c√≥ ho·∫∑c thi·∫øu t·ª´ 50% ƒë·∫•t s·∫£n xu·∫•t tr·ªü l√™n theo ƒë·ªãnh m·ª©c c·ªßa ƒë·ªãa ph∆∞∆°ng th√¨ ƒë∆∞·ª£c h∆∞·ªüng m·ªôt trong hai ch√≠nh s√°ch sau:\n",
      "+ H·ªó tr·ª£ tr·ª±c ti·∫øp ƒë·∫•t s·∫£n xu·∫•t: H·ªô kh√¥ng c√≥ ƒë·∫•t s·∫£n xu·∫•t n·∫øu c√≥ nhu c·∫ßu th√¨ ƒë∆∞·ª£c ch√≠nh quy·ªÅn ƒë·ªãa ph∆∞∆°ng tr·ª±c ti·∫øp giao ƒë·∫•t s·∫£n xu·∫•t;\n",
      "+ H·ªó tr·ª£ chuy·ªÉn ƒë·ªïi ngh·ªÅ: Tr∆∞·ªùng h·ª£p ch√≠nh quy·ªÅn ƒë·ªãa ph∆∞∆°ng kh√¥ng b·ªë tr√≠ ƒë∆∞·ª£c ƒë·∫•t s·∫£n xu·∫•t th√¨ h·ªô kh√¥ng c√≥ ƒë·∫•t ho·∫∑c thi·∫øu ƒë·∫•t s·∫£n xu·∫•t ƒë∆∞·ª£c h·ªó tr·ª£ chuy·ªÉn ƒë·ªïi ngh·ªÅ.\n",
      "- N·ªôi dung s·ªë 04: H·ªó tr·ª£ n∆∞·ªõc sinh ho·∫°t:\n",
      "+ H·ªó tr·ª£ n∆∞·ªõc sinh ho·∫°t ph√¢n t√°n: ∆Øu ti√™n h·ªó tr·ª£ ƒë·ªÉ mua s·∫Øm trang b·ªã ho·∫∑c x√¢y d·ª±ng b·ªÉ ch·ª©a n∆∞·ªõc ph·ª•c v·ª• sinh ho·∫°t c·ªßa h·ªô gia ƒë√¨nh;\n",
      "+ H·ªó tr·ª£ n∆∞·ªõc sinh ho·∫°t t·∫≠p trung: ƒê·∫ßu t∆∞ x√¢y d·ª±ng c√¥ng tr√¨nh n∆∞·ªõc t·∫≠p trung theo d·ª± √°n ƒë∆∞·ª£c c·∫•p c√≥ th·∫©m quy·ªÅn ph√™ duy·ªát. ∆Øu ti√™n cho ng∆∞·ªùi d√¢n v√πng th∆∞·ªùng xuy√™n x·∫£y ra h·∫°n h√°n, x√¢m nh·∫≠p m·∫∑n, v√πng ƒë·∫∑c bi·ªát kh√≥ khƒÉn, v√πng cao ch∆∞a c√≥ ngu·ªìn n∆∞·ªõc ho·∫∑c thi·∫øu n∆∞·ªõc sinh ho·∫°t h·ª£p v·ªá sinh.\n",
      "- H·ªô gia ƒë√¨nh thu·ªôc di·ªán ƒë·ªëi t∆∞·ª£ng theo quy ƒë·ªãnh c·ªßa D·ª± √°n n√†y c√≥ nhu c·∫ßu vay v·ªën ƒë∆∞·ª£c vay t·ª´ Ng√¢n h√†ng Ch√≠nh s√°ch x√£ h·ªôi ƒë·ªÉ c√≥ ƒë·∫•t ·ªü, x√¢y d·ª±ng m·ªõi ho·∫∑c s·ª≠a ch·ªØa nh√† ·ªü, t·∫°o qu·ªπ ƒë·∫•t s·∫£n xu·∫•t, h·ªçc ngh·ªÅ v√† chuy·ªÉn ƒë·ªïi ngh·ªÅ.\" (Theo Ph·∫ßn III, M·ª•c 1 Quy·∫øt ƒë·ªãnh Th·ªß t∆∞·ªõng ch√≠nh ph·ªß 1719/Qƒê-TTg)\n",
      "\n",
      "* N·ªôi dung s·ªë 01 c·ªßa D·ª± √°n 1 h·ªó tr·ª£ v·∫•n ƒë·ªÅ g√¨?\n",
      "N·ªôi dung s·ªë 01: H·ªó tr·ª£ ƒë·∫•t ·ªü: \n",
      "CƒÉn c·ª© qu·ªπ ƒë·∫•t, h·∫°n m·ª©c ƒë·∫•t ·ªü v√† kh·∫£ nƒÉng ng√¢n s√°ch, ·ª¶y ban nh√¢n d√¢n c·∫•p t·ªânh xem x√©t, quy·∫øt ƒë·ªãnh giao ƒë·∫•t ƒë·ªÉ l√†m nh√† ·ªü cho c√°c ƒë·ªëi t∆∞·ª£ng n√™u tr√™n ph√π h·ª£p v·ªõi ƒëi·ªÅu ki·ªán, t·∫≠p qu√°n ·ªü ƒë·ªãa ph∆∞∆°ng v√† ph√°p lu·∫≠t v·ªÅ ƒë·∫•t ƒëai, c·ª• th·ªÉ:\n",
      "+ ·ªû nh·ªØng n∆°i c√≥ ƒëi·ªÅu ki·ªán v·ªÅ ƒë·∫•t ƒëai, ch√≠nh quy·ªÅn ƒë·ªãa ph∆∞∆°ng s·ª≠ d·ª•ng s·ªë ti·ªÅn h·ªó tr·ª£ t·ª´ ng√¢n s√°ch ƒë·ªÉ t·∫°o m·∫∑t b·∫±ng, l√†m h·∫° t·∫ßng k·ªπ thu·∫≠t ƒë·ªÉ c·∫•p ƒë·∫•t ·ªü cho c√°c ƒë·ªëi t∆∞·ª£ng ƒë∆∞·ª£c th·ª• h∆∞·ªüng;\n",
      "+ ·ªû c√°c ƒë·ªãa ph∆∞∆°ng kh√¥ng c√≥ ƒëi·ªÅu ki·ªán v·ªÅ ƒë·∫•t ƒëai, ch√≠nh quy·ªÅn ƒë·ªãa ph∆∞∆°ng b·ªë tr√≠ kinh ph√≠ h·ªó tr·ª£ cho ng∆∞·ªùi d√¢n t·ª± ·ªïn ƒë·ªãnh ch·ªó ·ªü theo h√¨nh th·ª©c xen gh√©p.\" (Theo Ph·∫ßn III, M·ª•c 1 Quy·∫øt ƒë·ªãnh Th·ªß t∆∞·ªõng ch√≠nh ph·ªß 1719/Qƒê-TTg)\n",
      "\n",
      "* N·ªôi dung s·ªë 02 c·ªßa D·ª± √°n 1 h·ªó tr·ª£ v·∫•n ƒë·ªÅ g√¨?\n",
      "N·ªôi dung s·ªë 02: H·ªó tr·ª£ nh√† ·ªü: H·ªó tr·ª£ x√¢y d·ª±ng 01 cƒÉn nh√† theo phong t·ª•c t·∫≠p qu√°n c·ªßa ƒë·ªãa ph∆∞∆°ng, ƒë·ªãnh m·ª©c t√≠nh theo x√¢y d·ª±ng 01 cƒÉn nh√† c·∫•p 4 ƒë·∫£m b·∫£o 3 c·ª©ng (n·ªÅn c·ª©ng, khung - t∆∞·ªùng c·ª©ng, m√°i c·ª©ng). (Theo Ph·∫ßn III, M·ª•c 1 Quy·∫øt ƒë·ªãnh Th·ªß t∆∞·ªõng ch√≠nh ph·ªß 1719/Qƒê-TTg)\n",
      "\n",
      "* M·ª•c ti√™u c·ªßa N·ªôi dung 1, Ti·ªÉu d·ª± √°n 1, D·ª± √°n 10 l√† g√¨?\n",
      "M·ª•c ti√™u c·ªßa N·ªôi dung s·ªë 01, Ti·ªÉu d·ª± √°n 1, D·ª± √°n 10 l√†: \n",
      "X√¢y d·ª±ng, n√¢ng cao ch·∫•t l∆∞·ª£ng v√† hi·ªáu qu·∫£ c√¥ng t√°c v·∫≠n ƒë·ªông, ph√°t huy vai tr√≤ c·ªßa l·ª±c l∆∞·ª£ng c·ªët c√°n v√† ng∆∞·ªùi c√≥ uy t√≠n trong v√πng ƒë·ªìng b√†o d√¢n t·ªôc thi·ªÉu s·ªë v√† mi·ªÅn n√∫i. Bi·ªÉu d∆∞∆°ng, t√¥n vinh, ghi nh·∫≠n c√¥ng lao, s·ª± ƒë√≥ng g√≥p c·ªßa c√°c ƒëi·ªÉn h√¨nh ti√™n ti·∫øn trong v√πng ƒë·ªìng b√†o d√¢n t·ªôc thi·ªÉu s·ªë v√† mi·ªÅn n√∫i trong s·ª± nghi·ªáp x√¢y d·ª±ng, b·∫£o v·ªá T·ªï qu·ªëc v√† h·ªôi nh·∫≠p qu·ªëc t·∫ø.\"\n",
      " (Theo Ph·∫ßn III, M·ª•c 10 Quy·∫øt ƒë·ªãnh Th·ªß t∆∞·ªõng ch√≠nh ph·ªß 1719/Qƒê-TTg)\n",
      "\n",
      "* N·ªôi dung c·ªßa Ti·ªÉu d·ª± √°n 1, D·ª± √°n 5 l√† g√¨?\n",
      "N·ªôi dung c·ªßa Ti·ªÉu d·ª± √°n 1, D·ª± √°n 5 thu·ªôc Ch∆∞∆°ng tr√¨nh m·ª•c ti√™u qu·ªëc gia ph√°t tri·ªÉn kinh t·∫ø - x√£ h·ªôi v√πng ƒë·ªìng b√†o d√¢n t·ªôc thi·ªÉu s·ªë v√† mi·ªÅn n√∫i:\n",
      "+ ƒê·∫ßu t∆∞ c∆° s·ªü v·∫≠t ch·∫•t, trang thi·∫øt b·ªã cho c√°c tr∆∞·ªùng ph·ªï th√¥ng d√¢n t·ªôc n·ªôi tr√∫, b√°n tr√∫, c√≥ h·ªçc sinh b√°n tr√∫:\n",
      ". N√¢ng c·∫•p, c·∫£i t·∫°o c∆° s·ªü v·∫≠t ch·∫•t kh·ªëi ph√≤ng/c√¥ng tr√¨nh ph·ª•c v·ª• ƒÉn, ·ªü, sinh ho·∫°t cho h·ªçc sinh v√† ph√≤ng c√¥ng v·ª• gi√°o vi√™n;\n",
      ". N√¢ng c·∫•p, c·∫£i t·∫°o c∆° s·ªü v·∫≠t ch·∫•t/kh·ªëi ph√≤ng/c√¥ng tr√¨nh ph·ª•c v·ª• h·ªçc t·∫≠p; b·ªï sung, n√¢ng c·∫•p c√°c c√¥ng tr√¨nh ph·ª• tr·ª£ kh√°c;\n",
      ". ƒê·∫ßu t∆∞ c∆° s·ªü v·∫≠t ch·∫•t ph·ª•c v·ª• chuy·ªÉn ƒë·ªïi s·ªë gi√°o d·ª•c ph·ª•c v·ª• vi·ªác gi·∫£ng d·∫°y v√† h·ªçc t·∫≠p tr·ª±c tuy·∫øn cho h·ªçc sinh d√¢n t·ªôc thi·ªÉu s·ªë;\n",
      ". ∆Øu ti√™n ƒë·∫ßu t∆∞ x√¢y d·ª±ng tr∆∞·ªùng d√¢n t·ªôc n·ªôi tr√∫ cho huy·ªán c√≥ ƒë√¥ng ƒë·ªìng b√†o d√¢n t·ªôc thi·ªÉu s·ªë sinh s·ªëng nh∆∞ng ch∆∞a c√≥ ho·∫∑c ph·∫£i ƒëi thu√™ ƒë·ªãa ƒëi·ªÉm ƒë·ªÉ t·ªï ch·ª©c ho·∫°t ƒë·ªông.\n",
      "+ X√≥a m√π ch·ªØ cho ng∆∞·ªùi d√¢n v√πng ƒë·ªìng b√†o d√¢n t·ªôc thi·ªÉu s·ªë:\n",
      ". X√¢y d·ª±ng t√†i li·ªáu ph·ª•c v·ª• h∆∞·ªõng d·∫´n d·∫°y xo√° m√π ch·ªØ, thi·∫øt k·∫ø c√¥ng ngh·ªá, thi·∫øt b·ªã l∆∞u tr·ªØ c∆° s·ªü d·ªØ li·ªáu v·ªÅ xo√° m√π ch·ªØ, d·∫°y h·ªçc xo√° m√π ch·ªØ;\n",
      ". B·ªìi d∆∞·ª°ng, t·∫≠p hu·∫•n, truy·ªÅn th√¥ng, tuy√™n truy·ªÅn;\n",
      ". H·ªó tr·ª£ ng∆∞·ªùi d√¢n tham gia h·ªçc xo√° m√π ch·ªØ;\n",
      ". H·ªó tr·ª£ t√†i li·ªáu h·ªçc t·∫≠p, s√°ch gi√°o khoa, vƒÉn ph√≤ng ph·∫©m.\"\n",
      " (Theo Ph·∫ßn III, M·ª•c 5 Quy·∫øt ƒë·ªãnh Th·ªß t∆∞·ªõng ch√≠nh ph·ªß 1719/Qƒê-TTg)\n",
      "\n",
      "C√°c ƒëo·∫°n vƒÉn c√≥ li√™n quan: <tr√≠ch xu·∫•t c√°c ƒëo·∫°n vƒÉn c√≥ li√™n quan t·ª´ ng·ªØ c·∫£nh t·∫°i ƒë√¢y>\n",
      "C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng: N·ªôi dung d·ª± √°n 1 ch∆∞∆°ng tr√¨nh m·ª•c ti√™u qu·ªëc gia l√† g√¨?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N·ªôi dung d·ª± √°n 1 ch∆∞∆°ng tr√¨nh m·ª•c ti√™u qu·ªëc gia l√†: X√¢y d·ª±ng, n√¢ng cao ch·∫•t l∆∞·ª£ng v√† hi·ªáu qu·∫£ c√¥ng t√°c v·∫≠n ƒë·ªông, ph√°t huy vai tr√≤ c·ªßa l·ª±c l∆∞·ª£ng c·ªët c√°n v√† ng∆∞·ªùi c√≥ uy t√≠n trong v√πng ƒë·ªìng b√†o d√¢n t·ªôc thi·ªÉu s·ªë v√† mi·ªÅn n√∫i.\n"
     ]
    }
   ],
   "source": [
    "messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "generated_ids = model.generate(model_inputs.input_ids, \n",
    "                               do_sample=True,\n",
    "                               temperature=0.1,  \n",
    "                                top_k=40,  \n",
    "                                top_p=0.95,  \n",
    "                                max_new_tokens=1024,\n",
    "                                repetition_penalty = 1.05,  \n",
    "                               streamer=streamer)\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "messages.append({\"role\": \"assistant\", \"content\": response})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N·ªôi dung d·ª± √°n 1 ch∆∞∆°ng tr√¨nh m·ª•c ti√™u qu·ªëc gia l√†: X√¢y\n",
      "d·ª±ng, n√¢ng cao ch·∫•t l∆∞·ª£ng v√† hi·ªáu qu·∫£ c√¥ng t√°c v·∫≠n ƒë·ªông,\n",
      "ph√°t huy vai tr√≤ c·ªßa l·ª±c l∆∞·ª£ng c·ªët c√°n v√† ng∆∞·ªùi c√≥ uy t√≠n\n",
      "trong v√πng ƒë·ªìng b√†o d√¢n t·ªôc thi·ªÉu s·ªë v√† mi·ªÅn n√∫i.\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "a = textwrap.fill(response, width=60)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(query:str):\n",
    "    results = retrieve_relevant_resources(query=query, n_resources_to_return=5, threshold=0.5)\n",
    "    input_prompt = prompt_formatter(query=query, results=results)\n",
    "    input_ids = tokenizer(input_prompt, return_tensors=\"pt\")  \n",
    "\n",
    "    outputs = model.generate(  \n",
    "        inputs=input_ids[\"input_ids\"].to(\"cuda\"),  \n",
    "        attention_mask=input_ids[\"attention_mask\"].to(\"cuda\"),  \n",
    "        do_sample=True,  \n",
    "        temperature=0.8,  \n",
    "        top_k=50,  \n",
    "        top_p=0.9,  \n",
    "        max_new_tokens=1024,  \n",
    "        eos_token_id=tokenizer.eos_token_id,  \n",
    "        pad_token_id=tokenizer.pad_token_id  \n",
    "    )  \n",
    "\n",
    "    response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]  \n",
    "    # response = tokenizer.decode(outputs[0])\n",
    "    # response = response.split(\"### Tr·∫£ l·ªùi:\")[1]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
