{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andv/important/chatbot_rag/.venv/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andv/important/chatbot_rag/.venv/src/tts/TTS/utils/io.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(f, map_location=map_location, **kwargs)\n",
      "GPT2InferenceModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-27 16:53:28,037] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-27 16:53:28,701] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.15.2, git-hash=unknown, git-branch=unknown\n",
      "[2024-10-27 16:53:28,702] [WARNING] [config_utils.py:70:_process_deprecated_field] Config parameter replace_method is deprecated. This parameter is no longer needed, please remove from your call to DeepSpeed-inference\n",
      "[2024-10-27 16:53:28,702] [WARNING] [config_utils.py:70:_process_deprecated_field] Config parameter mp_size is deprecated use tensor_parallel.tp_size instead\n",
      "[2024-10-27 16:53:28,702] [INFO] [logging.py:96:log_dist] [Rank -1] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1\n",
      "[2024-10-27 16:53:28,797] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed-Inference config: {'layer_id': 0, 'hidden_size': 1024, 'intermediate_size': 4096, 'heads': 16, 'num_hidden_layers': -1, 'dtype': torch.float32, 'pre_layer_norm': True, 'norm_type': <NormType.LayerNorm: 1>, 'local_rank': -1, 'stochastic_mode': False, 'epsilon': 1e-05, 'mp_size': 1, 'scale_attention': True, 'triangular_masking': True, 'local_attention': False, 'window_size': 1, 'rotary_dim': -1, 'rotate_half': False, 'rotate_every_two': True, 'return_tuple': True, 'mlp_after_attn': True, 'mlp_act_func_type': <ActivationFuncType.GELU: 1>, 'training_mp_size': 1, 'bigscience_bloom': False, 'max_out_tokens': 1024, 'min_out_tokens': 1, 'scale_attn_by_inverse_layer_idx': False, 'enable_qkv_quantization': False, 'use_mup': False, 'return_single_tuple': False, 'set_empty_params': False, 'transposed_mode': False, 'use_triton': False, 'triton_autotune': False, 'num_kv': -1, 'rope_theta': 10000, 'invert_mask': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/andv/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/andv/.cache/torch_extensions/py310_cu124/transformer_inference/build.ninja...\n",
      "/home/andv/important/chatbot_rag/.venv/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Building extension module transformer_inference...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n",
      "Time to load transformer_inference op: 0.04631924629211426 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module transformer_inference...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models Loaded!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import CrossEncoder, SentenceTransformer, util\n",
    "from underthesea import sent_tokenize\n",
    "from vinorm import TTSnorm\n",
    "from TTS.tts.configs.xtts_config import XttsConfig\n",
    "from TTS.tts.models.xtts import Xtts\n",
    "import io\n",
    "import soundfile as sf  # Thư viện xử lý âm thanh\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "from pydub import AudioSegment\n",
    "from io import BytesIO\n",
    "import librosa\n",
    "from Xu_ly_text import Xu_ly_text, Xu_ly_text_de_doc\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "# from huggingface_hub import login\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# # Get the Hugging Face access token from environment variables\n",
    "# hf_token = os.getenv(\"PROJECTCB1_HUGGINGFACE_ACCESS_TOKEN\")\n",
    "\n",
    "# # Log in to Hugging Face using the access token\n",
    "# if hf_token:\n",
    "#     login(token=hf_token)\n",
    "# else:\n",
    "#     print(\n",
    "#         \"Access token not found. Please set the HUGGINGFACE_ACCESS_TOKEN in your .env file.\"\n",
    "#     )\n",
    "\n",
    "eb_model_path = os.getenv(\"PROJECTCB1_EMBEDDING_MODEL\")\n",
    "embeddings_path = os.getenv(\"PROJECTCB1_EMBEDDING_DATA_PATH\")\n",
    "\n",
    "\n",
    "# Hàm nội bộ\n",
    "def load_embedding_model(embedding_model_path):\n",
    "    embedding_model = SentenceTransformer(\n",
    "        model_name_or_path=embedding_model_path, device=device\n",
    "    )\n",
    "    return embedding_model\n",
    "\n",
    "\n",
    "# def load_reranking_model(pr_model_path):\n",
    "#     pr_model = CrossEncoder(model_name=pr_model_path, device=device, trust_remote_code=True)\n",
    "#     return pr_model\n",
    "\n",
    "\n",
    "def load_embeddings(embeddings_path):\n",
    "    text_chunks_and_embedding_df = pd.read_csv(embeddings_path)\n",
    "    text_chunks_and_embedding_df[\"embedding\"] = text_chunks_and_embedding_df[\n",
    "        \"embedding\"\n",
    "    ].apply(lambda x: np.fromstring(x.strip(\"[]\"), sep=\" \"))\n",
    "    pages_and_chunks = text_chunks_and_embedding_df.to_dict(orient=\"records\")\n",
    "    embeddings = torch.tensor(\n",
    "        np.array(text_chunks_and_embedding_df[\"embedding\"].tolist()),\n",
    "        dtype=torch.float32,\n",
    "    ).to(device)\n",
    "    return embeddings, pages_and_chunks\n",
    "\n",
    "\n",
    "def clear_gpu_cache():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def load_model_tts(xtts_checkpoint, xtts_config, xtts_vocab):\n",
    "    clear_gpu_cache()\n",
    "    config = XttsConfig()\n",
    "    config.load_json(xtts_config)\n",
    "    XTTS_MODEL = Xtts.init_from_config(config)\n",
    "\n",
    "    use_deepspeed = torch.cuda.is_available()\n",
    "\n",
    "    XTTS_MODEL.load_checkpoint(\n",
    "        config,\n",
    "        checkpoint_path=xtts_checkpoint,\n",
    "        vocab_path=xtts_vocab,\n",
    "        use_deepspeed=use_deepspeed,\n",
    "    )\n",
    "    if torch.cuda.is_available():\n",
    "        XTTS_MODEL.cuda()\n",
    "    return XTTS_MODEL\n",
    "\n",
    "\n",
    "def normalize_vietnamese_text(text):\n",
    "    text = Xu_ly_text_de_doc(text)\n",
    "    text = (\n",
    "        TTSnorm(text, unknown=False, lower=False, rule=True)\n",
    "        .replace(\"  \", \" \")\n",
    "        .replace(\":\", \".\")\n",
    "        .replace(\"!.\", \"!\")\n",
    "        .replace(\"?.\", \"?\")\n",
    "        .replace(\" .\", \".\")\n",
    "        .replace(\" ,\", \",\")\n",
    "        .replace('\"', \"\")\n",
    "        .replace(\"'\", \"\")\n",
    "        .replace(\"+\", \" \")\n",
    "        .replace(\"..\", \".\")\n",
    "        .replace(\"AI\", \"Ây Ai\")\n",
    "        .replace(\"A.I\", \"Ây Ai\")\n",
    "    )\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "def split_sentences(text, max_length=245):\n",
    "    text = (\n",
    "        text.replace(\"\\n\", \". \").replace(\";\", \".\").replace(\"?\", \".\").replace(\"!\", \".\")\n",
    "    )\n",
    "\n",
    "    sentences = re.findall(r\"[^,.]+[,.]\", text)\n",
    "    grouped_sentences = []\n",
    "    current_group = \"\"\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Nếu thêm câu vào mà không vượt quá giới hạn max_length\n",
    "        if len(current_group) + len(sentence) + 1 < max_length:\n",
    "            if current_group:\n",
    "                current_group += \" \" + sentence  # Ghép câu mới vào câu trước đó\n",
    "            else:\n",
    "                current_group = sentence  # Câu đầu tiên của nhóm\n",
    "        elif len(sentence) > max_length:  # Xử lý\n",
    "            if current_group:\n",
    "                grouped_sentences.append(current_group)\n",
    "                current_group = \"\"\n",
    "            k = 0\n",
    "            tamthoi = []\n",
    "            for i in sentence.split(\" \"):\n",
    "                tamthoi += [i]\n",
    "                if len(tamthoi) >= 40:\n",
    "                    grouped_sentences += [\" \".join(tamthoi)]\n",
    "                    tamthoi = []\n",
    "            if tamthoi:\n",
    "                grouped_sentences += [\" \".join(tamthoi)]\n",
    "        else:\n",
    "            grouped_sentences.append(current_group)  # Thêm nhóm vào list\n",
    "            current_group = sentence  # Khởi tạo nhóm mới với câu hiện tại\n",
    "\n",
    "    if current_group:\n",
    "        grouped_sentences.append(current_group)  # Thêm nhóm cuối cùng vào list\n",
    "\n",
    "    return grouped_sentences\n",
    "\n",
    "\n",
    "# Khai báo các mô hình\n",
    "\n",
    "print(\"Loading models... \")\n",
    "# Load model embedding\n",
    "\n",
    "embedding_model = load_embedding_model(eb_model_path)\n",
    "\n",
    "# Load reranking\n",
    "# rr_model_path = \"itdainb/PhoRanker\"\n",
    "# reranking_model = load_reranking_model(rr_model_path)\n",
    "\n",
    "# Dowload TTS capleaf/viXTTS\n",
    "# from huggingface_hub import snapshot_download\n",
    "\n",
    "# snapshot_download(\n",
    "#     repo_id=\"capleaf/viXTTS\", repo_type=\"model\", local_dir=\"Model/TTS_model\"\n",
    "# )\n",
    "\n",
    "tts_model_path = os.getenv(\"PROJECTCB1_TTS_MODEL\")\n",
    "# Load model TTS capleaf/viXTTS\n",
    "vixtts_model = load_model_tts(\n",
    "    xtts_checkpoint=f\"{tts_model_path}/model.pth\",\n",
    "    xtts_config=f\"{tts_model_path}/config.json\",\n",
    "    xtts_vocab=f\"{tts_model_path}/vocab.json\",\n",
    ")\n",
    "\n",
    "embeddings, pages_and_chunks = load_embeddings(embeddings_path)  # Load embeddings\n",
    "reference_audio = os.getenv(\"PROJECTCB1_REFERENCE_AUDIO\")  # Mẫu giọng nói\n",
    "\n",
    "# Load model STT nguyenvulebinh/wav2vec2-base-vietnamese-250h\n",
    "\n",
    "stt_model_path = os.getenv(\"PROJECTCB1_STT_MODEL\")\n",
    "processor = Wav2Vec2Processor.from_pretrained(stt_model_path)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(stt_model_path).to(device)\n",
    "print(\"Models Loaded!\")\n",
    "\n",
    "# processor.save_pretrained(stt_model_path)\n",
    "# model.save_pretrained(stt_model_path)\n",
    "# Hàm sử dụng cho API\n",
    "\n",
    "\n",
    "def run_stt(audio_bytes):\n",
    "    # Đọc tệp âm thanh từ byte\n",
    "    audio = AudioSegment.from_file(BytesIO(audio_bytes))\n",
    "\n",
    "    # Chuyển đổi âm thanh thành mảng numpy\n",
    "    samples = np.array(audio.get_array_of_samples())\n",
    "\n",
    "    # Đảm bảo là mono (1 kênh)\n",
    "    if audio.channels > 1:\n",
    "        samples = samples.reshape((-1, audio.channels))\n",
    "        samples = samples.mean(\n",
    "            axis=1\n",
    "        )  # Lấy trung bình giá trị của tất cả các kênh để chuyển sang mono\n",
    "\n",
    "    # Chuẩn hóa lại tần số mẫu về 16000 Hz\n",
    "    samples_16k = librosa.resample(\n",
    "        samples.astype(np.float32), orig_sr=audio.frame_rate, target_sr=16000\n",
    "    )\n",
    "\n",
    "    # Tokenize dữ liệu đầu vào\n",
    "    input_values = processor(\n",
    "        samples_16k, return_tensors=\"pt\", padding=\"longest\", sampling_rate=16000\n",
    "    ).input_values\n",
    "\n",
    "    # Chuyển sang GPU và chuyển đổi sang float\n",
    "    input_values = input_values.to(device).float()\n",
    "\n",
    "    # Lấy kết quả dự đoán từ mô hình\n",
    "    logits = model(input_values).logits\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    # Giải mã kết quả dự đoán thành văn bản\n",
    "    transcription = processor.batch_decode(predicted_ids)[0]\n",
    "    text = Xu_ly_text(transcription)\n",
    "    return text\n",
    "\n",
    "\n",
    "def run_tts(text, lang=\"vi\"):\n",
    "    if vixtts_model is None or not reference_audio:\n",
    "        return \"You need to run the previous step to load the model !!\", None, None\n",
    "\n",
    "    gpt_cond_latent, speaker_embedding = vixtts_model.get_conditioning_latents(\n",
    "        audio_path=reference_audio,\n",
    "        gpt_cond_len=vixtts_model.config.gpt_cond_len,\n",
    "        max_ref_length=vixtts_model.config.max_ref_len,\n",
    "        sound_norm_refs=vixtts_model.config.sound_norm_refs,\n",
    "    )\n",
    "\n",
    "    # Chuẩn hóa\n",
    "    tts_text = normalize_vietnamese_text(text)\n",
    "    tts_texts = split_sentences(tts_text)\n",
    "    print(tts_texts)\n",
    "    wav_chunks = []\n",
    "    for text in tts_texts:\n",
    "        if text.strip() == \"\":\n",
    "            continue\n",
    "\n",
    "        wav_chunk = vixtts_model.inference(\n",
    "            text=text,\n",
    "            language=lang,\n",
    "            gpt_cond_latent=gpt_cond_latent,\n",
    "            speaker_embedding=speaker_embedding,\n",
    "            temperature=0.01,  # 0.3\n",
    "            length_penalty=1.0,  # 1.0\n",
    "            repetition_penalty=50.0,  # 10.0\n",
    "            top_k=5,  # 30\n",
    "            top_p=0.95,  # 0.85\n",
    "        )\n",
    "\n",
    "        keep_len = -1\n",
    "        wav_chunk[\"wav\"] = torch.tensor(wav_chunk[\"wav\"][:keep_len])\n",
    "        wav_chunks.append(wav_chunk[\"wav\"])\n",
    "\n",
    "    out_wav = (\n",
    "        torch.cat(wav_chunks, dim=0).squeeze(0).cpu().numpy()\n",
    "    )  # Chuyển sang numpy array\n",
    "\n",
    "    # Chuyển đổi Tensor thành định dạng WAV\n",
    "    buffer = io.BytesIO()\n",
    "\n",
    "    # Ghi âm thanh vào buffer, đảm bảo dữ liệu đầu vào là numpy array và định dạng đúng\n",
    "    try:\n",
    "        sf.write(buffer, out_wav, 24000, format=\"WAV\")\n",
    "        buffer.seek(0)\n",
    "        wav_data = buffer.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing WAV file: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "    return wav_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_resources(query: str, n_resources_to_return: int = 15, threshold: int =0.2):\n",
    "    query_embedding = embedding_model.encode(query, convert_to_tensor=True)\n",
    "    # Use cosine similarity instead of dot score\n",
    "    cosine_scores = util.pytorch_cos_sim(query_embedding, embeddings)[0]\n",
    "    \n",
    "    # Get all scores and corresponding indices, then filter based on score > 0.5\n",
    "    scores, indices = torch.topk(input=cosine_scores, k=n_resources_to_return)\n",
    "    filtered_scores_indices = [(score.item(), index.item()) for score, index in zip(scores, indices) if score.item() > threshold]\n",
    "    \n",
    "    # Extract the scores and indices after filtering\n",
    "    filtered_indices = [index for _, index in filtered_scores_indices]\n",
    "    \n",
    "    # Take top 'n_resources_to_return' from the filtered list\n",
    "    # top_scores = filtered_scores[:n_resources_to_return]\n",
    "    top_indices = filtered_indices[:n_resources_to_return]\n",
    "    \n",
    "    context_items = [pages_and_chunks[i] for i in top_indices]\n",
    "    results = [item[\"Final_Answer\"] for item in context_items]\n",
    "    # ques = [item[\"Question\"] for item in context_items]\n",
    "    # pr_results = reranking_model.rank(query, results, return_documents=True, top_k=5)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = retrieve_relevant_resources(\"Tổng vốn của chương trình mục tiêu quốc gia là bao nhiêu?\", n_resources_to_return=10, threshold=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # coding: utf8\n",
    "# import torch\n",
    "# from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# model_path = \"vinai/PhoGPT-4B-Chat\"  \n",
    "# # model_path = \"vilm/vinallama-2.7b-chat\"\n",
    "# # model_path = \"/home/andv/important/Chatbot/finetune_llm/phogpt-mergerd-with-config-1\"\n",
    "\n",
    "# # model_path = \"/home/andv/important/Chatbot/finetune\"\n",
    "\n",
    "# # config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)  \n",
    "# # config.init_device = \"cuda\"\n",
    "# # config.attn_config['attn_impl'] = 'flash' # If installed: this will use either Flash Attention V1 or V2 depending on what is installed\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_path,\n",
    "#                                             #  config=config, \n",
    "#                                              torch_dtype=torch.bfloat16, \n",
    "#                                              trust_remote_code=True).to(\"cuda\")\n",
    "\n",
    "# # model = AutoModelForCausalLM.from_pretrained(model_path, config = config, trust_remote_code=True).to(\"cuda\")\n",
    "# # If your GPU does not support bfloat16:\n",
    "# # model = AutoModelForCausalLM.from_pretrained(model_path, config=config, torch_dtype=torch.float16, trust_remote_code=True)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import TextStreamer\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "model_path = \"SeaLLMs/SeaLLMs-v3-1.5B-Chat\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  model_path,\n",
    "  torch_dtype=torch.bfloat16, \n",
    "  device_map=device,\n",
    "  # load_in_4bit = True,\n",
    "  attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# prepare messages to model\n",
    "# Các đoạn văn có liên quan: <trích xuất các đoạn văn có liên quan từ ngữ cảnh tại đây>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to format the prompt\n",
    "def prompt_formatter(query: str, results: list) -> str:\n",
    "    context = \"* \" + \"\\n\\n* \".join(results)\n",
    "    base_prompt = \"\"\"Hãy cho bản thân không gian để suy nghĩ bằng cách trích xuất các đoạn văn có liên quan từ ngữ cảnh dưới đây trước khi trả lời câu hỏi của người dùng.\n",
    "Sử dụng các đoạn ngữ cảnh sau để trả lời câu hỏi của người dùng:\n",
    "\n",
    "{context}\n",
    "\n",
    "Các đoạn văn có liên quan: <trích xuất các đoạn văn có liên quan từ ngữ cảnh tại đây>\n",
    "Câu hỏi của người dùng: {query}\n",
    "\"\"\"\n",
    "    prompt = base_prompt.format(context=context, query=query)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to format the prompt\n",
    "# def prompt_formatter(query: str, results: list) -> str:\n",
    "#     context = \"* \" + \"\\n\\n* \".join(results)\n",
    "#     base_prompt = \"\"\"Dựa vào các đoạn ngữ cảnh dưới đây để trả lời cho câu hỏi của người dùng:\n",
    "\n",
    "# {context}\n",
    "\n",
    "# Câu hỏi của người dùng: {query}\n",
    "# \"\"\"\n",
    "#     prompt = base_prompt.format(context=context, query=query)\n",
    "#     return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Bạn là một trợ lí Tiếng Việt hữu ích. Hãy trả lời câu hỏi của người dùng một cách chính xác.\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Nội dung dự án 1 chương trình mục tiêu quốc gia là gì?\"\n",
    "results = retrieve_relevant_resources(query, n_resources_to_return=5, threshold = 0.5)\n",
    "prompt = prompt_formatter(query, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hãy cho bản thân không gian để suy nghĩ bằng cách trích xuất các đoạn văn có liên quan từ ngữ cảnh dưới đây trước khi trả lời câu hỏi của người dùng.\n",
      "Sử dụng các đoạn ngữ cảnh sau để trả lời câu hỏi của người dùng:\n",
      "\n",
      "* Nội dung cụ thể của Dự án 1 là gì?\n",
      "Nội dung cụ thể của Dự án 1 bao gồm:\n",
      "- Nội dung số 01: Hỗ trợ đất ở: Căn cứ quỹ đất, hạn mức đất ở và khả năng ngân sách, Ủy ban nhân dân cấp tỉnh xem xét, quyết định giao đất để làm nhà ở cho các đối tượng nêu trên phù hợp với điều kiện, tập quán ở địa phương và pháp luật về đất đai, cụ thể:\n",
      "+ Ở những nơi có điều kiện về đất đai, chính quyền địa phương sử dụng số tiền hỗ trợ từ ngân sách để tạo mặt bằng, làm hạ tầng kỹ thuật để cấp đất ở cho các đối tượng được thụ hưởng;\n",
      "+ Ở các địa phương không có điều kiện về đất đai, chính quyền địa phương bố trí kinh phí hỗ trợ cho người dân tự ổn định chỗ ở theo hình thức xen ghép.\n",
      "- Nội dung số 02: Hỗ trợ nhà ở: Hỗ trợ xây dựng 01 căn nhà theo phong tục tập quán của địa phương, định mức tính theo xây dựng 01 căn nhà cấp 4 đảm bảo 3 cứng (nền cứng, khung - tường cứng, mái cứng).\n",
      "- Nội dung số 03: Hỗ trợ đất sản xuất, chuyển đổi nghề: Hộ dân tộc thiểu số nghèo; hộ nghèo dân tộc Kinh sinh sống ở xã đặc biệt khó khăn, thôn đặc biệt khó khăn vùng đồng bào dân tộc thiểu số và miền núi làm nghề nông, lâm, ngư nghiệp không có hoặc thiếu từ 50% đất sản xuất trở lên theo định mức của địa phương thì được hưởng một trong hai chính sách sau:\n",
      "+ Hỗ trợ trực tiếp đất sản xuất: Hộ không có đất sản xuất nếu có nhu cầu thì được chính quyền địa phương trực tiếp giao đất sản xuất;\n",
      "+ Hỗ trợ chuyển đổi nghề: Trường hợp chính quyền địa phương không bố trí được đất sản xuất thì hộ không có đất hoặc thiếu đất sản xuất được hỗ trợ chuyển đổi nghề.\n",
      "- Nội dung số 04: Hỗ trợ nước sinh hoạt:\n",
      "+ Hỗ trợ nước sinh hoạt phân tán: Ưu tiên hỗ trợ để mua sắm trang bị hoặc xây dựng bể chứa nước phục vụ sinh hoạt của hộ gia đình;\n",
      "+ Hỗ trợ nước sinh hoạt tập trung: Đầu tư xây dựng công trình nước tập trung theo dự án được cấp có thẩm quyền phê duyệt. Ưu tiên cho người dân vùng thường xuyên xảy ra hạn hán, xâm nhập mặn, vùng đặc biệt khó khăn, vùng cao chưa có nguồn nước hoặc thiếu nước sinh hoạt hợp vệ sinh.\n",
      "- Hộ gia đình thuộc diện đối tượng theo quy định của Dự án này có nhu cầu vay vốn được vay từ Ngân hàng Chính sách xã hội để có đất ở, xây dựng mới hoặc sửa chữa nhà ở, tạo quỹ đất sản xuất, học nghề và chuyển đổi nghề.\" (Theo Phần III, Mục 1 Quyết định Thủ tướng chính phủ 1719/QĐ-TTg)\n",
      "\n",
      "* Nội dung số 01 của Dự án 1 hỗ trợ vấn đề gì?\n",
      "Nội dung số 01: Hỗ trợ đất ở: \n",
      "Căn cứ quỹ đất, hạn mức đất ở và khả năng ngân sách, Ủy ban nhân dân cấp tỉnh xem xét, quyết định giao đất để làm nhà ở cho các đối tượng nêu trên phù hợp với điều kiện, tập quán ở địa phương và pháp luật về đất đai, cụ thể:\n",
      "+ Ở những nơi có điều kiện về đất đai, chính quyền địa phương sử dụng số tiền hỗ trợ từ ngân sách để tạo mặt bằng, làm hạ tầng kỹ thuật để cấp đất ở cho các đối tượng được thụ hưởng;\n",
      "+ Ở các địa phương không có điều kiện về đất đai, chính quyền địa phương bố trí kinh phí hỗ trợ cho người dân tự ổn định chỗ ở theo hình thức xen ghép.\" (Theo Phần III, Mục 1 Quyết định Thủ tướng chính phủ 1719/QĐ-TTg)\n",
      "\n",
      "* Nội dung số 02 của Dự án 1 hỗ trợ vấn đề gì?\n",
      "Nội dung số 02: Hỗ trợ nhà ở: Hỗ trợ xây dựng 01 căn nhà theo phong tục tập quán của địa phương, định mức tính theo xây dựng 01 căn nhà cấp 4 đảm bảo 3 cứng (nền cứng, khung - tường cứng, mái cứng). (Theo Phần III, Mục 1 Quyết định Thủ tướng chính phủ 1719/QĐ-TTg)\n",
      "\n",
      "* Mục tiêu của Nội dung 1, Tiểu dự án 1, Dự án 10 là gì?\n",
      "Mục tiêu của Nội dung số 01, Tiểu dự án 1, Dự án 10 là: \n",
      "Xây dựng, nâng cao chất lượng và hiệu quả công tác vận động, phát huy vai trò của lực lượng cốt cán và người có uy tín trong vùng đồng bào dân tộc thiểu số và miền núi. Biểu dương, tôn vinh, ghi nhận công lao, sự đóng góp của các điển hình tiên tiến trong vùng đồng bào dân tộc thiểu số và miền núi trong sự nghiệp xây dựng, bảo vệ Tổ quốc và hội nhập quốc tế.\"\n",
      " (Theo Phần III, Mục 10 Quyết định Thủ tướng chính phủ 1719/QĐ-TTg)\n",
      "\n",
      "* Nội dung của Tiểu dự án 1, Dự án 5 là gì?\n",
      "Nội dung của Tiểu dự án 1, Dự án 5 thuộc Chương trình mục tiêu quốc gia phát triển kinh tế - xã hội vùng đồng bào dân tộc thiểu số và miền núi:\n",
      "+ Đầu tư cơ sở vật chất, trang thiết bị cho các trường phổ thông dân tộc nội trú, bán trú, có học sinh bán trú:\n",
      ". Nâng cấp, cải tạo cơ sở vật chất khối phòng/công trình phục vụ ăn, ở, sinh hoạt cho học sinh và phòng công vụ giáo viên;\n",
      ". Nâng cấp, cải tạo cơ sở vật chất/khối phòng/công trình phục vụ học tập; bổ sung, nâng cấp các công trình phụ trợ khác;\n",
      ". Đầu tư cơ sở vật chất phục vụ chuyển đổi số giáo dục phục vụ việc giảng dạy và học tập trực tuyến cho học sinh dân tộc thiểu số;\n",
      ". Ưu tiên đầu tư xây dựng trường dân tộc nội trú cho huyện có đông đồng bào dân tộc thiểu số sinh sống nhưng chưa có hoặc phải đi thuê địa điểm để tổ chức hoạt động.\n",
      "+ Xóa mù chữ cho người dân vùng đồng bào dân tộc thiểu số:\n",
      ". Xây dựng tài liệu phục vụ hướng dẫn dạy xoá mù chữ, thiết kế công nghệ, thiết bị lưu trữ cơ sở dữ liệu về xoá mù chữ, dạy học xoá mù chữ;\n",
      ". Bồi dưỡng, tập huấn, truyền thông, tuyên truyền;\n",
      ". Hỗ trợ người dân tham gia học xoá mù chữ;\n",
      ". Hỗ trợ tài liệu học tập, sách giáo khoa, văn phòng phẩm.\"\n",
      " (Theo Phần III, Mục 5 Quyết định Thủ tướng chính phủ 1719/QĐ-TTg)\n",
      "\n",
      "Các đoạn văn có liên quan: <trích xuất các đoạn văn có liên quan từ ngữ cảnh tại đây>\n",
      "Câu hỏi của người dùng: Nội dung dự án 1 chương trình mục tiêu quốc gia là gì?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nội dung dự án 1 chương trình mục tiêu quốc gia là: Xây dựng, nâng cao chất lượng và hiệu quả công tác vận động, phát huy vai trò của lực lượng cốt cán và người có uy tín trong vùng đồng bào dân tộc thiểu số và miền núi.\n"
     ]
    }
   ],
   "source": [
    "messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "generated_ids = model.generate(model_inputs.input_ids, \n",
    "                               do_sample=True,\n",
    "                               temperature=0.1,  \n",
    "                                top_k=40,  \n",
    "                                top_p=0.95,  \n",
    "                                max_new_tokens=1024,\n",
    "                                repetition_penalty = 1.05,  \n",
    "                               streamer=streamer)\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "messages.append({\"role\": \"assistant\", \"content\": response})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nội dung dự án 1 chương trình mục tiêu quốc gia là: Xây\n",
      "dựng, nâng cao chất lượng và hiệu quả công tác vận động,\n",
      "phát huy vai trò của lực lượng cốt cán và người có uy tín\n",
      "trong vùng đồng bào dân tộc thiểu số và miền núi.\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "a = textwrap.fill(response, width=60)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(query:str):\n",
    "    results = retrieve_relevant_resources(query=query, n_resources_to_return=5, threshold=0.5)\n",
    "    input_prompt = prompt_formatter(query=query, results=results)\n",
    "    input_ids = tokenizer(input_prompt, return_tensors=\"pt\")  \n",
    "\n",
    "    outputs = model.generate(  \n",
    "        inputs=input_ids[\"input_ids\"].to(\"cuda\"),  \n",
    "        attention_mask=input_ids[\"attention_mask\"].to(\"cuda\"),  \n",
    "        do_sample=True,  \n",
    "        temperature=0.8,  \n",
    "        top_k=50,  \n",
    "        top_p=0.9,  \n",
    "        max_new_tokens=1024,  \n",
    "        eos_token_id=tokenizer.eos_token_id,  \n",
    "        pad_token_id=tokenizer.pad_token_id  \n",
    "    )  \n",
    "\n",
    "    response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]  \n",
    "    # response = tokenizer.decode(outputs[0])\n",
    "    # response = response.split(\"### Trả lời:\")[1]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
