{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models... \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import CrossEncoder, SentenceTransformer, util\n",
    "import json\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# # Get the Hugging Face access token from environment variables\n",
    "# hf_token = os.getenv(\"PROJECTCB1_HUGGINGFACE_ACCESS_TOKEN\")\n",
    "\n",
    "# # Log in to Hugging Face using the access token\n",
    "# if hf_token:\n",
    "#     login(token=hf_token)\n",
    "# else:\n",
    "#     print(\n",
    "#         \"Access token not found. Please set the HUGGINGFACE_ACCESS_TOKEN in your .env file.\"\n",
    "#     )\n",
    "\n",
    "eb_model_path = \"./Model/vnuis_embedding_bge_final\"\n",
    "embeddings_path = \"./Data/Embedding.csv\"\n",
    "\n",
    "\n",
    "# HÃ m ná»™i bá»™\n",
    "def load_embedding_model(embedding_model_path):\n",
    "    embedding_model = SentenceTransformer(\n",
    "        model_name_or_path=embedding_model_path, \n",
    "        device=device,\n",
    "        # model_kwargs={\"torch_dtype\": \"bfloat16\"},\n",
    "        trust_remote_code= True\n",
    "    )\n",
    "    return embedding_model\n",
    "\n",
    "# def load_reranking_model(pr_model_path):\n",
    "#     pr_model = CrossEncoder(model_name=pr_model_path, device=device, trust_remote_code=True)\n",
    "#     return pr_model\n",
    "\n",
    "\n",
    "def load_embeddings(embeddings_path):\n",
    "    text_chunks_and_embedding_df = pd.read_csv(embeddings_path)\n",
    "    \n",
    "    # Convert the embedding column from a JSON string to a list of floats\n",
    "    text_chunks_and_embedding_df[\"embedding\"] = text_chunks_and_embedding_df[\"embedding\"].apply(json.loads)\n",
    "    \n",
    "    # Convert to PyTorch tensor\n",
    "    embeddings = torch.tensor(\n",
    "        np.array(text_chunks_and_embedding_df[\"embedding\"].tolist()),\n",
    "        dtype=torch.float32,\n",
    "    ).to(device)\n",
    "    \n",
    "    pages_and_chunks = text_chunks_and_embedding_df.to_dict(orient=\"records\")\n",
    "    return embeddings, pages_and_chunks\n",
    "\n",
    "\n",
    "# Khai bÃ¡o cÃ¡c mÃ´ hÃ¬nh\n",
    "\n",
    "print(\"Loading models... \")\n",
    "# Load model embedding\n",
    "\n",
    "embedding_model = load_embedding_model(eb_model_path)\n",
    "\n",
    "# Load reranking\n",
    "# rr_model_path = \"itdainb/PhoRanker\"\n",
    "# reranking_model = load_reranking_model(rr_model_path)\n",
    "\n",
    "# Dowload TTS capleaf/viXTTS\n",
    "# from huggingface_hub import snapshot_download\n",
    "\n",
    "# snapshot_download(\n",
    "#     repo_id=\"capleaf/viXTTS\", repo_type=\"model\", local_dir=\"Model/TTS_model\"\n",
    "# )\n",
    "\n",
    "embeddings, pages_and_chunks = load_embeddings(embeddings_path)  # Load embeddings\n",
    "\n",
    "# Load model STT nguyenvulebinh/wav2vec2-base-vietnamese-250h\n",
    "\n",
    "# processor.save_pretrained(stt_model_path)\n",
    "# model.save_pretrained(stt_model_path)\n",
    "# HÃ m sá»­ dá»¥ng cho API\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_resources(query: str, n_resources_to_return: int = 3, threshold: int =0.1):\n",
    "    query_embedding = embedding_model.encode(query, convert_to_tensor=True)\n",
    "    # Use cosine similarity instead of dot score\n",
    "    cosine_scores = util.pytorch_cos_sim(query_embedding, embeddings)[0]\n",
    "    \n",
    "    # Get all scores and corresponding indices, then filter based on score > 0.5\n",
    "    scores, indices = torch.topk(input=cosine_scores, k=n_resources_to_return)\n",
    "    filtered_scores_indices = [(score.item(), index.item()) for score, index in zip(scores, indices) if score.item() > threshold]\n",
    "    \n",
    "    # Extract the scores and indices after filtering\n",
    "    filtered_indices = [index for _, index in filtered_scores_indices]\n",
    "    \n",
    "    # Take top 'n_resources_to_return' from the filtered list\n",
    "    # top_scores = filtered_scores[:n_resources_to_return]\n",
    "    top_indices = filtered_indices[:n_resources_to_return]\n",
    "    \n",
    "    context_items = [pages_and_chunks[i] for i in top_indices]\n",
    "    results = [item[\"Relevant docs\"] for item in context_items]\n",
    "    # ques = [item[\"Question\"] for item in context_items]\n",
    "    # pr_results = reranking_model.rank(query, results, return_documents=True, top_k=5)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68871f5869a1496589cb19a85cd3e730",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import TextStreamer\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "model_path = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  model_path,\n",
    "  torch_dtype=torch.bfloat16, \n",
    "  device_map=device,\n",
    "  # load_in_4bit = True,\n",
    "  attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# prepare messages to model\n",
    "# CÃ¡c Ä‘oáº¡n vÄƒn cÃ³ liÃªn quan: <trÃ­ch xuáº¥t cÃ¡c Ä‘oáº¡n vÄƒn cÃ³ liÃªn quan tá»« ngá»¯ cáº£nh táº¡i Ä‘Ã¢y>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to format the prompt\n",
    "def prompt_formatter_root(query: str, results: list) -> str:\n",
    "    context = '- \"' + '\"\\n\\n- \"'.join(results) + '\"'\n",
    "\n",
    "    base_prompt = \"\"\"HÃ£y cho báº£n thÃ¢n khÃ´ng gian Ä‘á»ƒ suy nghÄ© báº±ng cÃ¡ch trÃ­ch xuáº¥t cÃ¡c Ä‘oáº¡n vÄƒn cÃ³ liÃªn quan tá»« ngá»¯ cáº£nh dÆ°á»›i Ä‘Ã¢y trÆ°á»›c khi tráº£ lá»i cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng.\n",
    "Sá»­ dá»¥ng cÃ¡c Ä‘oáº¡n ngá»¯ cáº£nh sau Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng:\n",
    "\n",
    "{context}\n",
    "\n",
    "CÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng: {query}\n",
    "KhÃ´ng sá»­ dung cÃ¡c cÃ¢u dáº«n dáº¯t, hÃ£y tráº£ vá» trá»±c tiáº¿p cÃ¢u tráº£ lá»i ngáº¯n gá»n, sÃºc tÃ­ch.\n",
    "Tráº£ lá»i:\"\"\"\n",
    "    prompt = base_prompt.format(context=context, query=query)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ask(query:str) -> str:\n",
    "# messages = [\n",
    "# {\"role\": \"system\", \"content\": \"Báº¡n lÃ  má»™t trá»£ lÃ­ Tiáº¿ng Viá»‡t há»¯u Ã­ch. HÃ£y tráº£ lá»i cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng má»™t cÃ¡ch chÃ­nh xÃ¡c.\"},\n",
    "# ]\n",
    "\n",
    "def ask(query:str)->str:\n",
    "    messages = [{\"role\": \"system\", \"content\": \"\"\"TÃ´i lÃ  Chatbot cá»§a TrÆ°á»ng Quá»‘c Táº¿ - Äáº¡i há»c Quá»‘c Gia HÃ  Ná»™i. HÃ£y tráº£ lá»i cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng má»™t cÃ¡ch chÃ­nh xÃ¡c.\"\"\",}]\n",
    "    print(f\"CÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng:{query}\" )\n",
    "    results = retrieve_relevant_resources(query, n_resources_to_return=5, threshold = 0.1)\n",
    "    prompt = prompt_formatter_root(query, results)\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "    generated_ids = model.generate(model_inputs.input_ids, \n",
    "                                do_sample=True,\n",
    "                                temperature=0.1,  \n",
    "                                    top_k=40,  \n",
    "                                    top_p=0.95,  \n",
    "                                    max_new_tokens=512,\n",
    "                                    repetition_penalty = 1.05,  \n",
    "                                streamer=streamer)\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    # old_messages.append({\"role\": \"user\", \"content\": query})\n",
    "    # old_messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HÃ£y cho báº£n thÃ¢n khÃ´ng gian Ä‘á»ƒ suy nghÄ© báº±ng cÃ¡ch trÃ­ch xuáº¥t cÃ¡c Ä‘oáº¡n vÄƒn cÃ³ liÃªn quan tá»« ngá»¯ cáº£nh dÆ°á»›i Ä‘Ã¢y trÆ°á»›c khi tráº£ lá»i cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng.\n",
      "Sá»­ dá»¥ng cÃ¡c Ä‘oáº¡n ngá»¯ cáº£nh sau Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng:\n",
      "\n",
      "- \"CÃ¡ch tÃ­nh Ä‘iá»ƒm Ä‘Ã¡nh giÃ¡ bá»™ pháº­n, Ä‘iá»ƒm há»c pháº§n theo thang Ä‘iá»ƒm 10:\n",
      "a) Loáº¡i Ä‘áº¡t:\n",
      "\n",
      "9,0 â€“ 10,0 tÆ°Æ¡ng á»©ng vá»›i A+\n",
      "8,5 â€“ 8,9 tÆ°Æ¡ng á»©ng vá»›i A\n",
      "8,0 â€“ 8,4 tÆ°Æ¡ng á»©ng vá»›i B+\n",
      "7,0 â€“ 7,9 tÆ°Æ¡ng á»©ng vá»›i B\n",
      "6,5 â€“ 6,9 tÆ°Æ¡ng á»©ng vá»›i C+\n",
      "5,5 â€“ 6,4 tÆ°Æ¡ng á»©ng vá»›i C\n",
      "5,0 â€“ 5,4 tÆ°Æ¡ng á»©ng vá»›i D+\n",
      "4,0 â€“ 4,9 tÆ°Æ¡ng á»©ng vá»›i D\n",
      "b) Loáº¡i khÃ´ng Ä‘áº¡t:\n",
      "\n",
      "DÆ°á»›i 4,0 tÆ°Æ¡ng á»©ng vá»›i F\"\n",
      "\n",
      "- \"Bao nhiÃªu Ä‘iá»ƒm thÃ¬ Ä‘áº¡t D? 4,0 â€“ 4,9 tÆ°Æ¡ng á»©ng vá»›i D theo thang Ä‘iá»ƒm 10\"\n",
      "\n",
      "CÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng: CÃ¡ch tÃ­nh Ä‘iá»ƒm Ä‘Ã¡nh giÃ¡ theo thang Ä‘iá»ƒm 10?\n",
      "KhÃ´ng sá»­ dung cÃ¡c cÃ¢u dáº«n dáº¯t, hÃ£y tráº£ vá» trá»±c tiáº¿p cÃ¢u tráº£ lá»i ngáº¯n gá»n, sÃºc tÃ­ch.\n",
      "Tráº£ lá»i:\n"
     ]
    }
   ],
   "source": [
    "query = \"CÃ¡ch tÃ­nh Ä‘iá»ƒm Ä‘Ã¡nh giÃ¡ theo thang Ä‘iá»ƒm 10?\"\n",
    "results = retrieve_relevant_resources(query, n_resources_to_return=2, threshold = 0.1)\n",
    "print(prompt_formatter_root(query=query, results=results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng:CÃ¡ch tÃ­nh Ä‘iá»ƒm Ä‘Ã¡nh giÃ¡ theo thang Ä‘iá»ƒm 10?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Äiá»ƒm Ä‘áº¡t A+: 9,0 - 10,0  \n",
      "Äiá»ƒm Ä‘áº¡t A: 8,5 - 8,9  \n",
      "Äiá»ƒm Ä‘áº¡t B+: 8,0 - 8,4  \n",
      "Äiá»ƒm Ä‘áº¡t B: 7,0 - 7,9  \n",
      "Äiá»ƒm Ä‘áº¡t C+: 5,5 - 6,4  \n",
      "Äiá»ƒm Ä‘áº¡t C: 5,0 - 5,4  \n",
      "Äiá»ƒm Ä‘áº¡t D+: 4,0 - 4,9  \n",
      "Äiá»ƒm Ä‘áº¡t D: 4,0 - 4,9  \n",
      "Äiá»ƒm khÃ´ng Ä‘áº¡t: DÆ°á»›i 4,0\n"
     ]
    }
   ],
   "source": [
    "response = ask(query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Äiá»ƒm Ä‘áº¡t A+: 9,0 - 10,0  \n",
      "Äiá»ƒm Ä‘áº¡t A: 8,5 - 8,9  \n",
      "Äiá»ƒm Ä‘áº¡t B+: 8,0 - 8,4  \n",
      "Äiá»ƒm Ä‘áº¡t B: 7,0 - 7,9  \n",
      "Äiá»ƒm Ä‘áº¡t C+: 5,5 - 6,4  \n",
      "Äiá»ƒm Ä‘áº¡t C: 5,0 - 5,4  \n",
      "Äiá»ƒm Ä‘áº¡t D+: 4,0 - 4,9  \n",
      "Äiá»ƒm Ä‘áº¡t D: 4,0 - 4,9  \n",
      "Äiá»ƒm khÃ´ng Ä‘áº¡t: DÆ°á»›i 4,0\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import streamlit as st\n",
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "\n",
    "# # Initialization\n",
    "# st.set_page_config(page_title=\"Vietnamese Chatbot\", page_icon=\"ğŸ’¬\")\n",
    "# st.title(\"Vietnamese Chatbot\")\n",
    "\n",
    "# # Define initial system message\n",
    "# messages = [\n",
    "#     {\"role\": \"system\", \"content\": \"Báº¡n lÃ  má»™t trá»£ lÃ­ Tiáº¿ng Viá»‡t há»¯u Ã­ch. HÃ£y tráº£ lá»i cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng má»™t cÃ¡ch chÃ­nh xÃ¡c.\"},\n",
    "# ]\n",
    "# lastest_conversation = []\n",
    "\n",
    "# # Streamlit UI\n",
    "# def main():\n",
    "#     st.write(\"### Há»i vÃ  ÄÃ¡p\")\n",
    "#     query = st.text_input(\"Nháº­p cÃ¢u há»i:\")\n",
    "\n",
    "#     if st.button(\"Gá»­i\") and query:\n",
    "#         st.write(f\"**Báº¡n há»i:** {query}\")\n",
    "#         response = get_response(query)\n",
    "#         st.write(f\"**Trá»£ lÃ½:** {response}\")\n",
    "\n",
    "\n",
    "# def get_response(query):\n",
    "#     # Update conversation context\n",
    "#     global lastest_conversation, messages\n",
    "#     if lastest_conversation:\n",
    "#         query = rewrite_query(query=query, lastest_conversation=lastest_conversation)\n",
    "\n",
    "#     results = retrieve_relevant_resources(query, n_resources_to_return=5, threshold=0.5)\n",
    "#     prompt = prompt_formatter_root(query, results)\n",
    "#     messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    \n",
    "#     # Generate the response\n",
    "#     text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "#     model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "#     streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "#     generated_ids = model.generate(\n",
    "#         model_inputs.input_ids,\n",
    "#         do_sample=True,\n",
    "#         temperature=0.1,\n",
    "#         top_k=40,\n",
    "#         top_p=0.95,\n",
    "#         max_new_tokens=1024,\n",
    "#         repetition_penalty=1.05,\n",
    "#         streamer=streamer\n",
    "#     )\n",
    "#     generated_ids = [\n",
    "#         output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "#     ]\n",
    "#     response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "#     # Update messages and latest conversation\n",
    "#     lastest_conversation.clear()\n",
    "#     lastest_conversation.extend([query, response])\n",
    "#     messages.pop()\n",
    "#     return response\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
