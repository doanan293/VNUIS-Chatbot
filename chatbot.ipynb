{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andv/important/chatbot_vnuis/.venv/src/tts/TTS/utils/io.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(f, map_location=map_location, **kwargs)\n",
      "GPT2InferenceModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-12-04 17:12:05,288] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-12-04 17:12:05,985] [INFO] [logging.py:128:log_dist] [Rank -1] DeepSpeed info: version=0.16.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-12-04 17:12:05,985] [WARNING] [config_utils.py:70:_process_deprecated_field] Config parameter replace_method is deprecated. This parameter is no longer needed, please remove from your call to DeepSpeed-inference\n",
      "[2024-12-04 17:12:05,986] [WARNING] [config_utils.py:70:_process_deprecated_field] Config parameter mp_size is deprecated use tensor_parallel.tp_size instead\n",
      "[2024-12-04 17:12:05,986] [INFO] [logging.py:128:log_dist] [Rank -1] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1\n",
      "[2024-12-04 17:12:06,087] [INFO] [logging.py:128:log_dist] [Rank -1] DeepSpeed-Inference config: {'layer_id': 0, 'hidden_size': 1024, 'intermediate_size': 4096, 'heads': 16, 'num_hidden_layers': -1, 'dtype': torch.float32, 'pre_layer_norm': True, 'norm_type': <NormType.LayerNorm: 1>, 'local_rank': -1, 'stochastic_mode': False, 'epsilon': 1e-05, 'mp_size': 1, 'scale_attention': True, 'triangular_masking': True, 'local_attention': False, 'window_size': 1, 'rotary_dim': -1, 'rotate_half': False, 'rotate_every_two': True, 'return_tuple': True, 'mlp_after_attn': True, 'mlp_act_func_type': <ActivationFuncType.GELU: 1>, 'training_mp_size': 1, 'bigscience_bloom': False, 'max_out_tokens': 1024, 'min_out_tokens': 1, 'scale_attn_by_inverse_layer_idx': False, 'enable_qkv_quantization': False, 'use_mup': False, 'return_single_tuple': False, 'set_empty_params': False, 'transposed_mode': False, 'use_triton': False, 'triton_autotune': False, 'num_kv': -1, 'rope_theta': 10000, 'invert_mask': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/andv/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/andv/.cache/torch_extensions/py310_cu124/transformer_inference/build.ninja...\n",
      "/home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Building extension module transformer_inference...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/11] /usr/local/cuda-12.4/bin/nvcc --generate-dependencies-with-compile --dependency-output pointwise_ops.cuda.o.d -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -isystem /home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/torch/include -isystem /home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/torch/include/TH -isystem /home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda-12.4/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ --threads=8 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_86,code=compute_86 -DBF16_AVAILABLE -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ -c /home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pointwise_ops.cu -o pointwise_ops.cuda.o \n",
      "[2/11] /usr/local/cuda-12.4/bin/nvcc --generate-dependencies-with-compile --dependency-output relu.cuda.o.d -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -isystem /home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/torch/include -isystem /home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/torch/include/TH -isystem /home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda-12.4/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ --threads=8 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_86,code=compute_86 -DBF16_AVAILABLE -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ -c /home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/relu.cu -o relu.cuda.o \n",
      "[3/11] /usr/local/cuda-12.4/bin/nvcc --generate-dependencies-with-compile --dependency-output transform.cuda.o.d -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -isystem /home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/torch/include -isystem /home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/torch/include/TH -isystem /home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda-12.4/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ --threads=8 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_86,code=compute_86 -DBF16_AVAILABLE -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ -c /home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/transform.cu -o transform.cuda.o \n",
      "/home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/transform.cu(38): warning #177-D: variable \"d0_stride\" was declared but never referenced\n",
      "      int d0_stride = hidden_dim * seq_length;\n",
      "          ^\n",
      "\n",
      "Remark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
      "\n",
      "/home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/transform.cu(66): warning #177-D: variable \"lane\" was declared but never referenced\n",
      "      int lane = d3 & 0x1f;\n",
      "          ^\n",
      "\n",
      "/home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/transform.cu(109): warning #177-D: variable \"half_dim\" was declared but never referenced\n",
      "      unsigned half_dim = (rotary_dim << 3) >> 1;\n",
      "               ^\n",
      "          detected during instantiation of \"void launch_bias_add_transform_0213(T *, T *, T *, const T *, const T *, int, int, unsigned int, int, int, int, int, int, __nv_bool, __nv_bool, cudaStream_t, int, int, float) [with T=__nv_bfloat16]\" at line 281\n",
      "\n",
      "/home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/transform.cu(110): warning #177-D: variable \"d0_stride\" was declared but never referenced\n",
      "      int d0_stride = hidden_dim * seq_length;\n",
      "          ^\n",
      "          detected during instantiation of \"void launch_bias_add_transform_0213(T *, T *, T *, const T *, const T *, int, int, unsigned int, int, int, int, int, int, __nv_bool, __nv_bool, cudaStream_t, int, int, float) [with T=__nv_bfloat16]\" at line 281\n",
      "\n",
      "/home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/transform.cu(126): warning #177-D: variable \"vals_half\" was declared but never referenced\n",
      "      T2* vals_half = reinterpret_cast<T2*>(&vals_arr);\n",
      "          ^\n",
      "          detected during instantiation of \"void launch_bias_add_transform_0213(T *, T *, T *, const T *, const T *, int, int, unsigned int, int, int, int, int, int, __nv_bool, __nv_bool, cudaStream_t, int, int, float) [with T=__nv_bfloat16]\" at line 281\n",
      "\n",
      "/home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/transform.cu(127): warning #177-D: variable \"output_half\" was declared but never referenced\n",
      "      T2* output_half = reinterpret_cast<T2*>(&output_arr);\n",
      "          ^\n",
      "          detected during instantiation of \"void launch_bias_add_transform_0213(T *, T *, T *, const T *, const T *, int, int, unsigned int, int, int, int, int, int, __nv_bool, __nv_bool, cudaStream_t, int, int, float) [with T=__nv_bfloat16]\" at line 281\n",
      "\n",
      "/home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/transform.cu(144): warning #177-D: variable \"lane\" was declared but never referenced\n",
      "      int lane = d3 & 0x1f;\n",
      "          ^\n",
      "          detected during instantiation of \"void launch_bias_add_transform_0213(T *, T *, T *, const T *, const T *, int, int, unsigned int, int, int, int, int, int, __nv_bool, __nv_bool, cudaStream_t, int, int, float) [with T=__nv_bfloat16]\" at line 281\n",
      "\n",
      "[4/11] /usr/local/cuda-12.4/bin/nvcc --generate-dependencies-with-compile --dependency-output gelu.cuda.o.d -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -isystem /home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/torch/include -isystem /home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/torch/include/TH -isystem /home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda-12.4/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ --threads=8 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_86,code=compute_86 -DBF16_AVAILABLE -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ -c /home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/gelu.cu -o gelu.cuda.o \n",
      "[5/11] /usr/local/cuda-12.4/bin/nvcc --generate-dependencies-with-compile --dependency-output dequantize.cuda.o.d -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -isystem /home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/torch/include -isystem /home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/torch/include/TH -isystem /home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda-12.4/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ --threads=8 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_86,code=compute_86 -DBF16_AVAILABLE -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ -c /home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/dequantize.cu -o dequantize.cuda.o \n",
      "[6/11] /usr/local/cuda-12.4/bin/nvcc --generate-dependencies-with-compile --dependency-output apply_rotary_pos_emb.cuda.o.d -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -isystem /home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/torch/include -isystem /home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/torch/include/TH -isystem /home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda-12.4/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ --threads=8 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_86,code=compute_86 -DBF16_AVAILABLE -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ -c /home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/apply_rotary_pos_emb.cu -o apply_rotary_pos_emb.cuda.o \n",
      "[7/11] /usr/local/cuda-12.4/bin/nvcc --generate-dependencies-with-compile --dependency-output rms_norm.cuda.o.d -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -isystem /home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/torch/include -isystem /home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/torch/include/TH -isystem /home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda-12.4/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ --threads=8 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_86,code=compute_86 -DBF16_AVAILABLE -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ -c /home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/rms_norm.cu -o rms_norm.cuda.o \n",
      "[8/11] /usr/local/cuda-12.4/bin/nvcc --generate-dependencies-with-compile --dependency-output softmax.cuda.o.d -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -isystem /home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/torch/include -isystem /home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/torch/include/TH -isystem /home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda-12.4/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ --threads=8 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_86,code=compute_86 -DBF16_AVAILABLE -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ -c /home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/softmax.cu -o softmax.cuda.o \n",
      "[9/11] /usr/local/cuda-12.4/bin/nvcc --generate-dependencies-with-compile --dependency-output layer_norm.cuda.o.d -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -isystem /home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/torch/include -isystem /home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/torch/include/TH -isystem /home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda-12.4/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ --threads=8 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_86,code=compute_86 -DBF16_AVAILABLE -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ -c /home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/layer_norm.cu -o layer_norm.cuda.o \n",
      "[10/11] c++ -MMD -MF pt_binding.o.d -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -isystem /home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/torch/include -isystem /home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/torch/include/TH -isystem /home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda-12.4/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -DBF16_AVAILABLE -c /home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp -o pt_binding.o \n",
      "[11/11] c++ pt_binding.o gelu.cuda.o relu.cuda.o layer_norm.cuda.o rms_norm.cuda.o softmax.cuda.o dequantize.cuda.o apply_rotary_pos_emb.cuda.o transform.cuda.o pointwise_ops.cuda.o -shared -lcurand -L/home/andv/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda-12.4/lib64 -lcudart -o transformer_inference.so\n",
      "Time to load transformer_inference op: 17.715929985046387 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module transformer_inference...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to load language model: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 132\u001b[0m, in \u001b[0;36mload_chat_model\u001b[0;34m(model_path, device)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 132\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_implementation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mflash_attention_2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_path)\n",
      "File \u001b[0;32m~/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:3880\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3879\u001b[0m config \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(config)  \u001b[38;5;66;03m# We do not want to modify the config inplace in from_pretrained.\u001b[39;00m\n\u001b[0;32m-> 3880\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autoset_attn_implementation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_flash_attention_2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_flash_attention_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\n\u001b[1;32m   3882\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3884\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[1;32m   3885\u001b[0m     \u001b[38;5;66;03m# Let's make sure we don't run the init function of buffer modules\u001b[39;00m\n",
      "File \u001b[0;32m~/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:1572\u001b[0m, in \u001b[0;36mPreTrainedModel._autoset_attn_implementation\u001b[0;34m(cls, config, use_flash_attention_2, torch_dtype, device_map, check_device_map)\u001b[0m\n\u001b[1;32m   1571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39m_attn_implementation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflash_attention_2\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1572\u001b[0m     \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_and_enable_flash_attn_2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1573\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1574\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1575\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1576\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhard_check_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_device_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_device_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1578\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1579\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m requested_attn_implementation \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msdpa\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available():\n\u001b[1;32m   1580\u001b[0m     \u001b[38;5;66;03m# use_flash_attention_2 takes priority over SDPA, hence SDPA treated in this elif.\u001b[39;00m\n",
      "File \u001b[0;32m~/important/chatbot_vnuis/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:1699\u001b[0m, in \u001b[0;36mPreTrainedModel._check_and_enable_flash_attn_2\u001b[0;34m(cls, config, torch_dtype, device_map, check_device_map, hard_check_only)\u001b[0m\n\u001b[1;32m   1698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mfind_spec(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflash_attn\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1699\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpreface\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m the package flash_attn seems to be not installed. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minstall_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1701\u001b[0m flash_attention_version \u001b[38;5;241m=\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(importlib\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mversion(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflash_attn\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mImportError\u001b[0m: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 185\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m# Load LLM\u001b[39;00m\n\u001b[1;32m    184\u001b[0m llm_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPROJECTCB1_LLM_MODEL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 185\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload_chat_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# Load reranking\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m#     repo_id=\"capleaf/viXTTS\", repo_type=\"model\", local_dir=\"Model/TTS_model\"\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModels Loaded!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 148\u001b[0m, in \u001b[0;36mload_chat_model\u001b[0;34m(model_path, device)\u001b[0m\n\u001b[1;32m    146\u001b[0m         tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_path)\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 148\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to load language model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model, tokenizer\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to load language model: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2."
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    Wav2Vec2Processor,\n",
    "    Wav2Vec2ForCTC,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TextStreamer,\n",
    ")\n",
    "from sentence_transformers import CrossEncoder, SentenceTransformer, util\n",
    "from routes.Xu_ly_text import Xu_ly_text, Xu_ly_text_de_doc\n",
    "from TTS.tts.configs.xtts_config import XttsConfig\n",
    "from TTS.tts.models.xtts import Xtts\n",
    "from pydub import AudioSegment\n",
    "from dotenv import load_dotenv\n",
    "from vinorm import TTSnorm\n",
    "import soundfile as sf\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import timeit\n",
    "import torch\n",
    "import json\n",
    "import io\n",
    "import re\n",
    "import os\n",
    "\n",
    "# from huggingface_hub import login\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "\n",
    "# Configure Chrome options\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "options.add_argument(\"--ignore-certificate-errors\")  # B·ªè qua l·ªói ch·ª©ng ch·ªâ SSL\n",
    "options.add_argument(\"--allow-insecure-localhost\")  # Cho ph√©p k·∫øt n·ªëi kh√¥ng an to√†n\n",
    "options.add_argument(\n",
    "    \"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    ")\n",
    "# service=Service(\"/usr/bin/chromedriver\"),\n",
    "# Initialize the WebDriver\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "# --------------------------Define function to load Model and Data---------------------------\n",
    "# H√†m n·ªôi b·ªô\n",
    "def load_embedding_model(embedding_model_path, device):\n",
    "    try:\n",
    "        embedding_model = SentenceTransformer(\n",
    "            model_name_or_path=embedding_model_path,\n",
    "            device=device,\n",
    "            model_kwargs={\"torch_dtype\": \"bfloat16\"},\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to load embedding model: {e}\")\n",
    "    return embedding_model\n",
    "\n",
    "\n",
    "# def load_reranking_model(pr_model_path):\n",
    "#     pr_model = CrossEncoder(model_name=pr_model_path, device=device)\n",
    "#     return pr_model\n",
    "\n",
    "\n",
    "def load_embeddings(embeddings_path, device):\n",
    "    try:\n",
    "        text_chunks_and_embedding_df = pd.read_csv(embeddings_path)\n",
    "\n",
    "        # Convert the embedding column from JSON strings to lists of floats\n",
    "        text_chunks_and_embedding_df[\"embedding\"] = text_chunks_and_embedding_df[\n",
    "            \"embedding\"\n",
    "        ].apply(json.loads)\n",
    "\n",
    "        # Convert embeddings to PyTorch tensors\n",
    "        embeddings = torch.tensor(\n",
    "            np.array(text_chunks_and_embedding_df[\"embedding\"].tolist()),\n",
    "            dtype=torch.bfloat16,\n",
    "        ).to(device)\n",
    "\n",
    "        pages_and_chunks = text_chunks_and_embedding_df.to_dict(orient=\"records\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to load embeddings: {e}\")\n",
    "\n",
    "    return embeddings, pages_and_chunks\n",
    "\n",
    "\n",
    "def clear_gpu_cache():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def load_model_tts(xtts_checkpoint, xtts_config, xtts_vocab):\n",
    "    clear_gpu_cache()\n",
    "    config = XttsConfig()\n",
    "    config.load_json(xtts_config)\n",
    "    XTTS_MODEL = Xtts.init_from_config(config)\n",
    "\n",
    "    use_deepspeed = torch.cuda.is_available()\n",
    "\n",
    "    XTTS_MODEL.load_checkpoint(\n",
    "        config,\n",
    "        checkpoint_path=xtts_checkpoint,\n",
    "        vocab_path=xtts_vocab,\n",
    "        use_deepspeed=use_deepspeed,\n",
    "    )\n",
    "    if torch.cuda.is_available():\n",
    "        XTTS_MODEL.cuda()\n",
    "        XTTS_MODEL.eval()\n",
    "    return XTTS_MODEL\n",
    "\n",
    "\n",
    "def load_model_stt(stt_model_path: str):\n",
    "    processor = Wav2Vec2Processor.from_pretrained(stt_model_path)\n",
    "    stt_model = Wav2Vec2ForCTC.from_pretrained(stt_model_path).to(device)\n",
    "    return processor, stt_model\n",
    "\n",
    "\n",
    "def load_chat_model(model_path, device):\n",
    "    try:\n",
    "        if device == \"cuda\":\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_path,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=device,\n",
    "                attn_implementation=\"flash_attention_2\",\n",
    "            )\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "            # model.eval()\n",
    "        else:\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_path,\n",
    "                torch_dtype=torch.float32,\n",
    "                device_map=device,\n",
    "            )\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to load language model: {e}\")\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "# -----------------------Load model and data--------------------------------------------------\n",
    "\n",
    "print(\"Loading models... \")\n",
    "# Load model embedding\n",
    "eb_model_path = os.getenv(\"PROJECTCB1_EMBEDDING_MODEL\")\n",
    "embedding_model = load_embedding_model(\n",
    "    embedding_model_path=eb_model_path, device=device\n",
    ")\n",
    "\n",
    "# Load data\n",
    "embeddings_path = os.getenv(\"PROJECTCB1_DATA_FINAL\")\n",
    "embeddings, pages_and_chunks = load_embeddings(\n",
    "    embeddings_path=embeddings_path, device=device\n",
    ")\n",
    "\n",
    "# Load model TTS capleaf/viXTTS\n",
    "tts_model_path = os.getenv(\"PROJECTCB1_TTS_MODEL\")\n",
    "vixtts_model = load_model_tts(\n",
    "    xtts_checkpoint=f\"{tts_model_path}/model.pth\",\n",
    "    xtts_config=f\"{tts_model_path}/config.json\",\n",
    "    xtts_vocab=f\"{tts_model_path}/vocab.json\",\n",
    ")\n",
    "\n",
    "# Load reference audio for tts\n",
    "reference_audio = os.getenv(\"PROJECTCB1_REFERENCE_AUDIO\")  # M·∫´u gi·ªçng n√≥i\n",
    "\n",
    "# Load model STT nguyenvulebinh/wav2vec2-base-vietnamese-250h\n",
    "stt_model_path = os.getenv(\"PROJECTCB1_STT_MODEL\")\n",
    "processor, stt_model = load_model_stt(stt_model_path=stt_model_path)\n",
    "\n",
    "# Load LLM\n",
    "llm_path = os.getenv(\"PROJECTCB1_LLM_MODEL\")\n",
    "model, tokenizer = load_chat_model(llm_path, device=device)\n",
    "# ------------------------------------------------------------------------------------\n",
    "\n",
    "# Load reranking\n",
    "# rr_model_path = \"embedding_model/PhoRanker\"\n",
    "# reranking_model = load_reranking_model(rr_model_path)\n",
    "\n",
    "# Dowload TTS capleaf/viXTTS\n",
    "# from huggingface_hub import snapshot_download\n",
    "\n",
    "# snapshot_download(\n",
    "#     repo_id=\"capleaf/viXTTS\", repo_type=\"model\", local_dir=\"Model/TTS_model\"\n",
    "# )\n",
    "\n",
    "\n",
    "print(\"Models Loaded!\")\n",
    "\n",
    "\n",
    "# ------------------------------------------Text processing-------------------------\n",
    "def normalize_vietnamese_text(text):\n",
    "    text = Xu_ly_text_de_doc(text)\n",
    "    text = (\n",
    "        TTSnorm(text, unknown=False, lower=False, rule=True)\n",
    "        .replace(\"  \", \" \")\n",
    "        .replace(\":\", \".\")\n",
    "        .replace(\"!.\", \"!\")\n",
    "        .replace(\"?.\", \"?\")\n",
    "        .replace(\" .\", \".\")\n",
    "        .replace(\" ,\", \",\")\n",
    "        .replace('\"', \"\")\n",
    "        .replace(\"'\", \"\")\n",
    "        .replace(\"+\", \" \")\n",
    "        .replace(\"..\", \".\")\n",
    "        .replace(\"AI\", \"√Çy Ai\")\n",
    "        .replace(\"A.I\", \"√Çy Ai\")\n",
    "    )\n",
    "\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "def split_sentences(text, max_length=245):\n",
    "    text = (\n",
    "        text.replace(\"\\n\", \". \").replace(\";\", \".\").replace(\"?\", \".\").replace(\"!\", \".\")\n",
    "    )\n",
    "\n",
    "    sentences = re.findall(r\"[^,.]+[,.]\", text)\n",
    "    grouped_sentences = []\n",
    "    current_group = \"\"\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # N·∫øu th√™m c√¢u v√†o m√† kh√¥ng v∆∞·ª£t qu√° gi·ªõi h·∫°n max_length\n",
    "        if len(current_group) + len(sentence) + 1 < max_length:\n",
    "            if current_group:\n",
    "                current_group += \" \" + sentence  # Gh√©p c√¢u m·ªõi v√†o c√¢u tr∆∞·ªõc ƒë√≥\n",
    "            else:\n",
    "                current_group = sentence  # C√¢u ƒë·∫ßu ti√™n c·ªßa nh√≥m\n",
    "        elif len(sentence) > max_length:  # X·ª≠ l√Ω\n",
    "            if current_group:\n",
    "                grouped_sentences.append(current_group)\n",
    "                current_group = \"\"\n",
    "            k = 0\n",
    "            tamthoi = []\n",
    "            for i in sentence.split(\" \"):\n",
    "                tamthoi += [i]\n",
    "                if len(tamthoi) >= 40:\n",
    "                    grouped_sentences += [\" \".join(tamthoi)]\n",
    "                    tamthoi = []\n",
    "            if tamthoi:\n",
    "                grouped_sentences += [\" \".join(tamthoi)]\n",
    "        else:\n",
    "            grouped_sentences.append(current_group)  # Th√™m nh√≥m v√†o list\n",
    "            current_group = sentence  # Kh·ªüi t·∫°o nh√≥m m·ªõi v·ªõi c√¢u hi·ªán t·∫°i\n",
    "\n",
    "    if current_group:\n",
    "        grouped_sentences.append(current_group)  # Th√™m nh√≥m cu·ªëi c√πng v√†o list\n",
    "\n",
    "    return grouped_sentences\n",
    "\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "rerank_tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-reranker-v2-m3\")\n",
    "rerank_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"BAAI/bge-reranker-v2-m3\", torch_dtype=torch.bfloat16, device_map=\"cuda:0\"\n",
    ")\n",
    "rerank_model.eval()\n",
    "\n",
    "\n",
    "# Retrieval with rerank\n",
    "def retrieve_relevant_resources(\n",
    "    query: str,\n",
    "    number_result_embedding: int = 20,\n",
    "    number_result_reranking: int = 5,\n",
    "    threshold: int = -4,\n",
    "):\n",
    "    query_embedding = embedding_model.encode(query, convert_to_tensor=True)\n",
    "    # cosine_scores = util.pytorch_cos_sim(query_embedding, embeddings)[0]\n",
    "    dot_scores = util.dot_score(query_embedding, embeddings)[0]\n",
    "\n",
    "    # Get top scores with a threshold\n",
    "    # scores, indices = torch.topk(input=cosine_scores, k=n_resources_to_return)\n",
    "    scores, indices = torch.topk(input=dot_scores, k=number_result_embedding)\n",
    "    print(scores)\n",
    "\n",
    "    context_items = [pages_and_chunks[i] for i in indices]\n",
    "    results = [item[\"Final_Answer\"] for item in context_items]\n",
    "\n",
    "    pairs = [[query, result] for result in results]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inputs = rerank_tokenizer(\n",
    "            pairs, padding=True, truncation=True, return_tensors=\"pt\", max_length=1024\n",
    "        )\n",
    "        inputs = {\n",
    "            key: value.to(\"cuda\") for key, value in inputs.items()\n",
    "        }  # Move all inputs to the same device as the model\n",
    "\n",
    "        # Compute scores\n",
    "        rerank_scores = rerank_model(**inputs, return_dict=True).logits.view(\n",
    "            -1,\n",
    "        )\n",
    "\n",
    "        top_scores, top_indices = torch.topk(rerank_scores, k=number_result_reranking)\n",
    "        # Help me add script to only take the score > -3\n",
    "        filtered_indices = top_indices[top_scores > threshold]\n",
    "        rerank_result = [results[i] for i in filtered_indices]\n",
    "\n",
    "    # return results, scores, top_scores, rerank_result\n",
    "    return rerank_result\n",
    "\n",
    "\n",
    "# Kh√¥ng s·ª≠ dung c√°c c√¢u d·∫´n d·∫Øt, h√£y tr·∫£ v·ªÅ tr·ª±c ti·∫øp c√¢u tr·∫£ l·ªùi.\n",
    "# ƒê·∫£m b·∫£o c√¢u tr·∫£ l·ªùi gi·∫£i th√≠ch r√µ nh·∫•t c√≥ th·ªÉ.\n",
    "# Prompt formatter\n",
    "def prompt_formatter_root(query: str, results: list) -> str:\n",
    "    context = '- \"' + '\"\\n\\n- \"'.join(results) + '\"'\n",
    "    base_prompt = \"\"\"H√£y cho b·∫£n th√¢n kh√¥ng gian ƒë·ªÉ suy nghƒ© b·∫±ng c√°ch tr√≠ch xu·∫•t c√°c ƒëo·∫°n vƒÉn c√≥ li√™n quan t·ª´ ng·ªØ c·∫£nh d∆∞·ªõi ƒë√¢y tr∆∞·ªõc khi tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng.\n",
    "S·ª≠ d·ª•ng c√°c ƒëo·∫°n ng·ªØ c·∫£nh sau ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng:\n",
    "\n",
    "{context}\n",
    "\n",
    "C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng: \"{query}\"\n",
    "Kh√¥ng s·ª≠ dung c√°c c√¢u d·∫´n d·∫Øt, h√£y tr·∫£ v·ªÅ tr·ª±c ti·∫øp c√¢u tr·∫£ l·ªùi. ƒê·∫£m b·∫£o c√¢u tr·∫£ l·ªùi gi·∫£i th√≠ch r√µ nh·∫•t c√≥ th·ªÉ. \n",
    "Tr·∫£ l·ªùi:\"\"\"\n",
    "    return base_prompt.format(context=context, query=query)\n",
    "\n",
    "\n",
    "# ----------------------------------Output function-----------------------------------------------\n",
    "def run_stt(audio_bytes):\n",
    "    # ƒê·ªçc t·ªáp √¢m thanh t·ª´ byte\n",
    "    audio = AudioSegment.from_file(BytesIO(audio_bytes))\n",
    "\n",
    "    # Chuy·ªÉn ƒë·ªïi √¢m thanh th√†nh m·∫£ng numpy\n",
    "    samples = np.array(audio.get_array_of_samples())\n",
    "\n",
    "    # ƒê·∫£m b·∫£o l√† mono (1 k√™nh)\n",
    "    if audio.channels > 1:\n",
    "        samples = samples.reshape((-1, audio.channels))\n",
    "        samples = samples.mean(\n",
    "            axis=1\n",
    "        )  # L·∫•y trung b√¨nh gi√° tr·ªã c·ªßa t·∫•t c·∫£ c√°c k√™nh ƒë·ªÉ chuy·ªÉn sang mono\n",
    "\n",
    "    # Chu·∫©n h√≥a l·∫°i t·∫ßn s·ªë m·∫´u v·ªÅ 16000 Hz\n",
    "    samples_16k = librosa.resample(\n",
    "        samples.astype(np.float32), orig_sr=audio.frame_rate, target_sr=16000\n",
    "    )\n",
    "\n",
    "    # Tokenize d·ªØ li·ªáu ƒë·∫ßu v√†o\n",
    "    input_values = processor(\n",
    "        samples_16k, return_tensors=\"pt\", padding=\"longest\", sampling_rate=16000\n",
    "    ).input_values\n",
    "\n",
    "    # Chuy·ªÉn sang GPU v√† chuy·ªÉn ƒë·ªïi sang float\n",
    "    input_values = input_values.to(device).float()\n",
    "\n",
    "    # L·∫•y k·∫øt qu·∫£ d·ª± ƒëo√°n t·ª´ m√¥ h√¨nh\n",
    "    logits = stt_model(input_values).logits\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    # Gi·∫£i m√£ k·∫øt qu·∫£ d·ª± ƒëo√°n th√†nh vƒÉn b·∫£n\n",
    "    transcription = processor.batch_decode(predicted_ids)[0]\n",
    "    text = Xu_ly_text(transcription)\n",
    "    return text\n",
    "\n",
    "\n",
    "def run_tts(text, lang=\"vi\"):\n",
    "    if vixtts_model is None or not reference_audio:\n",
    "        return \"You need to run the previous step to load the model !!\", None, None\n",
    "\n",
    "    gpt_cond_latent, speaker_embedding = vixtts_model.get_conditioning_latents(\n",
    "        audio_path=reference_audio,\n",
    "        gpt_cond_len=vixtts_model.config.gpt_cond_len,\n",
    "        max_ref_length=vixtts_model.config.max_ref_len,\n",
    "        sound_norm_refs=vixtts_model.config.sound_norm_refs,\n",
    "    )\n",
    "\n",
    "    # Chu·∫©n h√≥a\n",
    "    tts_text = normalize_vietnamese_text(text)\n",
    "    tts_texts = split_sentences(tts_text)\n",
    "    print(tts_texts)\n",
    "    wav_chunks = []\n",
    "    for text in tts_texts:\n",
    "        if text.strip() == \"\":\n",
    "            continue\n",
    "\n",
    "        wav_chunk = vixtts_model.inference(\n",
    "            text=text,\n",
    "            language=lang,\n",
    "            gpt_cond_latent=gpt_cond_latent,\n",
    "            speaker_embedding=speaker_embedding,\n",
    "            temperature=0.01,  # 0.3\n",
    "            length_penalty=1.0,  # 1.0\n",
    "            repetition_penalty=50.0,  # 10.0\n",
    "            top_k=5,  # 30\n",
    "            top_p=0.95,  # 0.85\n",
    "        )\n",
    "\n",
    "        keep_len = -1\n",
    "        wav_chunk[\"wav\"] = torch.tensor(wav_chunk[\"wav\"][:keep_len])\n",
    "        wav_chunks.append(wav_chunk[\"wav\"])\n",
    "\n",
    "    out_wav = (\n",
    "        torch.cat(wav_chunks, dim=0).squeeze(0).cpu().numpy()\n",
    "    )  # Chuy·ªÉn sang numpy array\n",
    "\n",
    "    # Chuy·ªÉn ƒë·ªïi Tensor th√†nh ƒë·ªãnh d·∫°ng WAV\n",
    "    buffer = io.BytesIO()\n",
    "\n",
    "    # Ghi √¢m thanh v√†o buffer, ƒë·∫£m b·∫£o d·ªØ li·ªáu ƒë·∫ßu v√†o l√† numpy array v√† ƒë·ªãnh d·∫°ng ƒë√∫ng\n",
    "    try:\n",
    "        sf.write(buffer, out_wav, 24000, format=\"WAV\")\n",
    "        buffer.seek(0)\n",
    "        wav_data = buffer.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing WAV file: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "    return wav_data\n",
    "\n",
    "\n",
    "#  Ph√¢n bi·ªát r√µ r√†ng, d·ª± √°n v√† ti·ªÉu d·ª± √°n l√† ho√†n to√†n kh√°c nhau\n",
    "\n",
    "\n",
    "# ---------------------------Web searching----------------------------\n",
    "def fetch_links(query: str, max_links: int = 10):\n",
    "    \"\"\"Fetch links from Google search results.\"\"\"\n",
    "    url = f\"https://www.google.com/search?q={query}\"\n",
    "    driver.get(url)\n",
    "    links = []\n",
    "\n",
    "    try:\n",
    "        # Wait for search results to load\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.XPATH, '//a[@jsname=\"UWckNb\"]'))\n",
    "        )\n",
    "        link_elements = driver.find_elements(By.XPATH, '//a[@jsname=\"UWckNb\"]')\n",
    "\n",
    "        for link_element in link_elements[:max_links]:\n",
    "            href = link_element.get_attribute(\"href\")\n",
    "            if href:\n",
    "                links.append(href)\n",
    "                print(f\"Link found: {href}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching links: {e}\")\n",
    "\n",
    "    return links\n",
    "\n",
    "\n",
    "def fetch_page_content(url: str):\n",
    "    \"\"\"Fetch page content for a given URL.\"\"\"\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "        )\n",
    "        content = driver.find_element(By.TAG_NAME, \"body\").text\n",
    "        print(f\"Content fetched from {url}\")\n",
    "        return content\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching content from {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def web_searching(query: str):\n",
    "    \"\"\"Perform web search and retrieve page content.\"\"\"\n",
    "    results_web_searching = []\n",
    "    links = fetch_links(query)\n",
    "    for link in links:\n",
    "        if any(bad_domain in link for bad_domain in [\"youtube.com\"]):\n",
    "            print(f\"Skipping: {link}\")\n",
    "            continue\n",
    "\n",
    "        content = fetch_page_content(link)\n",
    "        if content:\n",
    "            results_web_searching.append(content)\n",
    "        else:\n",
    "            continue\n",
    "        if len(results_web_searching) == 2:\n",
    "            break\n",
    "    # Return first valid content\n",
    "    text_web_searching = \"\\n\\n\".join(\n",
    "        [f'- \"{content}\"' for content in results_web_searching]\n",
    "    )\n",
    "    text_web_searching = text_web_searching[:20000]\n",
    "    return text_web_searching\n",
    "\n",
    "\n",
    "def web_searching(query: str):\n",
    "    \"\"\"Perform web search and retrieve page content.\"\"\"\n",
    "    results_web_searching = []\n",
    "    links = fetch_links(query)\n",
    "    for link in links:\n",
    "        if any(bad_domain in link for bad_domain in [\"youtube.com\"]):\n",
    "            print(f\"Skipping: {link}\")\n",
    "            continue\n",
    "\n",
    "        content = fetch_page_content(link)\n",
    "        if content:\n",
    "            results_web_searching.append(content)\n",
    "        else:\n",
    "            continue\n",
    "        if len(results_web_searching) == 2:\n",
    "            break\n",
    "    # Return first valid content\n",
    "    text_web_searching = \"\\n\\n\".join(\n",
    "        [f'- \"{content}\"' for content in results_web_searching]\n",
    "    )\n",
    "    text_web_searching = text_web_searching[:20000]\n",
    "    return text_web_searching\n",
    "\n",
    "\n",
    "# ---------------------------------Ask---------------------------\n",
    "from transformers import TextIteratorStreamer\n",
    "from threading import Thread\n",
    "\n",
    "\n",
    "def ask(query: str) -> str:\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"B·∫°n l√† m·ªôt tr·ª£ l√≠ ti·∫øng Vi·ªát h·ªØu √≠ch. H√£y tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng m·ªôt c√°ch ch√≠nh x√°c.\"\"\",\n",
    "        },\n",
    "    ]\n",
    "    results = retrieve_relevant_resources(\n",
    "        query, number_result_embedding=20, number_result_reranking=5, threshold=-4\n",
    "    )\n",
    "    if len(results) == 0:\n",
    "        web_search_result = web_searching(query=query)\n",
    "\n",
    "        prompt = f\"\"\"H√£y cho b·∫£n th√¢n kh√¥ng gian ƒë·ªÉ suy nghƒ© b·∫±ng c√°ch tr√≠ch xu·∫•t c√°c ƒëo·∫°n vƒÉn c√≥ li√™n quan t·ª´ ng·ªØ c·∫£nh d∆∞·ªõi ƒë√¢y tr∆∞·ªõc khi tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng.\n",
    "S·ª≠ d·ª•ng c√°c ƒëo·∫°n ng·ªØ c·∫£nh sau ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng:\n",
    "\n",
    "{web_search_result}\n",
    "\n",
    "C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng: \"{query}\"\n",
    "Kh√¥ng s·ª≠ dung c√°c c√¢u d·∫´n d·∫Øt, h√£y tr·∫£ v·ªÅ tr·ª±c ti·∫øp c√¢u tr·∫£ l·ªùi. ƒê·∫£m b·∫£o c√¢u tr·∫£ l·ªùi gi·∫£i th√≠ch ƒë·∫ßy ƒë·ªß, r√µ r√†ng nh·∫•t c√≥ th·ªÉ. \n",
    "Tr·∫£ l·ªùi:\"\"\"\n",
    "    else:\n",
    "        prompt = prompt_formatter_root(query, results)\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    streamer = TextIteratorStreamer(\n",
    "        tokenizer, skip_prompt=True, skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    # model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    thread = Thread(\n",
    "        target=model.generate,\n",
    "        kwargs={\n",
    "            \"input_ids\": inputs[\"input_ids\"],\n",
    "            \"streamer\": streamer,\n",
    "            \"do_sample\": True,\n",
    "            \"max_new_tokens\": 1024,\n",
    "            \"temperature\": 0.01,\n",
    "            # \"top_k\": 40,\n",
    "            # \"top_p\": 0.95,\n",
    "            # \"repetition_penalty\": 1.05,\n",
    "        },\n",
    "    )\n",
    "    thread.start()  # now start the thread\n",
    "\n",
    "    # for this example we'll both print out the new text and save it to a file\n",
    "    # -----------------------------\n",
    "    for new_text in streamer:\n",
    "        yield new_text + \"\"\n",
    "        # We can now process this text however we want,\n",
    "        # for this example we'll print the text to stdout and\n",
    "        # at the same time save the output to a text file.\n",
    "        # In reality however we can do any processing we want in this loop\n",
    "        # meaning we can fit this to just about any usecase we want!\n",
    "        print(new_text, end=\"\")\n",
    "\n",
    "    thread.join()  # join our thread\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
