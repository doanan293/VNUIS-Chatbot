{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a73fec1",
   "metadata": {},
   "source": [
    "# Test TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdf99063",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andv/VCS/VNUIS-Chatbot/.venv/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "from io import BytesIO\n",
    "from threading import Thread\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "from pydub import AudioSegment\n",
    "\n",
    "# from huggingface_hub import login\n",
    "# Opitimized\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "# from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForSpeechSeq2Seq,\n",
    "    AutoProcessor,\n",
    "    AutoTokenizer,\n",
    "    TextIteratorStreamer,\n",
    "    pipeline,\n",
    ")\n",
    "from TTS.tts.configs.xtts_config import XttsConfig  # type: ignore\n",
    "from TTS.tts.models.xtts import Xtts  # type: ignore\n",
    "\n",
    "from routes.Xu_ly_text import Xu_ly_text_de_doc\n",
    "from vi_cleaner.vi_cleaner import ViCleaner  # type: ignore\n",
    "\n",
    "# Configure logging instead of using print statements\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "device = 'cuda'\n",
    "use_deepspeed = True if device == 'cuda' else False\n",
    "def clear_gpu_cache():\n",
    "    if torch.cuda.is_available() and device == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97443c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TTS models... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andv/VCS/VNUIS-Chatbot/.venv/src/tts/TTS/utils/io.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(f, map_location=map_location, **kwargs)\n",
      "GPT2InferenceModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-12-13 19:22:49,901] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-12-13 19:22:50,544] [INFO] [logging.py:128:log_dist] [Rank -1] DeepSpeed info: version=0.16.1, git-hash=unknown, git-branch=unknown\n",
      "[2024-12-13 19:22:50,545] [WARNING] [config_utils.py:70:_process_deprecated_field] Config parameter replace_method is deprecated. This parameter is no longer needed, please remove from your call to DeepSpeed-inference\n",
      "[2024-12-13 19:22:50,545] [WARNING] [config_utils.py:70:_process_deprecated_field] Config parameter mp_size is deprecated use tensor_parallel.tp_size instead\n",
      "[2024-12-13 19:22:50,546] [INFO] [logging.py:128:log_dist] [Rank -1] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1\n",
      "[2024-12-13 19:22:50,642] [INFO] [logging.py:128:log_dist] [Rank -1] DeepSpeed-Inference config: {'layer_id': 0, 'hidden_size': 1024, 'intermediate_size': 4096, 'heads': 16, 'num_hidden_layers': -1, 'dtype': torch.float32, 'pre_layer_norm': True, 'norm_type': <NormType.LayerNorm: 1>, 'local_rank': -1, 'stochastic_mode': False, 'epsilon': 1e-05, 'mp_size': 1, 'scale_attention': True, 'triangular_masking': True, 'local_attention': False, 'window_size': 1, 'rotary_dim': -1, 'rotate_half': False, 'rotate_every_two': True, 'return_tuple': True, 'mlp_after_attn': True, 'mlp_act_func_type': <ActivationFuncType.GELU: 1>, 'training_mp_size': 1, 'bigscience_bloom': False, 'max_out_tokens': 1024, 'min_out_tokens': 1, 'scale_attn_by_inverse_layer_idx': False, 'enable_qkv_quantization': False, 'use_mup': False, 'return_single_tuple': False, 'set_empty_params': False, 'transposed_mode': False, 'use_triton': False, 'triton_autotune': False, 'num_kv': -1, 'rope_theta': 10000, 'invert_mask': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/andv/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/andv/.cache/torch_extensions/py310_cu124/transformer_inference/build.ninja...\n",
      "/home/andv/VCS/VNUIS-Chatbot/.venv/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Building extension module transformer_inference...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n",
      "Time to load transformer_inference op: 0.008982181549072266 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module transformer_inference...\n",
      "2024-12-13 19:22:50,976 - INFO - Done TTS\n"
     ]
    }
   ],
   "source": [
    "     \n",
    "# def load_model_tts(xtts_checkpoint, xtts_config, xtts_vocab):\n",
    "#     clear_gpu_cache()\n",
    "#     config = XttsConfig()\n",
    "#     config.load_json(xtts_config)\n",
    "#     XTTS_MODEL = Xtts.init_from_config(config)\n",
    "\n",
    "#     use_deepspeed = torch.cuda.is_available()\n",
    "\n",
    "#     XTTS_MODEL.load_checkpoint(\n",
    "#         config,\n",
    "#         checkpoint_path=xtts_checkpoint,\n",
    "#         vocab_path=xtts_vocab,\n",
    "#         use_deepspeed=use_deepspeed,\n",
    "#     )\n",
    "#     if torch.cuda.is_available():\n",
    "#         XTTS_MODEL.cuda()\n",
    "#         # Set model to use bfloat16 if supported\n",
    "#         if torch.cuda.get_device_capability() >= (8, 0):  # Check for bfloat16 support (Ampere+ GPUs)\n",
    "#             XTTS_MODEL = XTTS_MODEL.to(dtype=torch.bfloat16)\n",
    "#         else:\n",
    "#             logging.info(\"Warning: bfloat16 not supported on this device.\")\n",
    "#             XTTS_MODEL.eval()\n",
    "#     return XTTS_MODEL\n",
    "def load_model_tts(xtts_checkpoint, xtts_config, xtts_vocab):\n",
    "    clear_gpu_cache()\n",
    "    config = XttsConfig()\n",
    "    config.load_json(xtts_config)\n",
    "    XTTS_MODEL = Xtts.init_from_config(config)\n",
    "\n",
    "    use_deepspeed = torch.cuda.is_available()\n",
    "\n",
    "    XTTS_MODEL.load_checkpoint(\n",
    "        config,\n",
    "        checkpoint_path=xtts_checkpoint,\n",
    "        vocab_path=xtts_vocab,\n",
    "        use_deepspeed=use_deepspeed,\n",
    "    )\n",
    "    if torch.cuda.is_available():\n",
    "        XTTS_MODEL.cuda()\n",
    "        # Set model to use bfloat16 if supported\n",
    "        if torch.cuda.get_device_capability() >= (8, 0):  # Check for bfloat16 support (Ampere+ GPUs)\n",
    "            XTTS_MODEL = XTTS_MODEL.to(dtype=torch.bfloat16)\n",
    "        else:\n",
    "            logging.info(\"Warning: bfloat16 not supported on this device.\")\n",
    "            XTTS_MODEL.eval()\n",
    "    return XTTS_MODEL\n",
    "\n",
    "def run_tts(text, lang=\"vi\"):\n",
    "    if vixtts_model is None or not reference_audio:\n",
    "        return \"You need to run the previous step to load the model !!\", None, None\n",
    "\n",
    "    gpt_cond_latent, speaker_embedding = vixtts_model.get_conditioning_latents(\n",
    "        audio_path=reference_audio,\n",
    "        gpt_cond_len=vixtts_model.config.gpt_cond_len,\n",
    "        max_ref_length=vixtts_model.config.max_ref_len,\n",
    "        sound_norm_refs=vixtts_model.config.sound_norm_refs,\n",
    "    )\n",
    "\n",
    "    # Chu·∫©n h√≥a\n",
    "    tts_text = normalize_vietnamese_text(text)\n",
    "    tts_texts = split_sentences(tts_text)\n",
    "    print(tts_texts)\n",
    "    wav_chunks = []\n",
    "    for text in tts_texts:\n",
    "        if text.strip() == \"\":\n",
    "            continue\n",
    "\n",
    "        wav_chunk = vixtts_model.inference(\n",
    "            text=text,\n",
    "            language=lang,\n",
    "            gpt_cond_latent=gpt_cond_latent,\n",
    "            speaker_embedding=speaker_embedding,\n",
    "            temperature=0.01,  # 0.3\n",
    "            length_penalty=1.0,  # 1.0\n",
    "            repetition_penalty=50.0,  # 10.0\n",
    "            top_k=5,  # 30\n",
    "            top_p=0.95,  # 0.85\n",
    "        )\n",
    "\n",
    "        \n",
    "        keep_len = -1\n",
    "        wav_chunk[\"wav\"] = torch.tensor(wav_chunk[\"wav\"][:keep_len])\n",
    "        wav_chunks.append(wav_chunk[\"wav\"])\n",
    "\n",
    "\n",
    "\n",
    "    out_wav = torch.cat(wav_chunks, dim=0).unsqueeze(0).cpu()\n",
    "    out_path = 'test_nghich.wav'\n",
    "    torchaudio.save(out_path, out_wav, 24000)\n",
    "\n",
    "    return out_path\n",
    "\n",
    "\n",
    "def normalize_vietnamese_text(text):\n",
    "    cleaner = ViCleaner(text)\n",
    "    text = cleaner.clean()\n",
    "    text = Xu_ly_text_de_doc(text)\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "def split_sentences(text, max_length=245):\n",
    "    text = (\n",
    "        text.replace(\"\\n\", \". \").replace(\";\", \".\").replace(\"?\", \".\").replace(\"!\", \".\")\n",
    "    )\n",
    "\n",
    "    sentences = re.findall(r\"[^,.]+[,.]\", text)\n",
    "    grouped_sentences = []\n",
    "    current_group = \"\"\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # N·∫øu th√™m c√¢u v√†o m√† kh√¥ng v∆∞·ª£t qu√° gi·ªõi h·∫°n max_length\n",
    "        if len(current_group) + len(sentence) + 1 < max_length:\n",
    "            if current_group:\n",
    "                current_group += \" \" + sentence  # Gh√©p c√¢u m·ªõi v√†o c√¢u tr∆∞·ªõc ƒë√≥\n",
    "            else:\n",
    "                current_group = sentence  # C√¢u ƒë·∫ßu ti√™n c·ªßa nh√≥m\n",
    "        elif len(sentence) > max_length:  # X·ª≠ l√Ω\n",
    "            if current_group:\n",
    "                grouped_sentences.append(current_group)\n",
    "                current_group = \"\"\n",
    "            tamthoi = []\n",
    "            for i in sentence.split(\" \"):\n",
    "                tamthoi += [i]\n",
    "                if len(tamthoi) >= 40:\n",
    "                    grouped_sentences += [\" \".join(tamthoi)]\n",
    "                    tamthoi = []\n",
    "            if tamthoi:\n",
    "                grouped_sentences += [\" \".join(tamthoi)]\n",
    "        else:\n",
    "            grouped_sentences.append(current_group)  # Th√™m nh√≥m v√†o list\n",
    "            current_group = sentence  # Kh·ªüi t·∫°o nh√≥m m·ªõi v·ªõi c√¢u hi·ªán t·∫°i\n",
    "\n",
    "    if current_group:\n",
    "        grouped_sentences.append(current_group)  # Th√™m nh√≥m cu·ªëi c√πng v√†o list\n",
    "\n",
    "    return grouped_sentences\n",
    "\n",
    "# -------------------------------------------------------------------  LOAD MODEL ----------------------------------------------------------------------\n",
    "print(\"Loading TTS models... \")\n",
    "\n",
    "# Load model TTS capleaf/viXTTS\n",
    "tts_model_path = os.getenv(\"PROJECTCB1_TTS_MODEL\")\n",
    "vixtts_model = load_model_tts(\n",
    "    xtts_checkpoint=f\"{tts_model_path}/model.pth\",\n",
    "    xtts_config=f\"{tts_model_path}/config.json\",\n",
    "    xtts_vocab=f\"{tts_model_path}/vocab.json\",\n",
    ")\n",
    "\n",
    "logging.info(\"Done TTS\")\n",
    "# Load reference audio for tts\n",
    "reference_audio = os.getenv(\"PROJECTCB1_REFERENCE_AUDIO\")  # M·∫´u gi·ªçng n√≥i\n",
    "\n",
    "def convert_to_wav(audio_bytes):\n",
    "    try:\n",
    "        # S·ª≠ d·ª•ng pydub ƒë·ªÉ ƒë·ªçc t·ªáp √¢m thanh t·ª´ BytesIO\n",
    "        audio = AudioSegment.from_file(BytesIO(audio_bytes))\n",
    "        # T·∫°o m·ªôt t·ªáp WAV trong b·ªô nh·ªõ\n",
    "        wav_io = BytesIO()\n",
    "        audio.export(wav_io, format=\"wav\")\n",
    "        wav_io.seek(0)  # ƒê·∫∑t l·∫°i con tr·ªè t·ªáp v·ªÅ v·ªã tr√≠ ban ƒë·∫ßu\n",
    "        return wav_io\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error converting audio to WAV: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01821969",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.FloatTensor) and weight type (CUDABFloat16Type) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m text\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXin ch√†o\u001b[39m\u001b[38;5;124m\"\u001b[39m \n\u001b[1;32m      2\u001b[0m lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvi\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m gpt_cond_latent, speaker_embedding \u001b[38;5;241m=\u001b[39m \u001b[43mvixtts_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_conditioning_latents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43maudio_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreference_audio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgpt_cond_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvixtts_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgpt_cond_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_ref_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvixtts_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_ref_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43msound_norm_refs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvixtts_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msound_norm_refs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Chu·∫©n h√≥a\u001b[39;00m\n\u001b[1;32m     11\u001b[0m tts_text \u001b[38;5;241m=\u001b[39m normalize_vietnamese_text(text)\n",
      "File \u001b[0;32m~/VCS/VNUIS-Chatbot/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/VCS/VNUIS-Chatbot/.venv/src/tts/TTS/tts/models/xtts.py:365\u001b[0m, in \u001b[0;36mXtts.get_conditioning_latents\u001b[0;34m(self, audio_path, max_ref_length, gpt_cond_len, gpt_cond_chunk_len, librosa_trim_db, sound_norm_refs, load_sr)\u001b[0m\n\u001b[1;32m    362\u001b[0m     audio \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39meffects\u001b[38;5;241m.\u001b[39mtrim(audio, top_db\u001b[38;5;241m=\u001b[39mlibrosa_trim_db)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    364\u001b[0m \u001b[38;5;66;03m# compute latents for the decoder\u001b[39;00m\n\u001b[0;32m--> 365\u001b[0m speaker_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_speaker_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_sr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    366\u001b[0m speaker_embeddings\u001b[38;5;241m.\u001b[39mappend(speaker_embedding)\n\u001b[1;32m    368\u001b[0m audios\u001b[38;5;241m.\u001b[39mappend(audio)\n",
      "File \u001b[0;32m~/VCS/VNUIS-Chatbot/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/VCS/VNUIS-Chatbot/.venv/src/tts/TTS/tts/models/xtts.py:320\u001b[0m, in \u001b[0;36mXtts.get_speaker_embedding\u001b[0;34m(self, audio, sr)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39minference_mode()\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_speaker_embedding\u001b[39m(\u001b[38;5;28mself\u001b[39m, audio, sr):\n\u001b[1;32m    318\u001b[0m     audio_16k \u001b[38;5;241m=\u001b[39m torchaudio\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mresample(audio, sr, \u001b[38;5;241m16000\u001b[39m)\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 320\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhifigan_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspeaker_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_16k\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml2_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    322\u001b[0m         \u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    323\u001b[0m     )\n",
      "File \u001b[0;32m~/VCS/VNUIS-Chatbot/.venv/src/tts/TTS/tts/layers/xtts/hifigan_decoder.py:538\u001b[0m, in \u001b[0;36mResNetSpeakerEncoder.forward\u001b[0;34m(self, x, l2_norm)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;66;03m# if you torch spec compute it otherwise use the mel spec computed by the AP\u001b[39;00m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_torch_spec:\n\u001b[0;32m--> 538\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtorch_spec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_input:\n\u001b[1;32m    541\u001b[0m     x \u001b[38;5;241m=\u001b[39m (x \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-6\u001b[39m)\u001b[38;5;241m.\u001b[39mlog()\n",
      "File \u001b[0;32m~/VCS/VNUIS-Chatbot/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/VCS/VNUIS-Chatbot/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/VCS/VNUIS-Chatbot/.venv/lib/python3.10/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/VCS/VNUIS-Chatbot/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/VCS/VNUIS-Chatbot/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/VCS/VNUIS-Chatbot/.venv/src/tts/TTS/tts/layers/xtts/hifigan_decoder.py:418\u001b[0m, in \u001b[0;36mPreEmphasis.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x\u001b[38;5;241m.\u001b[39msize()) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    417\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mpad(x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m), (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreflect\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 418\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (CUDABFloat16Type) should be the same"
     ]
    }
   ],
   "source": [
    "text=\"Xin ch√†o\" \n",
    "lang=\"vi\"\n",
    "gpt_cond_latent, speaker_embedding = vixtts_model.get_conditioning_latents(\n",
    "    audio_path=reference_audio,\n",
    "    gpt_cond_len=vixtts_model.config.gpt_cond_len,\n",
    "    max_ref_length=vixtts_model.config.max_ref_len,\n",
    "    sound_norm_refs=vixtts_model.config.sound_norm_refs,\n",
    ")\n",
    "\n",
    "# Chu·∫©n h√≥a\n",
    "tts_text = normalize_vietnamese_text(text)\n",
    "tts_texts = split_sentences(tts_text)\n",
    "print(tts_texts)\n",
    "wav_chunks = []\n",
    "for text in tts_texts:\n",
    "    if text.strip() == \"\":\n",
    "        continue\n",
    "\n",
    "    # Ensure the dtype of the input matches the model's dtype\n",
    "    input_dtype = next(vixtts_model.parameters()).dtype  # Get the dtype of the model weights (bfloat16 or float32)\n",
    "    \n",
    "    # Convert the input tensors to the model's dtype (bfloat16 in this case)\n",
    "    gpt_cond_latent = gpt_cond_latent.to(dtype=torch.bfloat16)\n",
    "    speaker_embedding = speaker_embedding.to(dtype=torch.bfloat16)\n",
    "    \n",
    "    print(f\"Model dtype: {next(vixtts_model.parameters()).dtype}\")\n",
    "    print(f\"GPT Latent dtype: {gpt_cond_latent.dtype}\")\n",
    "    print(f\"Speaker Embedding dtype: {speaker_embedding.dtype}\")\n",
    "\n",
    "\n",
    "#     wav_chunk = vixtts_model.inference(\n",
    "#         text=text,\n",
    "#         language=lang,\n",
    "#         gpt_cond_latent=gpt_cond_latent,\n",
    "#         speaker_embedding=speaker_embedding,\n",
    "#         temperature=0.01,  # 0.3\n",
    "#         length_penalty=1.0,  # 1.0\n",
    "#         repetition_penalty=50.0,  # 10.0\n",
    "#         top_k=5,  # 30\n",
    "#         top_p=0.95,  # 0.85\n",
    "#     )\n",
    "\n",
    "    \n",
    "#     keep_len = -1\n",
    "#     wav_chunk[\"wav\"] = torch.tensor(wav_chunk[\"wav\"][:keep_len])\n",
    "#     wav_chunks.append(wav_chunk[\"wav\"])\n",
    "\n",
    "\n",
    "\n",
    "# out_wav = torch.cat(wav_chunks, dim=0).unsqueeze(0).cpu()\n",
    "# out_path = 'test_nghich.wav'\n",
    "# torchaudio.save(out_path, out_wav, 24000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb8594c",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.FloatTensor) and weight type (CUDABFloat16Type) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m audio_file \u001b[38;5;241m=\u001b[39m \u001b[43mrun_tts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[38;5;124;43mV·ªën c·ªßa t·ª´ng d·ª± √°n trong ch∆∞∆°ng tr√¨nh m·ª•c ti√™u qu·ªëc gia ph√°t tri·ªÉn kinh t·∫ø - x√£ h·ªôi v√πng ƒë·ªìng b√†o d√¢n t·ªôc thi·ªÉu s·ªë v√† mi·ªÅn n√∫i giai ƒëo·∫°n 2021 - 2030. Giai ƒëo·∫°n I: T·ª´ NƒÉm 2021 ƒë·∫øn nƒÉm 2025\u001b[39;49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;43mD·ª± ki·∫øn nhu c·∫ßu v·ªën v√† ngu·ªìn v·ªën ƒë·ªÉ th·ª±c hi·ªán D·ª± √°n 1: 18.177,448 t·ª∑ ƒë·ªìng, trong ƒë√≥:\u001b[39;49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;43m- Ng√¢n s√°ch trung ∆∞∆°ng: 7.840,553 t·ª∑ ƒë·ªìng (v·ªën ƒë·∫ßu t∆∞: 4.565,965 t·ª∑ ƒë·ªìng; v·ªën s·ª± nghi·ªáp: 3.274,588 t·ª∑ ƒë·ªìng);\u001b[39;49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;43m- Ng√¢n s√°ch ƒë·ªãa ph∆∞∆°ng: 640,321 t·ª∑ ƒë·ªìng;\u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;43m- V·ªën vay t√≠n d·ª•ng ch√≠nh s√°ch: 9.291,096 t·ª∑ ƒë·ªìng;\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;43m- V·ªën huy ƒë·ªông h·ª£p ph√°p kh√°c: 405,478 t·ª∑ ƒë·ªìng.\u001b[39;49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Audio\n\u001b[1;32m     10\u001b[0m Audio(audio_file)\n",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m, in \u001b[0;36mrun_tts\u001b[0;34m(text, lang)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vixtts_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m reference_audio:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to run the previous step to load the model !!\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m gpt_cond_latent, speaker_embedding \u001b[38;5;241m=\u001b[39m \u001b[43mvixtts_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_conditioning_latents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43maudio_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreference_audio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgpt_cond_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvixtts_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgpt_cond_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_ref_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvixtts_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_ref_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43msound_norm_refs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvixtts_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msound_norm_refs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Chu·∫©n h√≥a\u001b[39;00m\n\u001b[1;32m     13\u001b[0m tts_text \u001b[38;5;241m=\u001b[39m normalize_vietnamese_text(text)\n",
      "File \u001b[0;32m~/VCS/VNUIS-Chatbot/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/VCS/VNUIS-Chatbot/.venv/src/tts/TTS/tts/models/xtts.py:365\u001b[0m, in \u001b[0;36mXtts.get_conditioning_latents\u001b[0;34m(self, audio_path, max_ref_length, gpt_cond_len, gpt_cond_chunk_len, librosa_trim_db, sound_norm_refs, load_sr)\u001b[0m\n\u001b[1;32m    362\u001b[0m     audio \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39meffects\u001b[38;5;241m.\u001b[39mtrim(audio, top_db\u001b[38;5;241m=\u001b[39mlibrosa_trim_db)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    364\u001b[0m \u001b[38;5;66;03m# compute latents for the decoder\u001b[39;00m\n\u001b[0;32m--> 365\u001b[0m speaker_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_speaker_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_sr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    366\u001b[0m speaker_embeddings\u001b[38;5;241m.\u001b[39mappend(speaker_embedding)\n\u001b[1;32m    368\u001b[0m audios\u001b[38;5;241m.\u001b[39mappend(audio)\n",
      "File \u001b[0;32m~/VCS/VNUIS-Chatbot/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/VCS/VNUIS-Chatbot/.venv/src/tts/TTS/tts/models/xtts.py:320\u001b[0m, in \u001b[0;36mXtts.get_speaker_embedding\u001b[0;34m(self, audio, sr)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39minference_mode()\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_speaker_embedding\u001b[39m(\u001b[38;5;28mself\u001b[39m, audio, sr):\n\u001b[1;32m    318\u001b[0m     audio_16k \u001b[38;5;241m=\u001b[39m torchaudio\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mresample(audio, sr, \u001b[38;5;241m16000\u001b[39m)\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 320\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhifigan_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspeaker_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_16k\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml2_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    322\u001b[0m         \u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    323\u001b[0m     )\n",
      "File \u001b[0;32m~/VCS/VNUIS-Chatbot/.venv/src/tts/TTS/tts/layers/xtts/hifigan_decoder.py:538\u001b[0m, in \u001b[0;36mResNetSpeakerEncoder.forward\u001b[0;34m(self, x, l2_norm)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;66;03m# if you torch spec compute it otherwise use the mel spec computed by the AP\u001b[39;00m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_torch_spec:\n\u001b[0;32m--> 538\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtorch_spec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_input:\n\u001b[1;32m    541\u001b[0m     x \u001b[38;5;241m=\u001b[39m (x \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-6\u001b[39m)\u001b[38;5;241m.\u001b[39mlog()\n",
      "File \u001b[0;32m~/VCS/VNUIS-Chatbot/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/VCS/VNUIS-Chatbot/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/VCS/VNUIS-Chatbot/.venv/lib/python3.10/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/VCS/VNUIS-Chatbot/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/VCS/VNUIS-Chatbot/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/VCS/VNUIS-Chatbot/.venv/src/tts/TTS/tts/layers/xtts/hifigan_decoder.py:418\u001b[0m, in \u001b[0;36mPreEmphasis.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x\u001b[38;5;241m.\u001b[39msize()) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    417\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mpad(x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m), (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreflect\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 418\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (CUDABFloat16Type) should be the same"
     ]
    }
   ],
   "source": [
    "# audio_file = run_tts(text=\"\"\"V·ªën c·ªßa t·ª´ng d·ª± √°n trong ch∆∞∆°ng tr√¨nh m·ª•c ti√™u qu·ªëc gia ph√°t tri·ªÉn kinh t·∫ø - x√£ h·ªôi v√πng ƒë·ªìng b√†o d√¢n t·ªôc thi·ªÉu s·ªë v√† mi·ªÅn n√∫i giai ƒëo·∫°n 2021 - 2030. Giai ƒëo·∫°n I: T·ª´ NƒÉm 2021 ƒë·∫øn nƒÉm 2025\n",
    "# D·ª± ki·∫øn nhu c·∫ßu v·ªën v√† ngu·ªìn v·ªën ƒë·ªÉ th·ª±c hi·ªán D·ª± √°n 1: 18.177,448 t·ª∑ ƒë·ªìng, trong ƒë√≥:\n",
    "# - Ng√¢n s√°ch trung ∆∞∆°ng: 7.840,553 t·ª∑ ƒë·ªìng (v·ªën ƒë·∫ßu t∆∞: 4.565,965 t·ª∑ ƒë·ªìng; v·ªën s·ª± nghi·ªáp: 3.274,588 t·ª∑ ƒë·ªìng);\n",
    "# - Ng√¢n s√°ch ƒë·ªãa ph∆∞∆°ng: 640,321 t·ª∑ ƒë·ªìng;\n",
    "# - V·ªën vay t√≠n d·ª•ng ch√≠nh s√°ch: 9.291,096 t·ª∑ ƒë·ªìng;\n",
    "# - V·ªën huy ƒë·ªông h·ª£p ph√°p kh√°c: 405,478 t·ª∑ ƒë·ªìng.\n",
    "# \"\"\")\n",
    "\n",
    "# from IPython.display import Audio\n",
    "# Audio(audio_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f845db88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_tts(xtts_checkpoint, xtts_config, xtts_vocab):\n",
    "    clear_gpu_cache()\n",
    "    config = XttsConfig()\n",
    "    config.load_json(xtts_config)\n",
    "    XTTS_MODEL = Xtts.init_from_config(config)\n",
    "\n",
    "    use_deepspeed = torch.cuda.is_available()\n",
    "\n",
    "    XTTS_MODEL.load_checkpoint(\n",
    "        config,\n",
    "        checkpoint_path=xtts_checkpoint,\n",
    "        vocab_path=xtts_vocab,\n",
    "        use_deepspeed=use_deepspeed,\n",
    "    )\n",
    "    if torch.cuda.is_available():\n",
    "        XTTS_MODEL.cuda()\n",
    "        # Set model to use bfloat16 if supported\n",
    "        if torch.cuda.get_device_capability() >= (8, 0):  # Check for bfloat16 support (Ampere+ GPUs)\n",
    "            XTTS_MODEL = XTTS_MODEL.to(dtype=torch.bfloat16)\n",
    "        else:\n",
    "            logging.info(\"Warning: bfloat16 not supported on this device.\")\n",
    "            XTTS_MODEL.eval()\n",
    "    return XTTS_MODEL\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e99f889",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gpt_cond_latent, speaker_embedding = vixtts_model.get_conditioning_latents(\n",
    "    audio_path=reference_audio,\n",
    "    gpt_cond_len=vixtts_model.config.gpt_cond_len,\n",
    "    max_ref_length=vixtts_model.config.max_ref_len,\n",
    "    sound_norm_refs=vixtts_model.config.sound_norm_refs,\n",
    ")\n",
    "\n",
    "# Chu·∫©n h√≥a\n",
    "tts_text = normalize_vietnamese_text(text)\n",
    "tts_texts = split_sentences(tts_text)\n",
    "print(tts_texts)\n",
    "wav_chunks = []\n",
    "for text in tts_texts:\n",
    "    if text.strip() == \"\":\n",
    "        continue\n",
    "\n",
    "    # Ensure the dtype of the input matches the model's dtype\n",
    "    input_dtype = next(vixtts_model.parameters()).dtype  # Get the dtype of the model weights (bfloat16 or float32)\n",
    "    \n",
    "    # Convert the input tensors to the model's dtype (bfloat16 in this case)\n",
    "    gpt_cond_latent = gpt_cond_latent.to(dtype=input_dtype)\n",
    "    speaker_embedding = speaker_embedding.to(dtype=input_dtype)\n",
    "\n",
    "    wav_chunk = vixtts_model.inference(\n",
    "        text=text,\n",
    "        language=lang,\n",
    "        gpt_cond_latent=gpt_cond_latent,\n",
    "        speaker_embedding=speaker_embedding,\n",
    "        temperature=0.01,  # 0.3\n",
    "        length_penalty=1.0,  # 1.0\n",
    "        repetition_penalty=50.0,  # 10.0\n",
    "        top_k=5,  # 30\n",
    "        top_p=0.95,  # 0.85\n",
    "    )\n",
    "\n",
    "    keep_len = -1\n",
    "    wav_chunk[\"wav\"] = torch.tensor(wav_chunk[\"wav\"][:keep_len])\n",
    "    wav_chunks.append(wav_chunk[\"wav\"])\n",
    "\n",
    "out_wav = torch.cat(wav_chunks, dim=0).unsqueeze(0).cpu()\n",
    "out_path = 'test_nghich.wav'\n",
    "torchaudio.save(out_path, out_wav, 24000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
